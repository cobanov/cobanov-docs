{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hello \ud83d\udc4b Welcome to my blog, Here is a lot of stuff you can find, on my blog I usually write about topics of interest and sometimes I publish translations, In the portfolio part, I have my works, sometimes I produce artistic things. If you want to see it, you can visit it.","title":"Hello \ud83d\udc4b"},{"location":"#hello","text":"Welcome to my blog, Here is a lot of stuff you can find, on my blog I usually write about topics of interest and sometimes I publish translations, In the portfolio part, I have my works, sometimes I produce artistic things. If you want to see it, you can visit it.","title":"Hello \ud83d\udc4b"},{"location":"about/","text":"Mert Cobanov #0001 Username: @cobanov Job: Data Scientist, Refik Anadol Studio Date of Birth: 02.01.1996 About Community lover, data scientist and generative artist. I love to share what I know, so I try to provide free data science and machine learning education to everyone on my twitter and youtube channel. I teach at several schools and communities each year. I work as a data scientist at Refik Anadol Studio , where we do great artistic works with my excellent team, and we try to digitize the memories of humanity to immortalize it. Links More You can see my work in my github repos and my humble portfolio . I really enjoy writing minimal tools that make the lives of software developers easier, building end-to-end pipelines, and doing AI artistic painting works that motivate me immensely. I also digitize and repair old video tapes and bring them to high resolution and high fps forms.","title":"Main Page"},{"location":"about/#mert-cobanov-0001","text":"Username: @cobanov Job: Data Scientist, Refik Anadol Studio Date of Birth: 02.01.1996","title":"Mert Cobanov #0001"},{"location":"about/#about","text":"Community lover, data scientist and generative artist. I love to share what I know, so I try to provide free data science and machine learning education to everyone on my twitter and youtube channel. I teach at several schools and communities each year. I work as a data scientist at Refik Anadol Studio , where we do great artistic works with my excellent team, and we try to digitize the memories of humanity to immortalize it.","title":"About"},{"location":"about/#links","text":"","title":"Links"},{"location":"about/#more","text":"You can see my work in my github repos and my humble portfolio . I really enjoy writing minimal tools that make the lives of software developers easier, building end-to-end pipelines, and doing AI artistic painting works that motivate me immensely. I also digitize and repair old video tapes and bring them to high resolution and high fps forms.","title":"More"},{"location":"anonymous-domain/","text":"Domain Private domain registration is an add-on service that many domain registrars offer and charge an extra annual fee for. Normally, when you buy a domain name, your information is listed publicly in the ICANN databases. When you buy a domain privately, the registrar\u2019s information is listed as the contact information. So, the domain name purchase is not anonymous, however it is private. How to Buy A Domain Anonymously Use a private web browser like Tor Browser or Brave Browser Use a shell corporation to register the domain name Use a quality VPN service when buying Tick \"private registration\" when registering the domain name Pay with a prepaid credit card or cryptocurrency Use an email address that is only used for this domain registrar Buy from a domain registrar that accepts your payment method Register the domain privately Be Careful These Steps Some steps you can take to keep your identity secure include: Using Tor to register the domain name, but understand its limitations Purchase your domain name using a prepaid debit card (Visa or Mastercard) Tick \"private registration\" when registering the domain name, most registrars have this option and it will protect your details from being visible in the whois records. Keep in mind this needs to be done at the time of registration. If you register a domain and then make it private after the fact, your details could have been already archived. You can also use a service like Domains By Proxy (this is what some registrars use) Choose secure passwords (25 to 40 characters in length) for everything from your email, hosting and domain registration. This will be how a hacker would most likely get your information, by exploiting weak passwords in anything remotely linked to your site. I suggest a password manager like 1Password which allows you to generate secure passwords. Don't post any identifying information. Don't use usernames you've used elsewhere. Don't publicly disclose your location, name, age or where you work. Steer clear of social networks like Twitter or Facebook (don't create official accounts for your site on these sites). Choose secure usernames unique to your site, as mentioned above, don't use a username you have used elsewhere. NEVER use default users like in Wordpress, don't use \"admin\" for the username. Create a unique and independent email address, don't use commercial email services like Gmail, Yahoo! Mail or Hotmail as these are vulnerable and can be used to identify you. There are numerous secure email services out there that can help protect you. If you post images, make sure you have stripped any Exif data from them (date and time, location, GPS coordinates, filename, camera type). Disable any kind of social/beacon feature; no comments, no Facebook like buttons or boxes, no analytics code or anything else.","title":"Domain"},{"location":"anonymous-domain/#domain","text":"Private domain registration is an add-on service that many domain registrars offer and charge an extra annual fee for. Normally, when you buy a domain name, your information is listed publicly in the ICANN databases. When you buy a domain privately, the registrar\u2019s information is listed as the contact information. So, the domain name purchase is not anonymous, however it is private.","title":"Domain"},{"location":"anonymous-domain/#how-to-buy-a-domain-anonymously","text":"Use a private web browser like Tor Browser or Brave Browser Use a shell corporation to register the domain name Use a quality VPN service when buying Tick \"private registration\" when registering the domain name Pay with a prepaid credit card or cryptocurrency Use an email address that is only used for this domain registrar Buy from a domain registrar that accepts your payment method Register the domain privately","title":"How to Buy A Domain Anonymously"},{"location":"anonymous-domain/#be-careful-these-steps","text":"Some steps you can take to keep your identity secure include: Using Tor to register the domain name, but understand its limitations Purchase your domain name using a prepaid debit card (Visa or Mastercard) Tick \"private registration\" when registering the domain name, most registrars have this option and it will protect your details from being visible in the whois records. Keep in mind this needs to be done at the time of registration. If you register a domain and then make it private after the fact, your details could have been already archived. You can also use a service like Domains By Proxy (this is what some registrars use) Choose secure passwords (25 to 40 characters in length) for everything from your email, hosting and domain registration. This will be how a hacker would most likely get your information, by exploiting weak passwords in anything remotely linked to your site. I suggest a password manager like 1Password which allows you to generate secure passwords. Don't post any identifying information. Don't use usernames you've used elsewhere. Don't publicly disclose your location, name, age or where you work. Steer clear of social networks like Twitter or Facebook (don't create official accounts for your site on these sites). Choose secure usernames unique to your site, as mentioned above, don't use a username you have used elsewhere. NEVER use default users like in Wordpress, don't use \"admin\" for the username. Create a unique and independent email address, don't use commercial email services like Gmail, Yahoo! Mail or Hotmail as these are vulnerable and can be used to identify you. There are numerous secure email services out there that can help protect you. If you post images, make sure you have stripped any Exif data from them (date and time, location, GPS coordinates, filename, camera type). Disable any kind of social/beacon feature; no comments, no Facebook like buttons or boxes, no analytics code or anything else.","title":"Be Careful These Steps"},{"location":"cyberpunk/","text":"Bir Cypherpunk'\u0131n Manifestosu by Eric Hughes Elektronik \u00e7a\u011fda a\u00e7\u0131k bir toplum i\u00e7in mahremiyet gereklidir. Gizlilik gizlilik de\u011fildir. \u00d6zel bir konu, t\u00fcm d\u00fcnyan\u0131n bilmesini istemedi\u011fi bir \u015feydir, ancak gizli bir konu, kimsenin bilmesini istemedi\u011fi bir \u015feydir. Gizlilik, kendini d\u00fcnyaya se\u00e7ici olarak g\u00f6sterme g\u00fcc\u00fcd\u00fcr. E\u011fer iki taraf bir t\u00fcr anla\u015fmaya sahipse, her birinin etkile\u015fimlerinin bir an\u0131s\u0131 vard\u0131r. Her bir taraf bununla ilgili kendi an\u0131lar\u0131 hakk\u0131nda konu\u015fabilir; kimse bunu nas\u0131l engelleyebilir? Buna kar\u015f\u0131 yasalar \u00e7\u0131kar\u0131labilir, ancak konu\u015fma \u00f6zg\u00fcrl\u00fc\u011f\u00fc, mahremiyetten bile daha fazlas\u0131, a\u00e7\u0131k bir toplum i\u00e7in esast\u0131r; hi\u00e7bir konu\u015fmay\u0131 k\u0131s\u0131tlamamaya \u00e7al\u0131\u015f\u0131yoruz. Ayn\u0131 forumda bir\u00e7ok taraf birlikte konu\u015fursa, her biri di\u011ferleriyle konu\u015fabilir ve bireyler ve di\u011fer taraflar hakk\u0131ndaki bilgileri bir araya toplayabilir. Elektronik ileti\u015fimin g\u00fcc\u00fc bu t\u00fcr grup konu\u015fmalar\u0131n\u0131 m\u00fcmk\u00fcn k\u0131lm\u0131\u015ft\u0131r ve sadece biz isteyebilece\u011fimiz i\u00e7in ortadan kalkmayacakt\u0131r. Gizlili\u011fi arzulad\u0131\u011f\u0131m\u0131z i\u00e7in, bir i\u015flemin her bir taraf\u0131n\u0131n yaln\u0131zca o i\u015flem i\u00e7in do\u011frudan gerekli olan\u0131 bilmesini sa\u011flamal\u0131y\u0131z. Herhangi bir bilgi konu\u015fulabilece\u011finden, m\u00fcmk\u00fcn oldu\u011funca az a\u00e7\u0131klama yapt\u0131\u011f\u0131m\u0131zdan emin olmal\u0131y\u0131z. \u00c7o\u011fu durumda ki\u015fisel kimlik belirgin de\u011fildir. Bir ma\u011fazadan bir dergi sat\u0131n ald\u0131\u011f\u0131mda ve katiyere nakit verdi\u011fimde, kim oldu\u011fumu bilmeme gerek yok. Elektronik posta sa\u011flay\u0131c\u0131mdan mesaj g\u00f6nderip almas\u0131n\u0131 istedi\u011fimde, sa\u011flay\u0131c\u0131m\u0131n kiminle konu\u015ftu\u011fumu, ne s\u00f6yledi\u011fimi veya ba\u015fkalar\u0131n\u0131n bana ne s\u00f6yledi\u011fini bilmesine gerek yoktur; sa\u011flay\u0131c\u0131m\u0131n sadece mesaj\u0131 oraya nas\u0131l ula\u015ft\u0131raca\u011f\u0131n\u0131 ve onlara ne kadar \u00fccret bor\u00e7lu oldu\u011fumu bilmesi gerekiyor. Kimli\u011fim i\u015flemin alt\u0131nda yatan mekanizma taraf\u0131ndan if\u015fa edildi\u011finde, mahremiyetim yok. Burada se\u00e7ici olarak kendimi if\u015fa edemem; her zaman yapmal\u0131y\u0131m kendimi if\u015fa et. Bu nedenle, a\u00e7\u0131k bir toplumda mahremiyet, anonim i\u015flem sistemleri gerektirir. \u015eimdiye kadar, nakit bu t\u00fcr birincil sistem olmu\u015ftur. Anonim bir i\u015flem sistemi, gizli bir i\u015flem sistemi de\u011fildir. Anonim bir sistem, bireylere kimliklerini istendi\u011finde ve yaln\u0131zca istendi\u011finde if\u015fa etme yetkisi verir; mahremiyetin \u00f6z\u00fc budur. A\u00e7\u0131k bir toplumda gizlilik de kriptografi gerektirir. Bir \u015fey s\u00f6ylersem, sadece niyet etti\u011fim ki\u015filer taraf\u0131ndan duyulmas\u0131n\u0131 isterim. Konu\u015fmam\u0131n i\u00e7eri\u011fi d\u00fcnyaya a\u00e7\u0131ksa, mahremiyetim yok. \u015eifrelemek, mahremiyet arzusunu belirtmektir ve zay\u0131f \u015fifreleme ile \u015fifrelemek, mahremiyet i\u00e7in \u00e7ok fazla arzu olmad\u0131\u011f\u0131n\u0131 belirtmektir. Ayr\u0131ca, varsay\u0131lan anonimlik oldu\u011funda, ki\u015finin kimli\u011fini g\u00fcvence ile ortaya \u00e7\u0131karmak i\u00e7in kriptografik imza gerekir. H\u00fck\u00fcmetlerin, \u015firketlerin veya di\u011fer b\u00fcy\u00fck, kimli\u011fi belirsiz kurulu\u015flar\u0131n, kendi yararlar\u0131 d\u0131\u015f\u0131nda bize mahremiyet vermelerini bekleyemeyiz. Bizim hakk\u0131m\u0131zda konu\u015fmak onlar\u0131n yarar\u0131nad\u0131r ve konu\u015fmalar\u0131n\u0131 beklemeliyiz. Konu\u015fmalar\u0131n\u0131 engellemeye \u00e7al\u0131\u015fmak, bilgi ger\u00e7ekleriyle sava\u015fmakt\u0131r. Bilgi sadece \u00f6zg\u00fcr olmak istemez, \u00f6zg\u00fcr olmay\u0131 arzular. Bilgi, mevcut depolama alan\u0131n\u0131 dolduracak \u015fekilde geni\u015fler. Bilgi, Rumor'un daha gen\u00e7, daha g\u00fc\u00e7l\u00fc kuzenidir; Bilgi, daha h\u0131zl\u0131d\u0131r, daha fazla g\u00f6ze sahiptir, daha fazlas\u0131n\u0131 bilir ve S\u00f6ylentiden daha az anlar. Herhangi bir mahremiyete sahip olmay\u0131 umuyorsak, kendi mahremiyetimizi savunmal\u0131y\u0131z. Bir araya gelmeli ve anonim i\u015flemlerin ger\u00e7ekle\u015fmesine izin veren sistemler olu\u015fturmal\u0131y\u0131z. \u0130nsanlar y\u00fczy\u0131llard\u0131r f\u0131s\u0131lt\u0131lar, karanl\u0131k, zarflar, kapal\u0131 kap\u0131lar, gizli tokala\u015fmalar ve kuryelerle kendi mahremiyetlerini savunuyorlar. Ge\u00e7mi\u015fin teknolojileri g\u00fc\u00e7l\u00fc bir mahremiyete izin vermiyordu, ancak elektronik teknolojiler izin veriyor. Biz Cypherpunk'lar, kendimizi anonim sistemler olu\u015fturmaya adad\u0131k. Gizlili\u011fimizi kriptografi, anonim posta y\u00f6nlendirme sistemleri, dijital imzalar ve elektronik para ile koruyoruz. Cypherpunk'lar kod yazar. Birinin mahremiyeti savunmak i\u00e7in yaz\u0131l\u0131m yazmas\u0131 gerekti\u011fini biliyoruz ve hepimiz bunu yapmad\u0131k\u00e7a mahremiyet elde edemeyece\u011fimiz i\u00e7in yazaca\u011f\u0131z. Kodumuzu Cypherpunk arkada\u015flar\u0131m\u0131z\u0131n pratik yap\u0131p onunla oynayabilmeleri i\u00e7in yay\u0131nl\u0131yoruz. Kodumuz, d\u00fcnya \u00e7ap\u0131nda herkesin kullan\u0131m\u0131 i\u00e7in \u00fccretsizdir. Yazd\u0131\u011f\u0131m\u0131z yaz\u0131l\u0131m\u0131 onaylamaman\u0131z pek umurumuzda de\u011fil. Yaz\u0131l\u0131m\u0131n yok edilemeyece\u011fini ve geni\u015f bir alana yay\u0131lm\u0131\u015f bir sistemin kapat\u0131lamayaca\u011f\u0131n\u0131 biliyoruz. \u015eifreleme temelde \u00f6zel bir eylem oldu\u011fundan, Cypherpunk'lar kriptografi ile ilgili d\u00fczenlemelerden \u015fikayet\u00e7idir. \u015eifreleme eylemi asl\u0131nda bilgiyi kamusal alandan kald\u0131r\u0131r. Kriptografiye kar\u015f\u0131 yasalar bile ancak bir ulusun s\u0131n\u0131r\u0131na ve \u015fiddetinin koluna kadar ula\u015f\u0131r. Kriptografi ka\u00e7\u0131n\u0131lmaz olarak t\u00fcm d\u00fcnyaya ve onunla birlikte m\u00fcmk\u00fcn k\u0131ld\u0131\u011f\u0131 anonim i\u015flem sistemlerine yay\u0131lacakt\u0131r. Mahremiyetin yayg\u0131nla\u015fmas\u0131 i\u00e7in bir sosyal s\u00f6zle\u015fmenin par\u00e7as\u0131 olmas\u0131 gerekir. \u0130nsanlar bir araya gelmeli ve bu sistemleri ortak yarar i\u00e7in kullanmal\u0131d\u0131r. Mahremiyet ancak, ki\u015finin toplumdaki hemcinslerinin i\u015fbirli\u011fi kadar geni\u015fler. Biz Cypherpunk'lar sorular\u0131n\u0131z\u0131 ve endi\u015felerinizi ar\u0131yoruz ve kendimizi kand\u0131rmamak i\u00e7in sizi me\u015fgul edebilece\u011fimizi umuyoruz. Bununla birlikte, baz\u0131lar\u0131 hedeflerimize kat\u0131lmayabilece\u011finden, rotam\u0131zdan ayr\u0131lmayaca\u011f\u0131z. Cypherpunks, a\u011flar\u0131 gizlilik i\u00e7in daha g\u00fcvenli hale getirmek i\u00e7in aktif olarak \u00e7al\u0131\u015f\u0131yor. H\u0131zla birlikte ilerleyelim. \u0130leriye. Eric Hughes < hughes@soda.berkeley.edu > 9 Mart 1993","title":"Cypherpunk Manifesto"},{"location":"cyberpunk/#bir-cypherpunkn-manifestosu","text":"by Eric Hughes Elektronik \u00e7a\u011fda a\u00e7\u0131k bir toplum i\u00e7in mahremiyet gereklidir. Gizlilik gizlilik de\u011fildir. \u00d6zel bir konu, t\u00fcm d\u00fcnyan\u0131n bilmesini istemedi\u011fi bir \u015feydir, ancak gizli bir konu, kimsenin bilmesini istemedi\u011fi bir \u015feydir. Gizlilik, kendini d\u00fcnyaya se\u00e7ici olarak g\u00f6sterme g\u00fcc\u00fcd\u00fcr. E\u011fer iki taraf bir t\u00fcr anla\u015fmaya sahipse, her birinin etkile\u015fimlerinin bir an\u0131s\u0131 vard\u0131r. Her bir taraf bununla ilgili kendi an\u0131lar\u0131 hakk\u0131nda konu\u015fabilir; kimse bunu nas\u0131l engelleyebilir? Buna kar\u015f\u0131 yasalar \u00e7\u0131kar\u0131labilir, ancak konu\u015fma \u00f6zg\u00fcrl\u00fc\u011f\u00fc, mahremiyetten bile daha fazlas\u0131, a\u00e7\u0131k bir toplum i\u00e7in esast\u0131r; hi\u00e7bir konu\u015fmay\u0131 k\u0131s\u0131tlamamaya \u00e7al\u0131\u015f\u0131yoruz. Ayn\u0131 forumda bir\u00e7ok taraf birlikte konu\u015fursa, her biri di\u011ferleriyle konu\u015fabilir ve bireyler ve di\u011fer taraflar hakk\u0131ndaki bilgileri bir araya toplayabilir. Elektronik ileti\u015fimin g\u00fcc\u00fc bu t\u00fcr grup konu\u015fmalar\u0131n\u0131 m\u00fcmk\u00fcn k\u0131lm\u0131\u015ft\u0131r ve sadece biz isteyebilece\u011fimiz i\u00e7in ortadan kalkmayacakt\u0131r. Gizlili\u011fi arzulad\u0131\u011f\u0131m\u0131z i\u00e7in, bir i\u015flemin her bir taraf\u0131n\u0131n yaln\u0131zca o i\u015flem i\u00e7in do\u011frudan gerekli olan\u0131 bilmesini sa\u011flamal\u0131y\u0131z. Herhangi bir bilgi konu\u015fulabilece\u011finden, m\u00fcmk\u00fcn oldu\u011funca az a\u00e7\u0131klama yapt\u0131\u011f\u0131m\u0131zdan emin olmal\u0131y\u0131z. \u00c7o\u011fu durumda ki\u015fisel kimlik belirgin de\u011fildir. Bir ma\u011fazadan bir dergi sat\u0131n ald\u0131\u011f\u0131mda ve katiyere nakit verdi\u011fimde, kim oldu\u011fumu bilmeme gerek yok. Elektronik posta sa\u011flay\u0131c\u0131mdan mesaj g\u00f6nderip almas\u0131n\u0131 istedi\u011fimde, sa\u011flay\u0131c\u0131m\u0131n kiminle konu\u015ftu\u011fumu, ne s\u00f6yledi\u011fimi veya ba\u015fkalar\u0131n\u0131n bana ne s\u00f6yledi\u011fini bilmesine gerek yoktur; sa\u011flay\u0131c\u0131m\u0131n sadece mesaj\u0131 oraya nas\u0131l ula\u015ft\u0131raca\u011f\u0131n\u0131 ve onlara ne kadar \u00fccret bor\u00e7lu oldu\u011fumu bilmesi gerekiyor. Kimli\u011fim i\u015flemin alt\u0131nda yatan mekanizma taraf\u0131ndan if\u015fa edildi\u011finde, mahremiyetim yok. Burada se\u00e7ici olarak kendimi if\u015fa edemem; her zaman yapmal\u0131y\u0131m kendimi if\u015fa et. Bu nedenle, a\u00e7\u0131k bir toplumda mahremiyet, anonim i\u015flem sistemleri gerektirir. \u015eimdiye kadar, nakit bu t\u00fcr birincil sistem olmu\u015ftur. Anonim bir i\u015flem sistemi, gizli bir i\u015flem sistemi de\u011fildir. Anonim bir sistem, bireylere kimliklerini istendi\u011finde ve yaln\u0131zca istendi\u011finde if\u015fa etme yetkisi verir; mahremiyetin \u00f6z\u00fc budur. A\u00e7\u0131k bir toplumda gizlilik de kriptografi gerektirir. Bir \u015fey s\u00f6ylersem, sadece niyet etti\u011fim ki\u015filer taraf\u0131ndan duyulmas\u0131n\u0131 isterim. Konu\u015fmam\u0131n i\u00e7eri\u011fi d\u00fcnyaya a\u00e7\u0131ksa, mahremiyetim yok. \u015eifrelemek, mahremiyet arzusunu belirtmektir ve zay\u0131f \u015fifreleme ile \u015fifrelemek, mahremiyet i\u00e7in \u00e7ok fazla arzu olmad\u0131\u011f\u0131n\u0131 belirtmektir. Ayr\u0131ca, varsay\u0131lan anonimlik oldu\u011funda, ki\u015finin kimli\u011fini g\u00fcvence ile ortaya \u00e7\u0131karmak i\u00e7in kriptografik imza gerekir. H\u00fck\u00fcmetlerin, \u015firketlerin veya di\u011fer b\u00fcy\u00fck, kimli\u011fi belirsiz kurulu\u015flar\u0131n, kendi yararlar\u0131 d\u0131\u015f\u0131nda bize mahremiyet vermelerini bekleyemeyiz. Bizim hakk\u0131m\u0131zda konu\u015fmak onlar\u0131n yarar\u0131nad\u0131r ve konu\u015fmalar\u0131n\u0131 beklemeliyiz. Konu\u015fmalar\u0131n\u0131 engellemeye \u00e7al\u0131\u015fmak, bilgi ger\u00e7ekleriyle sava\u015fmakt\u0131r. Bilgi sadece \u00f6zg\u00fcr olmak istemez, \u00f6zg\u00fcr olmay\u0131 arzular. Bilgi, mevcut depolama alan\u0131n\u0131 dolduracak \u015fekilde geni\u015fler. Bilgi, Rumor'un daha gen\u00e7, daha g\u00fc\u00e7l\u00fc kuzenidir; Bilgi, daha h\u0131zl\u0131d\u0131r, daha fazla g\u00f6ze sahiptir, daha fazlas\u0131n\u0131 bilir ve S\u00f6ylentiden daha az anlar. Herhangi bir mahremiyete sahip olmay\u0131 umuyorsak, kendi mahremiyetimizi savunmal\u0131y\u0131z. Bir araya gelmeli ve anonim i\u015flemlerin ger\u00e7ekle\u015fmesine izin veren sistemler olu\u015fturmal\u0131y\u0131z. \u0130nsanlar y\u00fczy\u0131llard\u0131r f\u0131s\u0131lt\u0131lar, karanl\u0131k, zarflar, kapal\u0131 kap\u0131lar, gizli tokala\u015fmalar ve kuryelerle kendi mahremiyetlerini savunuyorlar. Ge\u00e7mi\u015fin teknolojileri g\u00fc\u00e7l\u00fc bir mahremiyete izin vermiyordu, ancak elektronik teknolojiler izin veriyor. Biz Cypherpunk'lar, kendimizi anonim sistemler olu\u015fturmaya adad\u0131k. Gizlili\u011fimizi kriptografi, anonim posta y\u00f6nlendirme sistemleri, dijital imzalar ve elektronik para ile koruyoruz. Cypherpunk'lar kod yazar. Birinin mahremiyeti savunmak i\u00e7in yaz\u0131l\u0131m yazmas\u0131 gerekti\u011fini biliyoruz ve hepimiz bunu yapmad\u0131k\u00e7a mahremiyet elde edemeyece\u011fimiz i\u00e7in yazaca\u011f\u0131z. Kodumuzu Cypherpunk arkada\u015flar\u0131m\u0131z\u0131n pratik yap\u0131p onunla oynayabilmeleri i\u00e7in yay\u0131nl\u0131yoruz. Kodumuz, d\u00fcnya \u00e7ap\u0131nda herkesin kullan\u0131m\u0131 i\u00e7in \u00fccretsizdir. Yazd\u0131\u011f\u0131m\u0131z yaz\u0131l\u0131m\u0131 onaylamaman\u0131z pek umurumuzda de\u011fil. Yaz\u0131l\u0131m\u0131n yok edilemeyece\u011fini ve geni\u015f bir alana yay\u0131lm\u0131\u015f bir sistemin kapat\u0131lamayaca\u011f\u0131n\u0131 biliyoruz. \u015eifreleme temelde \u00f6zel bir eylem oldu\u011fundan, Cypherpunk'lar kriptografi ile ilgili d\u00fczenlemelerden \u015fikayet\u00e7idir. \u015eifreleme eylemi asl\u0131nda bilgiyi kamusal alandan kald\u0131r\u0131r. Kriptografiye kar\u015f\u0131 yasalar bile ancak bir ulusun s\u0131n\u0131r\u0131na ve \u015fiddetinin koluna kadar ula\u015f\u0131r. Kriptografi ka\u00e7\u0131n\u0131lmaz olarak t\u00fcm d\u00fcnyaya ve onunla birlikte m\u00fcmk\u00fcn k\u0131ld\u0131\u011f\u0131 anonim i\u015flem sistemlerine yay\u0131lacakt\u0131r. Mahremiyetin yayg\u0131nla\u015fmas\u0131 i\u00e7in bir sosyal s\u00f6zle\u015fmenin par\u00e7as\u0131 olmas\u0131 gerekir. \u0130nsanlar bir araya gelmeli ve bu sistemleri ortak yarar i\u00e7in kullanmal\u0131d\u0131r. Mahremiyet ancak, ki\u015finin toplumdaki hemcinslerinin i\u015fbirli\u011fi kadar geni\u015fler. Biz Cypherpunk'lar sorular\u0131n\u0131z\u0131 ve endi\u015felerinizi ar\u0131yoruz ve kendimizi kand\u0131rmamak i\u00e7in sizi me\u015fgul edebilece\u011fimizi umuyoruz. Bununla birlikte, baz\u0131lar\u0131 hedeflerimize kat\u0131lmayabilece\u011finden, rotam\u0131zdan ayr\u0131lmayaca\u011f\u0131z. Cypherpunks, a\u011flar\u0131 gizlilik i\u00e7in daha g\u00fcvenli hale getirmek i\u00e7in aktif olarak \u00e7al\u0131\u015f\u0131yor. H\u0131zla birlikte ilerleyelim. \u0130leriye. Eric Hughes < hughes@soda.berkeley.edu > 9 Mart 1993","title":"Bir Cypherpunk'\u0131n Manifestosu"},{"location":"helpers/","text":"Helper Codes Remote Connection SSH ssh -J mert@{servername}.ddns.net:port mert@target SCP From Local scp -o \"ProxyJump mert@servername.ddns.net -p {port}\" test.txt mert@target-pc:/home/mert/ Download File From Remote Server scp -o \"ProxyJump mert@servername.ddns.net -p {port}\" mert@target-pc:/home/mert/ test.txt Various Download File wget --user-agent Mozilla/4.0 'big address' -O dest_file_name Rename Files ls -v | cat -n | while read n f; do mv -n \"$f\" \"$n.ext\"; done Extract Files 7za x test.7z String Slicing # From Character for f in raw_daily/*.csv; do echo $f /dimensions_\"${f#*blocks_}\"; done # TO Character for f in raw_daily/*.csv; do echo $f /dimensions_\"${f%*blocks_}\"; done Random File Name for i in *.jpg; do mv -i \"$i\" ${RANDOM}${RANDOM}.jpg; done Move Files for f in png-exports/*; do cp $f/*.png all_images; done Delete Files Recursively find e -maxdepth 10 -type f -name \".*\" -delete Get Dimensions from Folder ls -U | while read n; do identify -format \"%f,%w,%h\\n\" \"$n\"; done > file_size.csv FFMPEG MP3 \u2192 WAV for f in *.mp3; do ffmpeg -i \"$f\" -acodec pcm_s16le -ac 1 -ar 16000 \"wav-exports/${f%.}.wav\"; done for f in *.flac; do ffmpeg -i \"$f\" \"wav-exports/${f%.}.wav\"; done WAV \u2192 mp3 for f in *.*; do ffmpeg -i \"$f\" \"wav-exports/${f%.}.wav\"; done for f in *; do ffmpeg -i \"${f}\" -vn -ab 128k -ar 44100 -y \"${f}.mp3\" ; done PNG Sequence \u2192 MP4 ffmpeg -f image2 -r 30 -i image_%6d.png -vcodec libx264 -crf 18 -pix_fmt yuv420p output.mp4 MP4 \u2192 PNG ffmpeg -i test.mp4 -vf fps=1/2 png-exports/video13_%06d.png for f in *.mp4; do ffmpeg -i \"$f\" -vf fps=2 png-exports/${f%.*}_%06d.png; done MOV to Optimized GIF ffmpeg -i test.mov -vf scale=320:-1 -r 10 output/ffout%3d.png convert -delay 8 -loop 0 output/ffout*.png output/test.gif Image Convert all images in directory mogrify -format png *.* ESRGAN python inference_realesrgan.py -n RealESRGAN_x4plus -i v13 -s 3 --suffix 8k -t 1500 -o v13_out Utilities Delete all hidden Mac junk files in Windows (Like .DS_STORE) del /s /q /f /a .DS_STORE del /s /q /f /a ._.*","title":"Helper one-liners"},{"location":"helpers/#helper-codes","text":"","title":"Helper Codes"},{"location":"helpers/#remote-connection","text":"","title":"Remote Connection"},{"location":"helpers/#ssh","text":"ssh -J mert@{servername}.ddns.net:port mert@target","title":"SSH"},{"location":"helpers/#scp","text":"","title":"SCP"},{"location":"helpers/#from-local","text":"scp -o \"ProxyJump mert@servername.ddns.net -p {port}\" test.txt mert@target-pc:/home/mert/","title":"From Local"},{"location":"helpers/#download-file-from-remote-server","text":"scp -o \"ProxyJump mert@servername.ddns.net -p {port}\" mert@target-pc:/home/mert/ test.txt","title":"Download File From Remote Server"},{"location":"helpers/#various","text":"","title":"Various"},{"location":"helpers/#download-file","text":"wget --user-agent Mozilla/4.0 'big address' -O dest_file_name","title":"Download File"},{"location":"helpers/#rename-files","text":"ls -v | cat -n | while read n f; do mv -n \"$f\" \"$n.ext\"; done","title":"Rename Files"},{"location":"helpers/#extract-files","text":"7za x test.7z","title":"Extract Files"},{"location":"helpers/#string-slicing","text":"# From Character for f in raw_daily/*.csv; do echo $f /dimensions_\"${f#*blocks_}\"; done # TO Character for f in raw_daily/*.csv; do echo $f /dimensions_\"${f%*blocks_}\"; done","title":"String Slicing"},{"location":"helpers/#random-file-name","text":"for i in *.jpg; do mv -i \"$i\" ${RANDOM}${RANDOM}.jpg; done","title":"Random File Name"},{"location":"helpers/#move-files","text":"for f in png-exports/*; do cp $f/*.png all_images; done","title":"Move Files"},{"location":"helpers/#delete-files-recursively","text":"find e -maxdepth 10 -type f -name \".*\" -delete","title":"Delete Files Recursively"},{"location":"helpers/#get-dimensions-from-folder","text":"ls -U | while read n; do identify -format \"%f,%w,%h\\n\" \"$n\"; done > file_size.csv","title":"Get Dimensions from Folder"},{"location":"helpers/#ffmpeg","text":"","title":"FFMPEG"},{"location":"helpers/#mp3-wav","text":"for f in *.mp3; do ffmpeg -i \"$f\" -acodec pcm_s16le -ac 1 -ar 16000 \"wav-exports/${f%.}.wav\"; done for f in *.flac; do ffmpeg -i \"$f\" \"wav-exports/${f%.}.wav\"; done","title":"MP3 \u2192 WAV"},{"location":"helpers/#wav-mp3","text":"for f in *.*; do ffmpeg -i \"$f\" \"wav-exports/${f%.}.wav\"; done for f in *; do ffmpeg -i \"${f}\" -vn -ab 128k -ar 44100 -y \"${f}.mp3\" ; done","title":"WAV  \u2192 mp3"},{"location":"helpers/#png-sequence-mp4","text":"ffmpeg -f image2 -r 30 -i image_%6d.png -vcodec libx264 -crf 18 -pix_fmt yuv420p output.mp4","title":"PNG Sequence \u2192 MP4"},{"location":"helpers/#mp4-png","text":"ffmpeg -i test.mp4 -vf fps=1/2 png-exports/video13_%06d.png for f in *.mp4; do ffmpeg -i \"$f\" -vf fps=2 png-exports/${f%.*}_%06d.png; done","title":"MP4 \u2192 PNG"},{"location":"helpers/#mov-to-optimized-gif","text":"ffmpeg -i test.mov -vf scale=320:-1 -r 10 output/ffout%3d.png convert -delay 8 -loop 0 output/ffout*.png output/test.gif","title":"MOV to Optimized GIF"},{"location":"helpers/#image","text":"Convert all images in directory mogrify -format png *.*","title":"Image"},{"location":"helpers/#esrgan","text":"python inference_realesrgan.py -n RealESRGAN_x4plus -i v13 -s 3 --suffix 8k -t 1500 -o v13_out","title":"ESRGAN"},{"location":"helpers/#utilities","text":"","title":"Utilities"},{"location":"helpers/#delete-all-hidden-mac-junk-files-in-windows-like-ds_store","text":"del /s /q /f /a .DS_STORE del /s /q /f /a ._.*","title":"Delete all hidden Mac junk files in Windows (Like .DS_STORE)"},{"location":"instagram/","text":"Instagram Unfollowers Bu harika github projesiyle instagramdaki sizi geri takip etmeyen kisileri bulabilir ve kolayca takipten cikabilirsiniz. Bu projeye davidarroyo1234 'nun bu github reposundan ulasabilirsiniz. Asagidaki kodu kopyalayin \"use strict\";const INSTAGRAM_HOSTNAME=\"www.instagram.com\";let nonFollowersList=[],userIdsToUnfollow=[],isActiveProcess=!1;function sleep(e){return new Promise((n=>{setTimeout(n,e)}))}function getCookie(e){const n=`; ${document.cookie}`.split(`; ${e}=`);if(2===n.length)return n.pop().split(\";\").shift()}function afterUrlGenerator(e){return`https://www.instagram.com/graphql/query/?query_hash=3dec7e2c57367ef3da3d987d89f9dbc8&variables={\"id\":\"${getCookie(\"ds_user_id\")}\",\"include_reel\":\"true\",\"fetch_mutual\":\"false\",\"first\":\"24\",\"after\":\"${e}\"}`}function unfollowUserUrlGenerator(e){return`https://www.instagram.com/web/friendships/${e}/unfollow/`}function getElementByClass(e){const n=document.querySelector(e);if(null===n)throw new Error(`Unable to find element by class: ${e}`);return n}function getUserById(e){const n=nonFollowersList.find((n=>n.id.toString()===e.toString()));return void 0===n&&console.error(`Unable to find user by id: ${e}`),n}function copyListToClipboard(){const e=[...nonFollowersList].sort(((e,n)=>e.username>n.username?1:-1));let n=\"\";e.forEach((e=>{n+=e.username+\"\\n\"})),copyToClipboard(n)}async function copyToClipboard(e){await navigator.clipboard.writeText(e),alert(\"List copied to clipboard!\")}function onToggleUser(){getElementByClass(\".selected-user-count\").innerHTML=`[${userIdsToUnfollow.length}]`}function toggleUser(e){userIdsToUnfollow=-1===userIdsToUnfollow.indexOf(e)?[...userIdsToUnfollow,e]:userIdsToUnfollow.filter((n=>n!==e)),onToggleUser()}function toggleAllUsers(e=!1){document.querySelectorAll(\".account-checkbox\").forEach((n=>n.checked=e)),userIdsToUnfollow=e?nonFollowersList.map((e=>e.id)):[],onToggleUser()}function renderResults(e){const n=[...e].sort(((e,n)=>e.username>n.username?1:-1));getElementByClass(\".toggle-all-checkbox\").disabled=!1;const t=getElementByClass(\".results-container\");t.innerHTML=\"\";let o=\"\";n.forEach((e=>{const n=-1!==userIdsToUnfollow.indexOf(parseInt(e.id,10)),r=e.username.substring(0,1).toUpperCase();o!==r&&(o=r,t.innerHTML+=`<div class='alphabet-character'>${o}</div>`),t.innerHTML+=`<label class='result-item'>\\n <div class='flex grow align-center'>\\n <img class='avatar' src=${e.profile_pic_url} />&nbsp;&nbsp;&nbsp;&nbsp;\\n <div class='flex column'>\\n <a class='fs-xlarge' target='_blank' href='../${e.username}'>${e.username}</a>\\n <span class='fs-medium'>${e.full_name}</span>\\n </div>\\n ${e.is_verified?\"&nbsp;&nbsp;&nbsp;<div class='verified-badge'>\u2714</div>\":\"\"}\\n ${e.is_private?\"<div class='flex justify-center w-100'>\\n <span class='private-indicator'>Private</span>\\n </div>\":\"\"}\\n </div>\\n <input\\n class='account-checkbox'\\n type='checkbox'\\n onchange='toggleUser(${e.id})'\\n ${n?\"checked\":\"\"} />\\n </label>`}))}async function run(e){getElementByClass(\".run-scan\").remove(),getElementByClass(\".include-verified-checkbox\").disabled=!0,nonFollowersList=await getNonFollowersList(e),getElementByClass(\".copy-list\").disabled=!1}function renderOverlay(){let e=!0;document.body.innerHTML=`\\n <main class='iu'>\\n <div class='overlay'>\\n <header class='top-bar'>\\n <div class='logo' onclick='location.reload()'>InstagramUnfollowers</div>\\n <label class='flex align-center'>\\n <input type='checkbox' class='include-verified-checkbox' ${e?\"checked\":\"\"} /> Include verified\\n </label>\\n <button class='copy-list' onclick='copyListToClipboard()' disabled>COPY LIST</button>\\n <button class='fs-large clr-red' onclick='unfollow()'>UNFOLLOW <span class='selected-user-count'>[0]</span></button>\\n <input type='checkbox' class='toggle-all-checkbox' onclick='toggleAllUsers(this.checked)' disabled />\\n </header>\\n\\n <button class='run-scan'>RUN</button>\\n <div class='results-container'></div>\\n\\n <footer class='bottom-bar'>\\n <div>Non-followers: <span class='nonfollower-count' /></div>\\n <div class='sleeping-text'></div>\\n <div class='progressbar-container'>\\n <div class='progressbar-bar'></div>\\n <span class='progressbar-text'>0%</span>\\n </div>\\n </footer>\\n </div>\\n </main>`,getElementByClass(\".run-scan\").addEventListener(\"click\",(()=>run(e))),getElementByClass(\".include-verified-checkbox\").addEventListener(\"change\",(()=>e=!e))}async function getNonFollowersList(e=!0){if(isActiveProcess)return;let n=[],t=!0,o=0,r=0,s=-1;isActiveProcess=!0;let l=`https://www.instagram.com/graphql/query/?query_hash=3dec7e2c57367ef3da3d987d89f9dbc8&variables={\"id\":\"${getCookie(\"ds_user_id\")}\",\"include_reel\":\"true\",\"fetch_mutual\":\"false\",\"first\":\"24\"}`;const i=getElementByClass(\".progressbar-bar\"),a=getElementByClass(\".progressbar-text\"),c=getElementByClass(\".nonfollower-count\"),d=getElementByClass(\".sleeping-text\");for(;t;){let u;try{u=await fetch(l).then((e=>e.json()))}catch(e){console.error(e);continue}-1===s&&(s=u.data.user.edge_follow.count),t=u.data.user.edge_follow.page_info.has_next_page,l=afterUrlGenerator(u.data.user.edge_follow.page_info.end_cursor),r+=u.data.user.edge_follow.edges.length,u.data.user.edge_follow.edges.forEach((t=>{!e&&t.node.is_verified||t.node.follows_viewer||n.push(t.node)}));const f=`${Math.ceil(r/s*100)}%`;a.innerHTML=f,i.style.width=f,c.innerHTML=n.length.toString(),renderResults(n),await sleep(Math.floor(400*Math.random())+1e3),o++,o>6&&(o=0,d.style.display=\"block\",d.innerHTML=\"Sleeping 10 secs to prevent getting temp blocked...\",await sleep(1e4)),d.style.display=\"none\"}return i.style.backgroundColor=\"#59A942\",a.innerHTML=\"DONE\",isActiveProcess=!1,n}async function unfollow(){if(isActiveProcess)return;if(0===userIdsToUnfollow.length)return void alert(\"Must select at least a single user to unfollow\");if(!confirm(\"Are you sure?\"))return;let e=getCookie(\"csrftoken\");if(void 0===e)throw new Error(\"csrftoken cookie is undefined\");const n=getElementByClass(\".sleeping-text\"),t=getElementByClass(\".progressbar-bar\"),o=getElementByClass(\".progressbar-text\");getElementByClass(\".toggle-all-checkbox\").disabled=!0;const r=getElementByClass(\".results-container\");r.innerHTML=\"\";const s=()=>window.scrollTo(0,r.scrollHeight);o.innerHTML=\"0%\",t.style.width=\"0%\",isActiveProcess=!0;let l=0;for(const i of userIdsToUnfollow){const a=getUserById(i);try{await fetch(unfollowUserUrlGenerator(i),{headers:{\"content-type\":\"application/x-www-form-urlencoded\",\"x-csrftoken\":e},method:\"POST\",mode:\"cors\",credentials:\"include\"}),r.innerHTML+=`<div class='p-medium'>Unfollowed\\n <a class='clr-inherit' target='_blank' href='${a.username}'> ${a.username}</a>\\n <span class='clr-cyan'> [${l+1}/${userIdsToUnfollow.length}]</span>\\n </div>`}catch(e){console.error(e),r.innerHTML+=`<div class='p-medium clr-red'>Failed to unfollow ${a.username} [${l+1}/${userIdsToUnfollow.length}]</div>`}l+=1;const c=`${Math.ceil(l/userIdsToUnfollow.length*100)}%`;if(o.innerHTML=c,t.style.width=c,s(),i===userIdsToUnfollow[userIdsToUnfollow.length-1])break;await sleep(Math.floor(2e3*Math.random())+4e3),l%5==0&&(n.style.display=\"block\",n.innerHTML=\"Sleeping 5 minutes to prevent getting temp blocked...\",s(),await sleep(3e5)),n.style.display=\"none\"}t.style.backgroundColor=\"#59A942\",o.innerHTML=\"DONE\",isActiveProcess=!1,r.innerHTML+=\"<hr /><div class='fs-large p-medium clr-green'>All DONE!</div><hr />\",s()}function init(){location.hostname===INSTAGRAM_HOSTNAME?(document.title=\"InstagramUnfollowers\",renderOverlay()):alert(\"Can be used only on Instagram routes\")}window.addEventListener(\"beforeunload\",(e=>{if(isActiveProcess)return(e=e||window.event)&&(e.returnValue=\"Changes you made may not be saved.\"),\"Changes you made may not be saved.\"})),init();const styleMarkup=\"html {\\n background-color: #222 !important; }\\n html .iu {\\n margin-bottom: 10rem; }\\n html .iu .overlay {\\n background-color: #222;\\n color: #fff;\\n height: 100%;\\n font-family: system-ui; }\\n html .iu header.top-bar {\\n position: fixed;\\n top: 0;\\n left: 0;\\n right: 0;\\n display: flex;\\n align-items: center;\\n justify-content: space-between;\\n padding: 1rem;\\n height: 2.5rem;\\n background-color: #333;\\n z-index: 1; }\\n html .iu header .logo {\\n font-family: monospace;\\n font-size: 1.5em;\\n cursor: pointer; }\\n html .iu footer.bottom-bar {\\n position: fixed;\\n bottom: 0;\\n left: 0px;\\n right: 0px;\\n display: flex;\\n align-items: center;\\n justify-content: space-between;\\n padding: 1rem;\\n background-color: #000;\\n font-weight: bold;\\n z-index: 1; }\\n html .iu label {\\n cursor: pointer; }\\n html .iu input[type='checkbox'] {\\n height: 1.1rem;\\n width: 1.1rem; }\\n html .iu a {\\n color: inherit;\\n text-decoration-color: transparent;\\n transition: text-decoration-color 0.1s; }\\n html .iu a:hover {\\n text-decoration-color: inherit; }\\n html .iu button {\\n background: none;\\n border: none;\\n padding: 0;\\n cursor: pointer; }\\n html .iu button.copy-list {\\n color: white;\\n font-size: 1rem; }\\n html .iu button.run-scan {\\n position: absolute;\\n left: 50%;\\n top: 50%;\\n transform: translate(-50%, -50%);\\n font-size: 2em;\\n color: white;\\n border: 1px solid white;\\n height: 160px;\\n width: 160px;\\n border-radius: 50%; }\\n html .iu .progressbar-container {\\n width: 175px;\\n height: 30px;\\n border-radius: 5px;\\n position: relative;\\n border: 1px solid #7b7777; }\\n html .iu .progressbar-bar {\\n width: 0;\\n height: 100%;\\n background-color: #7b7777; }\\n html .iu .progressbar-text {\\n position: absolute;\\n top: 50%;\\n left: 50%;\\n transform: translate(-50%, -50%); }\\n html .iu .sleeping-text {\\n color: yellow; }\\n html .iu .results-container {\\n transform: translateY(75px); }\\n html .iu .results-container .alphabet-character {\\n margin: 1rem;\\n padding: 1rem;\\n font-size: 2em;\\n border-bottom: 1px solid #333; }\\n html .iu .results-container .result-item {\\n display: flex;\\n align-items: center;\\n padding: 1rem;\\n border-radius: 3px;\\n cursor: pointer; }\\n html .iu .results-container .result-item .avatar {\\n width: 75px;\\n border-radius: 50%; }\\n html .iu .results-container .result-item .verified-badge {\\n background-color: #49adf4;\\n border-radius: 50%;\\n padding: 0.2rem 0.3rem;\\n font-size: 0.45em;\\n height: fit-content;\\n transform: translateY(-15px); }\\n html .iu .results-container .result-item .private-indicator {\\n border: 2px solid #51bb42;\\n border-radius: 25px;\\n padding: 0.5rem;\\n color: #51bb42;\\n font-weight: 500; }\\n\\n/** HELPERS */\\n.flex {\\n display: flex; }\\n .flex.align-center {\\n align-items: center; }\\n .flex.justify-center {\\n justify-content: center; }\\n .flex.column {\\n flex-direction: column; }\\n\\n.grow {\\n flex: 1; }\\n\\n.w-100 {\\n width: 100%; }\\n\\n.p-small {\\n padding: 0.5rem; }\\n\\n.p-medium {\\n padding: 1rem; }\\n\\n.p-large {\\n padding: 1.5rem; }\\n\\n.p-xlarge {\\n padding: 2rem; }\\n\\n.fs-small {\\n font-size: 0.5rem; }\\n\\n.fs-medium {\\n font-size: 1rem; }\\n\\n.fs-large {\\n font-size: 1.5rem; }\\n\\n.fs-xlarge {\\n font-size: 2rem; }\\n\\n.clr-inherit {\\n color: inherit; }\\n\\n.clr-red {\\n color: #bd2121; }\\n\\n.clr-green {\\n color: #56d756; }\\n\\n.clr-cyan {\\n color: #00ffff; }\\n\",elStyle=document.createElement(\"style\");elStyle.innerHTML=styleMarkup,document.head.appendChild(elStyle); Instagram hesabiniza girin ve developer console'u acin Windows: Ctrl+Shift+J MacOS: \u2318+\u2325+I Kodu Console'a yapistirip calistirin Soyle bir ekran karsilayacak Calistirin ve takipten cikmak istediginiz kullanicilari tikleyip calistirin.","title":"Instagram"},{"location":"instagram/#instagram-unfollowers","text":"Bu harika github projesiyle instagramdaki sizi geri takip etmeyen kisileri bulabilir ve kolayca takipten cikabilirsiniz. Bu projeye davidarroyo1234 'nun bu github reposundan ulasabilirsiniz.","title":"Instagram Unfollowers"},{"location":"instagram/#asagidaki-kodu-kopyalayin","text":"\"use strict\";const INSTAGRAM_HOSTNAME=\"www.instagram.com\";let nonFollowersList=[],userIdsToUnfollow=[],isActiveProcess=!1;function sleep(e){return new Promise((n=>{setTimeout(n,e)}))}function getCookie(e){const n=`; ${document.cookie}`.split(`; ${e}=`);if(2===n.length)return n.pop().split(\";\").shift()}function afterUrlGenerator(e){return`https://www.instagram.com/graphql/query/?query_hash=3dec7e2c57367ef3da3d987d89f9dbc8&variables={\"id\":\"${getCookie(\"ds_user_id\")}\",\"include_reel\":\"true\",\"fetch_mutual\":\"false\",\"first\":\"24\",\"after\":\"${e}\"}`}function unfollowUserUrlGenerator(e){return`https://www.instagram.com/web/friendships/${e}/unfollow/`}function getElementByClass(e){const n=document.querySelector(e);if(null===n)throw new Error(`Unable to find element by class: ${e}`);return n}function getUserById(e){const n=nonFollowersList.find((n=>n.id.toString()===e.toString()));return void 0===n&&console.error(`Unable to find user by id: ${e}`),n}function copyListToClipboard(){const e=[...nonFollowersList].sort(((e,n)=>e.username>n.username?1:-1));let n=\"\";e.forEach((e=>{n+=e.username+\"\\n\"})),copyToClipboard(n)}async function copyToClipboard(e){await navigator.clipboard.writeText(e),alert(\"List copied to clipboard!\")}function onToggleUser(){getElementByClass(\".selected-user-count\").innerHTML=`[${userIdsToUnfollow.length}]`}function toggleUser(e){userIdsToUnfollow=-1===userIdsToUnfollow.indexOf(e)?[...userIdsToUnfollow,e]:userIdsToUnfollow.filter((n=>n!==e)),onToggleUser()}function toggleAllUsers(e=!1){document.querySelectorAll(\".account-checkbox\").forEach((n=>n.checked=e)),userIdsToUnfollow=e?nonFollowersList.map((e=>e.id)):[],onToggleUser()}function renderResults(e){const n=[...e].sort(((e,n)=>e.username>n.username?1:-1));getElementByClass(\".toggle-all-checkbox\").disabled=!1;const t=getElementByClass(\".results-container\");t.innerHTML=\"\";let o=\"\";n.forEach((e=>{const n=-1!==userIdsToUnfollow.indexOf(parseInt(e.id,10)),r=e.username.substring(0,1).toUpperCase();o!==r&&(o=r,t.innerHTML+=`<div class='alphabet-character'>${o}</div>`),t.innerHTML+=`<label class='result-item'>\\n <div class='flex grow align-center'>\\n <img class='avatar' src=${e.profile_pic_url} />&nbsp;&nbsp;&nbsp;&nbsp;\\n <div class='flex column'>\\n <a class='fs-xlarge' target='_blank' href='../${e.username}'>${e.username}</a>\\n <span class='fs-medium'>${e.full_name}</span>\\n </div>\\n ${e.is_verified?\"&nbsp;&nbsp;&nbsp;<div class='verified-badge'>\u2714</div>\":\"\"}\\n ${e.is_private?\"<div class='flex justify-center w-100'>\\n <span class='private-indicator'>Private</span>\\n </div>\":\"\"}\\n </div>\\n <input\\n class='account-checkbox'\\n type='checkbox'\\n onchange='toggleUser(${e.id})'\\n ${n?\"checked\":\"\"} />\\n </label>`}))}async function run(e){getElementByClass(\".run-scan\").remove(),getElementByClass(\".include-verified-checkbox\").disabled=!0,nonFollowersList=await getNonFollowersList(e),getElementByClass(\".copy-list\").disabled=!1}function renderOverlay(){let e=!0;document.body.innerHTML=`\\n <main class='iu'>\\n <div class='overlay'>\\n <header class='top-bar'>\\n <div class='logo' onclick='location.reload()'>InstagramUnfollowers</div>\\n <label class='flex align-center'>\\n <input type='checkbox' class='include-verified-checkbox' ${e?\"checked\":\"\"} /> Include verified\\n </label>\\n <button class='copy-list' onclick='copyListToClipboard()' disabled>COPY LIST</button>\\n <button class='fs-large clr-red' onclick='unfollow()'>UNFOLLOW <span class='selected-user-count'>[0]</span></button>\\n <input type='checkbox' class='toggle-all-checkbox' onclick='toggleAllUsers(this.checked)' disabled />\\n </header>\\n\\n <button class='run-scan'>RUN</button>\\n <div class='results-container'></div>\\n\\n <footer class='bottom-bar'>\\n <div>Non-followers: <span class='nonfollower-count' /></div>\\n <div class='sleeping-text'></div>\\n <div class='progressbar-container'>\\n <div class='progressbar-bar'></div>\\n <span class='progressbar-text'>0%</span>\\n </div>\\n </footer>\\n </div>\\n </main>`,getElementByClass(\".run-scan\").addEventListener(\"click\",(()=>run(e))),getElementByClass(\".include-verified-checkbox\").addEventListener(\"change\",(()=>e=!e))}async function getNonFollowersList(e=!0){if(isActiveProcess)return;let n=[],t=!0,o=0,r=0,s=-1;isActiveProcess=!0;let l=`https://www.instagram.com/graphql/query/?query_hash=3dec7e2c57367ef3da3d987d89f9dbc8&variables={\"id\":\"${getCookie(\"ds_user_id\")}\",\"include_reel\":\"true\",\"fetch_mutual\":\"false\",\"first\":\"24\"}`;const i=getElementByClass(\".progressbar-bar\"),a=getElementByClass(\".progressbar-text\"),c=getElementByClass(\".nonfollower-count\"),d=getElementByClass(\".sleeping-text\");for(;t;){let u;try{u=await fetch(l).then((e=>e.json()))}catch(e){console.error(e);continue}-1===s&&(s=u.data.user.edge_follow.count),t=u.data.user.edge_follow.page_info.has_next_page,l=afterUrlGenerator(u.data.user.edge_follow.page_info.end_cursor),r+=u.data.user.edge_follow.edges.length,u.data.user.edge_follow.edges.forEach((t=>{!e&&t.node.is_verified||t.node.follows_viewer||n.push(t.node)}));const f=`${Math.ceil(r/s*100)}%`;a.innerHTML=f,i.style.width=f,c.innerHTML=n.length.toString(),renderResults(n),await sleep(Math.floor(400*Math.random())+1e3),o++,o>6&&(o=0,d.style.display=\"block\",d.innerHTML=\"Sleeping 10 secs to prevent getting temp blocked...\",await sleep(1e4)),d.style.display=\"none\"}return i.style.backgroundColor=\"#59A942\",a.innerHTML=\"DONE\",isActiveProcess=!1,n}async function unfollow(){if(isActiveProcess)return;if(0===userIdsToUnfollow.length)return void alert(\"Must select at least a single user to unfollow\");if(!confirm(\"Are you sure?\"))return;let e=getCookie(\"csrftoken\");if(void 0===e)throw new Error(\"csrftoken cookie is undefined\");const n=getElementByClass(\".sleeping-text\"),t=getElementByClass(\".progressbar-bar\"),o=getElementByClass(\".progressbar-text\");getElementByClass(\".toggle-all-checkbox\").disabled=!0;const r=getElementByClass(\".results-container\");r.innerHTML=\"\";const s=()=>window.scrollTo(0,r.scrollHeight);o.innerHTML=\"0%\",t.style.width=\"0%\",isActiveProcess=!0;let l=0;for(const i of userIdsToUnfollow){const a=getUserById(i);try{await fetch(unfollowUserUrlGenerator(i),{headers:{\"content-type\":\"application/x-www-form-urlencoded\",\"x-csrftoken\":e},method:\"POST\",mode:\"cors\",credentials:\"include\"}),r.innerHTML+=`<div class='p-medium'>Unfollowed\\n <a class='clr-inherit' target='_blank' href='${a.username}'> ${a.username}</a>\\n <span class='clr-cyan'> [${l+1}/${userIdsToUnfollow.length}]</span>\\n </div>`}catch(e){console.error(e),r.innerHTML+=`<div class='p-medium clr-red'>Failed to unfollow ${a.username} [${l+1}/${userIdsToUnfollow.length}]</div>`}l+=1;const c=`${Math.ceil(l/userIdsToUnfollow.length*100)}%`;if(o.innerHTML=c,t.style.width=c,s(),i===userIdsToUnfollow[userIdsToUnfollow.length-1])break;await sleep(Math.floor(2e3*Math.random())+4e3),l%5==0&&(n.style.display=\"block\",n.innerHTML=\"Sleeping 5 minutes to prevent getting temp blocked...\",s(),await sleep(3e5)),n.style.display=\"none\"}t.style.backgroundColor=\"#59A942\",o.innerHTML=\"DONE\",isActiveProcess=!1,r.innerHTML+=\"<hr /><div class='fs-large p-medium clr-green'>All DONE!</div><hr />\",s()}function init(){location.hostname===INSTAGRAM_HOSTNAME?(document.title=\"InstagramUnfollowers\",renderOverlay()):alert(\"Can be used only on Instagram routes\")}window.addEventListener(\"beforeunload\",(e=>{if(isActiveProcess)return(e=e||window.event)&&(e.returnValue=\"Changes you made may not be saved.\"),\"Changes you made may not be saved.\"})),init();const styleMarkup=\"html {\\n background-color: #222 !important; }\\n html .iu {\\n margin-bottom: 10rem; }\\n html .iu .overlay {\\n background-color: #222;\\n color: #fff;\\n height: 100%;\\n font-family: system-ui; }\\n html .iu header.top-bar {\\n position: fixed;\\n top: 0;\\n left: 0;\\n right: 0;\\n display: flex;\\n align-items: center;\\n justify-content: space-between;\\n padding: 1rem;\\n height: 2.5rem;\\n background-color: #333;\\n z-index: 1; }\\n html .iu header .logo {\\n font-family: monospace;\\n font-size: 1.5em;\\n cursor: pointer; }\\n html .iu footer.bottom-bar {\\n position: fixed;\\n bottom: 0;\\n left: 0px;\\n right: 0px;\\n display: flex;\\n align-items: center;\\n justify-content: space-between;\\n padding: 1rem;\\n background-color: #000;\\n font-weight: bold;\\n z-index: 1; }\\n html .iu label {\\n cursor: pointer; }\\n html .iu input[type='checkbox'] {\\n height: 1.1rem;\\n width: 1.1rem; }\\n html .iu a {\\n color: inherit;\\n text-decoration-color: transparent;\\n transition: text-decoration-color 0.1s; }\\n html .iu a:hover {\\n text-decoration-color: inherit; }\\n html .iu button {\\n background: none;\\n border: none;\\n padding: 0;\\n cursor: pointer; }\\n html .iu button.copy-list {\\n color: white;\\n font-size: 1rem; }\\n html .iu button.run-scan {\\n position: absolute;\\n left: 50%;\\n top: 50%;\\n transform: translate(-50%, -50%);\\n font-size: 2em;\\n color: white;\\n border: 1px solid white;\\n height: 160px;\\n width: 160px;\\n border-radius: 50%; }\\n html .iu .progressbar-container {\\n width: 175px;\\n height: 30px;\\n border-radius: 5px;\\n position: relative;\\n border: 1px solid #7b7777; }\\n html .iu .progressbar-bar {\\n width: 0;\\n height: 100%;\\n background-color: #7b7777; }\\n html .iu .progressbar-text {\\n position: absolute;\\n top: 50%;\\n left: 50%;\\n transform: translate(-50%, -50%); }\\n html .iu .sleeping-text {\\n color: yellow; }\\n html .iu .results-container {\\n transform: translateY(75px); }\\n html .iu .results-container .alphabet-character {\\n margin: 1rem;\\n padding: 1rem;\\n font-size: 2em;\\n border-bottom: 1px solid #333; }\\n html .iu .results-container .result-item {\\n display: flex;\\n align-items: center;\\n padding: 1rem;\\n border-radius: 3px;\\n cursor: pointer; }\\n html .iu .results-container .result-item .avatar {\\n width: 75px;\\n border-radius: 50%; }\\n html .iu .results-container .result-item .verified-badge {\\n background-color: #49adf4;\\n border-radius: 50%;\\n padding: 0.2rem 0.3rem;\\n font-size: 0.45em;\\n height: fit-content;\\n transform: translateY(-15px); }\\n html .iu .results-container .result-item .private-indicator {\\n border: 2px solid #51bb42;\\n border-radius: 25px;\\n padding: 0.5rem;\\n color: #51bb42;\\n font-weight: 500; }\\n\\n/** HELPERS */\\n.flex {\\n display: flex; }\\n .flex.align-center {\\n align-items: center; }\\n .flex.justify-center {\\n justify-content: center; }\\n .flex.column {\\n flex-direction: column; }\\n\\n.grow {\\n flex: 1; }\\n\\n.w-100 {\\n width: 100%; }\\n\\n.p-small {\\n padding: 0.5rem; }\\n\\n.p-medium {\\n padding: 1rem; }\\n\\n.p-large {\\n padding: 1.5rem; }\\n\\n.p-xlarge {\\n padding: 2rem; }\\n\\n.fs-small {\\n font-size: 0.5rem; }\\n\\n.fs-medium {\\n font-size: 1rem; }\\n\\n.fs-large {\\n font-size: 1.5rem; }\\n\\n.fs-xlarge {\\n font-size: 2rem; }\\n\\n.clr-inherit {\\n color: inherit; }\\n\\n.clr-red {\\n color: #bd2121; }\\n\\n.clr-green {\\n color: #56d756; }\\n\\n.clr-cyan {\\n color: #00ffff; }\\n\",elStyle=document.createElement(\"style\");elStyle.innerHTML=styleMarkup,document.head.appendChild(elStyle);","title":"Asagidaki kodu kopyalayin"},{"location":"instagram/#instagram-hesabiniza-girin-ve-developer-consoleu-acin","text":"Windows: Ctrl+Shift+J MacOS: \u2318+\u2325+I","title":"Instagram hesabiniza girin ve developer console'u acin"},{"location":"instagram/#kodu-consolea-yapistirip-calistirin","text":"","title":"Kodu Console'a yapistirip calistirin"},{"location":"instagram/#soyle-bir-ekran-karsilayacak","text":"Calistirin ve takipten cikmak istediginiz kullanicilari tikleyip calistirin.","title":"Soyle bir ekran karsilayacak"},{"location":"nerf/","text":"NeRF Docker Setup Prerequisites Docker Latest nvidia drivers CUDA 11.8 capable GPU Install Pull latest docker image docker pull dromni/nerfstudio:<version_number> You can see the latest tag in this link https://hub.docker.com/r/dromni/nerfstudio/tags # example command docker pull dromni/nerfstudio:0.1.19 Run docker image docker run --gpus all -v /folder/of/your/data:/workspace/ -v /home/<YOUR_USER>/cache/:/home/user/.cache/ -p 7007:7007 --rm -it dromni/nerfstudio:0.1.13 Example for Windows: docker run --gpus all -v D:\\nerf:/workspace/ -v D:\\nerf/:/home/user/.cache/ -p 7007:7007 --rm -it dromni/nerfstudio:0.1.19 - Give the container access to nvidia GPU (required). - Mount a folder from the local machine into the container to be able to process them (required). - Mount cache folder to avoid re-downloading of models everytime (recommended). - Map port from local machine to docker container (required to access the web interface UI). - Remove container after it is closed (recommended). - Start container in interactive mode. - Docker image name Training First Model Don't forget to change the directory cd /workspace Be sure to do everything you do after this point in the /workspace directory. Start Training ns-download-data nerfstudio --capture-name=poster Because this code is often used too much, google drive can put a download restriction, read the error message and try to download the file in the link with your browser. There are also different dataset you can use, to show them run this code ns-download-data nerfstudio --help bww_entrance, campanile, desolation, library, poster, redwoods2, storefront, vegetation, Egypt, person, kitchen, plane, dozer, floating-tree, aspen, stump, sculpture, Giannini-Hall, all, nerfstudio-dataset After select one of the dataset you can download as well with following command ns-download-data nerfstudio --capture-name=bmw_entrance You will see all the downloaded datasets in this directory D:\\nerf\\data\\nerfstudio Explore NeRF Studio Start training with the following command, you will need change the datapath to your dataset folder ns-train nerfacto --data data/nerfstudio/poster or ns-train nerfacto --data data/nerfstudio/bww_entrance If everything is correct, ideal output would be like this. A web page similar to the link below will appear on your terminal screen, open this page and continue. https://viewer.nerf.studio/versions/22-12-02-0/?websocket_url=ws://localhost:7007 Resume & Stop Training You can stop the training with the Ctrl + c shortcut. If you want to continue the training from where it left off, run the code below. ns-train nerfacto --data data/nerfstudio/poster --load-dir {outputs/.../nerfstudio_models} Example code ns-train nerfacto --data data/nerfstudio/bww_entrance/ --load-dir outputs/bww_entrance/nerfacto/2023-03-14_183456/nerfstudio_models/ Visualize Only (without training) ns-viewer --load-config {outputs/.../config.yml} # example code ns-viewer --load-config outputs/bww_entrance/nerfacto/2023-03-14_183456/config.yml Training on Custom Data Put your files in the main directory of the volume you opened, you can also keep them in a folder in the main directory. ns-process-data {video,images,polycam,insta360,record3d} --data {DATA_PATH} --output-dir {PROCESSED_DATA_DIR} # example code ns-process-data video --data raw_video/forest_walk.mp4 --output-dir forest_walk_processed Arguments: ns-process-data images [-h] --data PATH --output-dir PATH [--camera-type {perspective,fisheye}] [--matching-method {exhaustive,sequential,vocab_tree}] [--sfm-tool {any,colmap,hloc}] [--feature-type {any,sift,superpoint,superpoint_aachen,superpoint_max,superpoint_inloc,r2d2,d2 net-ss,sosnet,disk}] [--matcher-type {any,NN,superglue,superglue-fast,NN-superpoint,NN-ratio,NN-mutual,adalam}] [--num-downscales INT] [--skip-colmap] [--colmap-cmd STR] [--no-gpu] [--verbose] ns-train nerfacto --data {PROCESSED_DATA_DIR} # example code ns-train nerfacto --data forest_walk_processed/ Example It points to the same place in the following two paths. one its location on the host and the other its location inside the container. Raw File Path: D:\\nerf\\raw_data\\IMG_6070.MOV /workspace/raw_data/IMG_6070.MOV Target Folder Path: D:\\nerf\\outputs\\test_video /workspace/outputs/bank Preparing ns-process-data video --data /workspace/raw_data/IMG_6070.MOV --output-dir /workspace/outputs/bank Training ns-train nerfacto --data outputs/bank KIRI Engine Coming soon...","title":"NeRF"},{"location":"nerf/#nerf-docker-setup","text":"","title":"NeRF Docker Setup"},{"location":"nerf/#prerequisites","text":"Docker Latest nvidia drivers CUDA 11.8 capable GPU","title":"Prerequisites"},{"location":"nerf/#install","text":"Pull latest docker image docker pull dromni/nerfstudio:<version_number> You can see the latest tag in this link https://hub.docker.com/r/dromni/nerfstudio/tags # example command docker pull dromni/nerfstudio:0.1.19 Run docker image docker run --gpus all -v /folder/of/your/data:/workspace/ -v /home/<YOUR_USER>/cache/:/home/user/.cache/ -p 7007:7007 --rm -it dromni/nerfstudio:0.1.13 Example for Windows: docker run --gpus all -v D:\\nerf:/workspace/ -v D:\\nerf/:/home/user/.cache/ -p 7007:7007 --rm -it dromni/nerfstudio:0.1.19 - Give the container access to nvidia GPU (required). - Mount a folder from the local machine into the container to be able to process them (required). - Mount cache folder to avoid re-downloading of models everytime (recommended). - Map port from local machine to docker container (required to access the web interface UI). - Remove container after it is closed (recommended). - Start container in interactive mode. - Docker image name","title":"Install"},{"location":"nerf/#training-first-model","text":"","title":"Training First Model"},{"location":"nerf/#dont-forget-to-change-the-directory","text":"cd /workspace Be sure to do everything you do after this point in the /workspace directory.","title":"Don't forget to change the directory"},{"location":"nerf/#start-training","text":"ns-download-data nerfstudio --capture-name=poster Because this code is often used too much, google drive can put a download restriction, read the error message and try to download the file in the link with your browser. There are also different dataset you can use, to show them run this code ns-download-data nerfstudio --help bww_entrance, campanile, desolation, library, poster, redwoods2, storefront, vegetation, Egypt, person, kitchen, plane, dozer, floating-tree, aspen, stump, sculpture, Giannini-Hall, all, nerfstudio-dataset After select one of the dataset you can download as well with following command ns-download-data nerfstudio --capture-name=bmw_entrance You will see all the downloaded datasets in this directory D:\\nerf\\data\\nerfstudio","title":"Start Training"},{"location":"nerf/#explore-nerf-studio","text":"Start training with the following command, you will need change the datapath to your dataset folder ns-train nerfacto --data data/nerfstudio/poster or ns-train nerfacto --data data/nerfstudio/bww_entrance If everything is correct, ideal output would be like this. A web page similar to the link below will appear on your terminal screen, open this page and continue. https://viewer.nerf.studio/versions/22-12-02-0/?websocket_url=ws://localhost:7007","title":"Explore NeRF Studio"},{"location":"nerf/#resume-stop-training","text":"You can stop the training with the Ctrl + c shortcut. If you want to continue the training from where it left off, run the code below. ns-train nerfacto --data data/nerfstudio/poster --load-dir {outputs/.../nerfstudio_models} Example code ns-train nerfacto --data data/nerfstudio/bww_entrance/ --load-dir outputs/bww_entrance/nerfacto/2023-03-14_183456/nerfstudio_models/","title":"Resume &amp; Stop Training"},{"location":"nerf/#visualize-only-without-training","text":"ns-viewer --load-config {outputs/.../config.yml} # example code ns-viewer --load-config outputs/bww_entrance/nerfacto/2023-03-14_183456/config.yml","title":"Visualize Only (without training)"},{"location":"nerf/#training-on-custom-data","text":"Put your files in the main directory of the volume you opened, you can also keep them in a folder in the main directory. ns-process-data {video,images,polycam,insta360,record3d} --data {DATA_PATH} --output-dir {PROCESSED_DATA_DIR} # example code ns-process-data video --data raw_video/forest_walk.mp4 --output-dir forest_walk_processed Arguments: ns-process-data images [-h] --data PATH --output-dir PATH [--camera-type {perspective,fisheye}] [--matching-method {exhaustive,sequential,vocab_tree}] [--sfm-tool {any,colmap,hloc}] [--feature-type {any,sift,superpoint,superpoint_aachen,superpoint_max,superpoint_inloc,r2d2,d2 net-ss,sosnet,disk}] [--matcher-type {any,NN,superglue,superglue-fast,NN-superpoint,NN-ratio,NN-mutual,adalam}] [--num-downscales INT] [--skip-colmap] [--colmap-cmd STR] [--no-gpu] [--verbose] ns-train nerfacto --data {PROCESSED_DATA_DIR} # example code ns-train nerfacto --data forest_walk_processed/","title":"Training on Custom Data"},{"location":"nerf/#example","text":"It points to the same place in the following two paths. one its location on the host and the other its location inside the container. Raw File Path: D:\\nerf\\raw_data\\IMG_6070.MOV /workspace/raw_data/IMG_6070.MOV Target Folder Path: D:\\nerf\\outputs\\test_video /workspace/outputs/bank Preparing ns-process-data video --data /workspace/raw_data/IMG_6070.MOV --output-dir /workspace/outputs/bank Training ns-train nerfacto --data outputs/bank","title":"Example"},{"location":"nerf/#kiri-engine","text":"Coming soon...","title":"KIRI Engine"},{"location":"new-macbook/","text":"Install XCode sudo xcode-select --install Install Brew. /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" Brew Services brew install tree wget vim fig git htop imagemagick ffmpeg tmux brew install --cask iterm2 visual-studio-code Install ZSH. sudo apt install zsh-autosuggestions zsh-syntax-highlighting zsh Install Oh my ZSH. sh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\" iTerm2 Settings Preferences > Profiles > Keys > Presets > Natural Text Editing Install Python Dev Environment Install Anaconda curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh sh Miniconda3-latest-MacOSX-arm64.sh Initialize on Terminal if needed conda init source ~/.zshrc Install popular libraries python -m pip install --upgrade pip pip install pandas numpy matplotlib seaborn scikit-learn requests PyTorch Env Create environment for PyTorch conda create --name torchenv python=3.9 activate torchenv Install Dependencies brew install gcc conda install astunparse numpy ninja pyyaml setuptools cmake cffi typing_extensions future six requests dataclasses conda install pkg-config libuv MPS acceleration is available on MacOS 12.3+ conda install pytorch torchvision torchaudio -c pytorch-nightly if you get error conda install pytorch -c pytorch-nightly pip install --pre torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu Check if everyting is okay python import torch; torch.backends.mps.is_available() # True","title":"New Macbook Installation"},{"location":"new-macbook/#install-xcode","text":"sudo xcode-select --install","title":"Install XCode"},{"location":"new-macbook/#install-brew","text":"/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"","title":"Install Brew."},{"location":"new-macbook/#brew-services","text":"brew install tree wget vim fig git htop imagemagick ffmpeg tmux brew install --cask iterm2 visual-studio-code","title":"Brew Services"},{"location":"new-macbook/#install-zsh","text":"sudo apt install zsh-autosuggestions zsh-syntax-highlighting zsh","title":"Install ZSH."},{"location":"new-macbook/#install-oh-my-zsh","text":"sh -c \"$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"","title":"Install Oh my ZSH."},{"location":"new-macbook/#iterm2-settings","text":"Preferences > Profiles > Keys > Presets > Natural Text Editing","title":"iTerm2 Settings"},{"location":"new-macbook/#install-python-dev-environment","text":"Install Anaconda curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh sh Miniconda3-latest-MacOSX-arm64.sh Initialize on Terminal if needed conda init source ~/.zshrc Install popular libraries python -m pip install --upgrade pip pip install pandas numpy matplotlib seaborn scikit-learn requests","title":"Install Python Dev Environment"},{"location":"new-macbook/#pytorch-env","text":"Create environment for PyTorch conda create --name torchenv python=3.9 activate torchenv Install Dependencies brew install gcc conda install astunparse numpy ninja pyyaml setuptools cmake cffi typing_extensions future six requests dataclasses conda install pkg-config libuv MPS acceleration is available on MacOS 12.3+ conda install pytorch torchvision torchaudio -c pytorch-nightly if you get error conda install pytorch -c pytorch-nightly pip install --pre torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu Check if everyting is okay python import torch; torch.backends.mps.is_available() # True","title":"PyTorch Env"},{"location":"portfolio/","text":"Hey Cobanov Chrome Little Girls Women Cartoon Concept Art Cosmic People Line-art WLOP Coral & Glacier","title":"Hey"},{"location":"portfolio/#hey","text":"","title":"Hey"},{"location":"portfolio/#cobanov","text":"","title":"Cobanov"},{"location":"portfolio/#chrome","text":"","title":"Chrome"},{"location":"portfolio/#little-girls","text":"","title":"Little Girls"},{"location":"portfolio/#women","text":"","title":"Women"},{"location":"portfolio/#cartoon","text":"","title":"Cartoon"},{"location":"portfolio/#concept-art","text":"","title":"Concept Art"},{"location":"portfolio/#cosmic-people","text":"","title":"Cosmic People"},{"location":"portfolio/#line-art","text":"","title":"Line-art"},{"location":"portfolio/#wlop","text":"","title":"WLOP"},{"location":"portfolio/#coral-glacier","text":"","title":"Coral &amp; Glacier"},{"location":"python-conf/","text":"Source : https://whalesalad.com/blog/doing-python-configuration-right Doing Python Configuration Right Let's talk about configuring Python applications, specifically the kind that might live in multiple environments \u2013 dev, stage, production, etc... The tools and frameworks used in the application are not super important because the approach that I will outline below is based on vanilla Python. The impetus for this approach was caused by frustration with Django's settings , but this is my go-to for any kind of Python application I might be working on. Recap: Python Modules & Packages One of my favorite Python features is the way that the files and directories your application is made of map one-to-one with how you import and use them in code. For example, given this import statement: from app.utils import numbers We can infer the following directory structure: app \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 database.py \u251c\u2500\u2500 services.py \u251c\u2500\u2500 ... \u2514\u2500\u2500 utils \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 numbers.py Lots of languages and frameworks rely on this novel concept, including Clojure and ES6. In our example, Python considers the utils directory a Package . A directory becomes a package as soon as you place an empty __init__.py inside of it. A common scenario you might encounter as a Python hacker is one where you have a utils.py file that eventually gets too large, so you break it out into a utils/ directory containing many smaller files. When met with this situation we might do the following: # Create a new package: mkdir utils && touch utils/__init__.py # Move our existing code into the new package mv utils.py utils/something.py So now we've seen that a Python package is decided by the existence of an empty __init__.py file in a directory... but what if the file isn't empty ? Putting Code in __init__.py Since it's just a regular old Python file, you can actually put whatever you want there and it will be executed the first time the package is imported. You can test this out on your own. Create a directory named foo and give it an empty __init__.py file. From a Python REPL in the same directory: Seeing no output here is good, it means the statement was successful. Now let's edit our __init__.py file to include the following code: sys.exit() is typically used to cause a process to exit with a specific status . Rerunning the same experiment in a new REPL you will observe that your Python shell immediately exits after the import. In a larger application the effect would be more pronounced: the whole application would exit. So we understand the fundamentals and we've seen how this feature can be used maliciously. Perhaps we can use it for good? Multiple Environments & Twelve-Factor Apps It's likely that your application has lived in multiple environments. Your local development environment is likely the first, and you might have a test environment that lives on Jenkins or another CI platform. Your code is deployed to a production or live environment. Some systems might have a staging environment that is used just before things go live. Even if you only consider yourself a hobbyist, developing code locally and deploying it to a VPS or Heroku-like platform means you're dealing with multiple environments. A rule I follow when building applications is that I should be able to deploy a codebase \u2013 without modification \u2013 into any environment , assuming we have a way to tell the system where it's running. Contrast this to building multiple artifacts for each deployment target, requiring additional time and complexity to build and persist. These artifacts are typically designed to run in a single target environment, so running them locally or in a test mode is often difficult or impossible. The famous twelve-factor methodology shares this belief, in addition to the idea that all configuration should exist as environment variables, too. I agree with this to an extent, but there is sometimes a tendency to make everything an environment variable which quickly becomes difficult to support. If every knob and dial of your system is an environment variable, you'll find that you end up keeping various permutations of variables stored somewhere for running or debugging. See the problem here? We've pulled config out of one area (the code, something that is typically kept in version control) and moved them to an area that is more prone to error and human mistakes. The general guidelines I use to decide where to draw the line: Static things that don't change often, or things that dramatically influence the behavior of the system should live in the code. Dynamic things that change frequently, or things that should be kept secret (API keys/credentials) should live outside the code. How do we Switch Environments? In order for an application to change its behavior between environments we need a way to tell it where it is running. Leaning on environment variables (see a pattern?), I tend to use ENV (or a variation) for this purpose. The Ruby/Rails ecosystem uses RACK_ENV or RAILS_ENV Javascript projects will oftentimes leverage NODE_ENV I recently completed a project for a client with the following convention: My local development environment does not set an ENV variable, so the system infers development by default. The test environment on AWS CodePipeline uses ENV=test The production environment on EC2 uses ENV=production Note: It's important to consider the consequences of not setting this variable. Could that be catastrophic? For instance, could the app boot in DEV mode inside of a production cluster and end up showing tracebacks to the public? For some applications, the default should be production. There is no right or wrong answer here, but it needs to be considered. The End Goal From a developer standpoint we want to access our config like so: from service.config import AWS_S3_BUCKET The import line above doesn't contain anything that would suggest which environment we are in. We don't see the word development or production anywhere. Instead, we just import what we need and allow the configuration system to decide where that comes from. We're leveraging the filesystem and the language itself to provide an API for reading configuration. Behind the curtain, this is what the config directory looks like on disk: service/config \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 common.py \u2514\u2500\u2500 environments \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 development.py \u251c\u2500\u2500 production.py \u2514\u2500\u2500 staging.py common.py contains all of our common or shared configuration. These are things that don't significantly differ from one environment to the next. You could call this base or shared if you'd like. environments/development.py contains development configuration. This file could be excluded from version control so that each developer on the team can implement his or her own configuration settings. environments/(production|staging).py include configuration unique to each of their respective environments. Let's take a look inside common.py : import os APP_NAME = \"My Application\" # Conveniences to always have a quick reference to the # top-level application directory. ROOT_DIR = os.path.join( os.path.dirname(os.path.realpath(__file__)), os.pardir, os.pardir, ) SERVICE_DIR = os.path.join(ROOT_DIR, \"service\") # Used in DNS lookup jobs. DEFAULT_NAMESERVERS = [ \"1.1.1.1\", \"1.0.0.1\" ] # Job runner behaviors JOB_SUPERVISOR_SLEEP_SECONDS = 1 JOB_STATUS_TTL = 10 # AWS Configuration AWS_DYNAMO_REGION = 'us-west-1' AWS_DYNAMO_TABLE_PREFIX = 'acme-' AWS_S3_BUCKET = 'acme-production' SYSTEM_REBOOT_COMMAND = \"sudo systemctl restart foo.service\" This is a contrived example so don't read too deeply into the specifics. The important thing to notice is that this is fairly static configuration that is not going to change very often. Now let's look at environment/development.py : from ..common import * AWS_DYNAMO_TABLE_PREFIX = 'acme-dev-mwhalen-' AWS_S3_BUCKET = 'acme-dev-mwhalen' GOOGLE_CLIENT_ID = \"XXXX\" GOOGLE_CLIENT_SECRET = \"XXXX\" GOOGLE_CLIENT_REFRESH_TOKEN = \"XXXX\" # We're inserting our own DNS servers to the front of the defaults. DEFAULT_NAMESERVERS = [ \"10.0.0.3\" ] + DEFAULT_NAMESERVERS # This is intentionally a no-op command. # Our specific application is designed to be supervised by systemd, # but this is not available on macOS. SYSTEM_REBOOT_COMMAND = \"uname -a\" We start by importing the common configuration so that we inherit all of the common configuration by default. Now we have the ability to add, replace or augment parameters without the need to copy-paste from the parent. To support local development, I can customize the AWS resources being used in my environment. The rest of the system is unchanged, but now my local system is using my own tables in Dynamo as well as my own S3 bucket. Because this file is not in version control I can confidently store secrets such as my own GOOGLE_CLIENT_ credentials. Because there is access to the common DEFAULT_NAMESERVERS I have the ability extend them versus copy-pasting whatever the common values are into my own configuration. In production systemd commands are used to restart the application in response to certain admin actions. Because my Mac doesn't have systemd , I avoid that problem entirely by replacing the system reboot command with a simple no-op. How it Works Circling back to our config/__init__.py file, what could we implement here to make this possible? It's actually quite straightforward: import os import importlib # Determine the environment we want to load, default is development.py ENV = os.environ.get(\"ENV\", \"development\") module = importlib.import_module( f\".environments.{ENV}\", package=\"service.config\" ) # update globals of this module (i.e. settings) with imported. globals().update(vars(module)) def is_development_env(): return ENV == \"development\" def is_production_env(): return ENV == \"production\" We're leveraging import-time evaluation to dynamically fetch the necessary configuration from the corresponding child environment. Let's step through it piece by piece: First we import the importlib module ( docs ) which gives us some handy tools for importing code with code. Using the convention we established \u2013 the ENV environment variable \u2013 we grab the name of the environment we're currently running in. We choose development as the default if one is not set, but as noted earlier this decision will vary depending on the system. We might even consider preventing our application from starting unless this variable is defined. Here is an example of how that could work: ``` ENV = os.environ.get(\"ENV\", None) if ENV is None: raise Exception(\"The ENV environment variable must be set!\") ``` Next we use the importlib.import_module function to load the module containing our specific environment's code into a local variable, module . Finally, we update the globals of this module merging in the ones from the development.py file. At the end you will see a few conveniences (a-la Rails) to make it easier to toggle specific logic based on the environment. These are kept as functions so that they isolate implementation to this module instead of wherever its being used. This approach was heavily inspired by Ruby on Rails configuration which achieves a very similar outward appearance albeit with a different under-the-hood implementation. A Real World Example To provide another example of this in action, below is the configuration for this website: First, here is the exact directory structure of my config directory: config \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 base.py \u2514\u2500\u2500 environments \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 development.py \u251c\u2500\u2500 production.py \u2514\u2500\u2500 test.py development.py is used locally production.py is used on Heroku test.py is used for local unit tests with pytest # config/environments/base.py import os import logging import pytz LOG_FORMAT = '[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s' ROOT_DIR = os.path.join( os.path.dirname(os.path.realpath(__file__)), os.pardir, os.pardir ) SYSTEM_DIR = os.path.join(ROOT_DIR, 'system') BLOG_DIR = os.path.join(ROOT_DIR, 'blog') SASS_DIR = os.path.join(ROOT_DIR, 'static', 'sass') TZ = pytz.timezone('America/Los_Angeles') def relative_to_root(path): return os.path.abspath(os.path.join(ROOT_DIR, path)) DEFAULT_SITE_TITLE = 'Michael Whalen \u2013 whalesalad.com' base.py contains fairly static configuration: A centralized log format to use elsewhere in the project. Common directories and a helper function to make path-related work easier. The timezone for my service. The default title to use when a page doesn't provide it's own. # config/environments/development.py import os from ..base import * DEFAULT_SITE_TITLE = f'[DEV] {DEFAULT_SITE_TITLE}' REDIS = { 'host': os.environ.get('REDIS_HOST', 'localhost'), 'port': os.environ.get('REDIS_PORT', 6379), 'db': os.environ.get('REDIS_DB', 0), 'socket_timeout': 120 } In development.py , the site title is overridden so while I am editing I know I am looking at a local copy. I also define some local Redis configuration that differs greatly from Production. # config/environments/production.py import os from ..base import * SENTRY_DSN = \"https://*******@sentry.io/*******\" REDIS = { 'url': os.environ.get('REDIS_URL') } SENTRY_DSN is only defined in production.py and not in the base or any other environments. This is to prevent Sentry (centralized error logs) from becoming activated in dev or test situations. On Heroku the Redis connection details come from a URL, so that is configured here. Finally, to demonstrate how this is used elsewhere in the app, take a look at how Redis connections are built: from redis import ConnectionPool from redis import Redis as R from system import config class RedisManager(object): @classmethod def from_config(cls, redis_config): if 'url' in redis_config: pool = ConnectionPool.from_url(redis_config['url']) else: pool = ConnectionPool(**redis_config) return cls(connection_pool=pool) def __init__(self, connection_pool): self.pool = connection_pool def get_connection(self): return R(connection_pool=self.pool) conn = property(get_connection) Redis = RedisManager.from_config(config.REDIS) Notice the last line: RedisManager.from_config() is used to isolate concerns. The rest of RedisManager doesn't know what the shape of the data in config looks like and shouldn't have to. This is one of the handoff points between the configuration layer and the rest of the system. Closing Thoughts I use this approach in all of my Python projects and have yet to find a situation where this (or a variation of it) doesn't work. We have the flexibility to create an unlimited number of environments. If for example we wanted to spin-up a temporary environment for a pull request: cp environments/staging.py environments/PR_402.py and ENV=PR_402 is all you need. When developing locally we can run the system in production mode by prefixing it with ENV=production and vice versa, running software anywhere else in a dev or test mode. Developers can quickly glean the major differences between environments by taking a look at the configuration each of them is overriding. This makes it easier to onboard new team members to your codebase. Similarly, each developer on the team can have his or her own unique configuration. No more clobbering central config because your system has something setup a little differently than the others. We can protect our test environment from accidentally reaching out to production resources by explicitly setting certain variables in environments/test.py to None . We eliminate the heft of passing big key/val configuration maps between various CLI tools such as Docker et-all (although tooling more and more capable of reading env from a file these days) We expose our configuration as a vanilla Python package so there is little to no learning curve and interoperability with other Python tools. We avoid the cost of supporting external libraries/dependencies At the end of the day this approach is not very glamorous, and that is exactly what we want when we're building systems that need to be reliable, maintainable and efficient. With some plain old Python and a few lines of special code we've unlocked a tremendous amount of flexibility and power in the configuration of our system. Did you find this useful? Consider following @whalesalad on Twitter so you don't miss out on other techniques for building better software .","title":"Doing Python Configuration Right"},{"location":"python-conf/#doing-python-configuration-right","text":"Let's talk about configuring Python applications, specifically the kind that might live in multiple environments \u2013 dev, stage, production, etc... The tools and frameworks used in the application are not super important because the approach that I will outline below is based on vanilla Python. The impetus for this approach was caused by frustration with Django's settings , but this is my go-to for any kind of Python application I might be working on.","title":"Doing Python Configuration Right"},{"location":"python-conf/#recap-python-modules-packages","text":"One of my favorite Python features is the way that the files and directories your application is made of map one-to-one with how you import and use them in code. For example, given this import statement: from app.utils import numbers We can infer the following directory structure: app \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 database.py \u251c\u2500\u2500 services.py \u251c\u2500\u2500 ... \u2514\u2500\u2500 utils \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 numbers.py Lots of languages and frameworks rely on this novel concept, including Clojure and ES6. In our example, Python considers the utils directory a Package . A directory becomes a package as soon as you place an empty __init__.py inside of it. A common scenario you might encounter as a Python hacker is one where you have a utils.py file that eventually gets too large, so you break it out into a utils/ directory containing many smaller files. When met with this situation we might do the following: # Create a new package: mkdir utils && touch utils/__init__.py # Move our existing code into the new package mv utils.py utils/something.py So now we've seen that a Python package is decided by the existence of an empty __init__.py file in a directory... but what if the file isn't empty ?","title":"Recap: Python Modules &amp; Packages"},{"location":"python-conf/#putting-code-in-__init__py","text":"Since it's just a regular old Python file, you can actually put whatever you want there and it will be executed the first time the package is imported. You can test this out on your own. Create a directory named foo and give it an empty __init__.py file. From a Python REPL in the same directory: Seeing no output here is good, it means the statement was successful. Now let's edit our __init__.py file to include the following code: sys.exit() is typically used to cause a process to exit with a specific status . Rerunning the same experiment in a new REPL you will observe that your Python shell immediately exits after the import. In a larger application the effect would be more pronounced: the whole application would exit. So we understand the fundamentals and we've seen how this feature can be used maliciously. Perhaps we can use it for good?","title":"Putting Code in __init__.py"},{"location":"python-conf/#multiple-environments-twelve-factor-apps","text":"It's likely that your application has lived in multiple environments. Your local development environment is likely the first, and you might have a test environment that lives on Jenkins or another CI platform. Your code is deployed to a production or live environment. Some systems might have a staging environment that is used just before things go live. Even if you only consider yourself a hobbyist, developing code locally and deploying it to a VPS or Heroku-like platform means you're dealing with multiple environments. A rule I follow when building applications is that I should be able to deploy a codebase \u2013 without modification \u2013 into any environment , assuming we have a way to tell the system where it's running. Contrast this to building multiple artifacts for each deployment target, requiring additional time and complexity to build and persist. These artifacts are typically designed to run in a single target environment, so running them locally or in a test mode is often difficult or impossible. The famous twelve-factor methodology shares this belief, in addition to the idea that all configuration should exist as environment variables, too. I agree with this to an extent, but there is sometimes a tendency to make everything an environment variable which quickly becomes difficult to support. If every knob and dial of your system is an environment variable, you'll find that you end up keeping various permutations of variables stored somewhere for running or debugging. See the problem here? We've pulled config out of one area (the code, something that is typically kept in version control) and moved them to an area that is more prone to error and human mistakes. The general guidelines I use to decide where to draw the line: Static things that don't change often, or things that dramatically influence the behavior of the system should live in the code. Dynamic things that change frequently, or things that should be kept secret (API keys/credentials) should live outside the code.","title":"Multiple Environments &amp; Twelve-Factor Apps"},{"location":"python-conf/#how-do-we-switch-environments","text":"In order for an application to change its behavior between environments we need a way to tell it where it is running. Leaning on environment variables (see a pattern?), I tend to use ENV (or a variation) for this purpose. The Ruby/Rails ecosystem uses RACK_ENV or RAILS_ENV Javascript projects will oftentimes leverage NODE_ENV I recently completed a project for a client with the following convention: My local development environment does not set an ENV variable, so the system infers development by default. The test environment on AWS CodePipeline uses ENV=test The production environment on EC2 uses ENV=production Note: It's important to consider the consequences of not setting this variable. Could that be catastrophic? For instance, could the app boot in DEV mode inside of a production cluster and end up showing tracebacks to the public? For some applications, the default should be production. There is no right or wrong answer here, but it needs to be considered.","title":"How do we Switch Environments?"},{"location":"python-conf/#the-end-goal","text":"From a developer standpoint we want to access our config like so: from service.config import AWS_S3_BUCKET The import line above doesn't contain anything that would suggest which environment we are in. We don't see the word development or production anywhere. Instead, we just import what we need and allow the configuration system to decide where that comes from. We're leveraging the filesystem and the language itself to provide an API for reading configuration. Behind the curtain, this is what the config directory looks like on disk: service/config \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 common.py \u2514\u2500\u2500 environments \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 development.py \u251c\u2500\u2500 production.py \u2514\u2500\u2500 staging.py common.py contains all of our common or shared configuration. These are things that don't significantly differ from one environment to the next. You could call this base or shared if you'd like. environments/development.py contains development configuration. This file could be excluded from version control so that each developer on the team can implement his or her own configuration settings. environments/(production|staging).py include configuration unique to each of their respective environments. Let's take a look inside common.py : import os APP_NAME = \"My Application\" # Conveniences to always have a quick reference to the # top-level application directory. ROOT_DIR = os.path.join( os.path.dirname(os.path.realpath(__file__)), os.pardir, os.pardir, ) SERVICE_DIR = os.path.join(ROOT_DIR, \"service\") # Used in DNS lookup jobs. DEFAULT_NAMESERVERS = [ \"1.1.1.1\", \"1.0.0.1\" ] # Job runner behaviors JOB_SUPERVISOR_SLEEP_SECONDS = 1 JOB_STATUS_TTL = 10 # AWS Configuration AWS_DYNAMO_REGION = 'us-west-1' AWS_DYNAMO_TABLE_PREFIX = 'acme-' AWS_S3_BUCKET = 'acme-production' SYSTEM_REBOOT_COMMAND = \"sudo systemctl restart foo.service\" This is a contrived example so don't read too deeply into the specifics. The important thing to notice is that this is fairly static configuration that is not going to change very often. Now let's look at environment/development.py : from ..common import * AWS_DYNAMO_TABLE_PREFIX = 'acme-dev-mwhalen-' AWS_S3_BUCKET = 'acme-dev-mwhalen' GOOGLE_CLIENT_ID = \"XXXX\" GOOGLE_CLIENT_SECRET = \"XXXX\" GOOGLE_CLIENT_REFRESH_TOKEN = \"XXXX\" # We're inserting our own DNS servers to the front of the defaults. DEFAULT_NAMESERVERS = [ \"10.0.0.3\" ] + DEFAULT_NAMESERVERS # This is intentionally a no-op command. # Our specific application is designed to be supervised by systemd, # but this is not available on macOS. SYSTEM_REBOOT_COMMAND = \"uname -a\" We start by importing the common configuration so that we inherit all of the common configuration by default. Now we have the ability to add, replace or augment parameters without the need to copy-paste from the parent. To support local development, I can customize the AWS resources being used in my environment. The rest of the system is unchanged, but now my local system is using my own tables in Dynamo as well as my own S3 bucket. Because this file is not in version control I can confidently store secrets such as my own GOOGLE_CLIENT_ credentials. Because there is access to the common DEFAULT_NAMESERVERS I have the ability extend them versus copy-pasting whatever the common values are into my own configuration. In production systemd commands are used to restart the application in response to certain admin actions. Because my Mac doesn't have systemd , I avoid that problem entirely by replacing the system reboot command with a simple no-op.","title":"The End Goal"},{"location":"python-conf/#how-it-works","text":"Circling back to our config/__init__.py file, what could we implement here to make this possible? It's actually quite straightforward: import os import importlib # Determine the environment we want to load, default is development.py ENV = os.environ.get(\"ENV\", \"development\") module = importlib.import_module( f\".environments.{ENV}\", package=\"service.config\" ) # update globals of this module (i.e. settings) with imported. globals().update(vars(module)) def is_development_env(): return ENV == \"development\" def is_production_env(): return ENV == \"production\" We're leveraging import-time evaluation to dynamically fetch the necessary configuration from the corresponding child environment. Let's step through it piece by piece: First we import the importlib module ( docs ) which gives us some handy tools for importing code with code. Using the convention we established \u2013 the ENV environment variable \u2013 we grab the name of the environment we're currently running in. We choose development as the default if one is not set, but as noted earlier this decision will vary depending on the system. We might even consider preventing our application from starting unless this variable is defined. Here is an example of how that could work: ``` ENV = os.environ.get(\"ENV\", None) if ENV is None: raise Exception(\"The ENV environment variable must be set!\") ``` Next we use the importlib.import_module function to load the module containing our specific environment's code into a local variable, module . Finally, we update the globals of this module merging in the ones from the development.py file. At the end you will see a few conveniences (a-la Rails) to make it easier to toggle specific logic based on the environment. These are kept as functions so that they isolate implementation to this module instead of wherever its being used. This approach was heavily inspired by Ruby on Rails configuration which achieves a very similar outward appearance albeit with a different under-the-hood implementation.","title":"How it Works"},{"location":"python-conf/#a-real-world-example","text":"To provide another example of this in action, below is the configuration for this website: First, here is the exact directory structure of my config directory: config \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 base.py \u2514\u2500\u2500 environments \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 development.py \u251c\u2500\u2500 production.py \u2514\u2500\u2500 test.py development.py is used locally production.py is used on Heroku test.py is used for local unit tests with pytest # config/environments/base.py import os import logging import pytz LOG_FORMAT = '[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s' ROOT_DIR = os.path.join( os.path.dirname(os.path.realpath(__file__)), os.pardir, os.pardir ) SYSTEM_DIR = os.path.join(ROOT_DIR, 'system') BLOG_DIR = os.path.join(ROOT_DIR, 'blog') SASS_DIR = os.path.join(ROOT_DIR, 'static', 'sass') TZ = pytz.timezone('America/Los_Angeles') def relative_to_root(path): return os.path.abspath(os.path.join(ROOT_DIR, path)) DEFAULT_SITE_TITLE = 'Michael Whalen \u2013 whalesalad.com' base.py contains fairly static configuration: A centralized log format to use elsewhere in the project. Common directories and a helper function to make path-related work easier. The timezone for my service. The default title to use when a page doesn't provide it's own. # config/environments/development.py import os from ..base import * DEFAULT_SITE_TITLE = f'[DEV] {DEFAULT_SITE_TITLE}' REDIS = { 'host': os.environ.get('REDIS_HOST', 'localhost'), 'port': os.environ.get('REDIS_PORT', 6379), 'db': os.environ.get('REDIS_DB', 0), 'socket_timeout': 120 } In development.py , the site title is overridden so while I am editing I know I am looking at a local copy. I also define some local Redis configuration that differs greatly from Production. # config/environments/production.py import os from ..base import * SENTRY_DSN = \"https://*******@sentry.io/*******\" REDIS = { 'url': os.environ.get('REDIS_URL') } SENTRY_DSN is only defined in production.py and not in the base or any other environments. This is to prevent Sentry (centralized error logs) from becoming activated in dev or test situations. On Heroku the Redis connection details come from a URL, so that is configured here. Finally, to demonstrate how this is used elsewhere in the app, take a look at how Redis connections are built: from redis import ConnectionPool from redis import Redis as R from system import config class RedisManager(object): @classmethod def from_config(cls, redis_config): if 'url' in redis_config: pool = ConnectionPool.from_url(redis_config['url']) else: pool = ConnectionPool(**redis_config) return cls(connection_pool=pool) def __init__(self, connection_pool): self.pool = connection_pool def get_connection(self): return R(connection_pool=self.pool) conn = property(get_connection) Redis = RedisManager.from_config(config.REDIS) Notice the last line: RedisManager.from_config() is used to isolate concerns. The rest of RedisManager doesn't know what the shape of the data in config looks like and shouldn't have to. This is one of the handoff points between the configuration layer and the rest of the system.","title":"A Real World Example"},{"location":"python-conf/#closing-thoughts","text":"I use this approach in all of my Python projects and have yet to find a situation where this (or a variation of it) doesn't work. We have the flexibility to create an unlimited number of environments. If for example we wanted to spin-up a temporary environment for a pull request: cp environments/staging.py environments/PR_402.py and ENV=PR_402 is all you need. When developing locally we can run the system in production mode by prefixing it with ENV=production and vice versa, running software anywhere else in a dev or test mode. Developers can quickly glean the major differences between environments by taking a look at the configuration each of them is overriding. This makes it easier to onboard new team members to your codebase. Similarly, each developer on the team can have his or her own unique configuration. No more clobbering central config because your system has something setup a little differently than the others. We can protect our test environment from accidentally reaching out to production resources by explicitly setting certain variables in environments/test.py to None . We eliminate the heft of passing big key/val configuration maps between various CLI tools such as Docker et-all (although tooling more and more capable of reading env from a file these days) We expose our configuration as a vanilla Python package so there is little to no learning curve and interoperability with other Python tools. We avoid the cost of supporting external libraries/dependencies At the end of the day this approach is not very glamorous, and that is exactly what we want when we're building systems that need to be reliable, maintainable and efficient. With some plain old Python and a few lines of special code we've unlocked a tremendous amount of flexibility and power in the configuration of our system. Did you find this useful? Consider following @whalesalad on Twitter so you don't miss out on other techniques for building better software .","title":"Closing Thoughts"},{"location":"pyyaml/","text":"Python YAML Configuration Guide YAML (YAML Ain't Markup Language) is a human-readable data serialization format that is commonly used for configuration files and data exchange between programming languages. Python has built-in support for parsing YAML files using the yaml module, which provides functions for reading and writing YAML data. Install and Import YAML pip install PyYAML After the package installation, you can proceed to import the PyYAML package in Python. import yaml Loading YAML with Python In this example, we open the file config.yaml in read mode and pass it to the yaml.load() function. We also specify the Loader parameter as yaml.FullLoader, which is a safe loader that can load any YAML document. with open(\"config.yaml\") as f: cfg = yaml.load(f, Loader=yaml.FullLoader) Printing Config print(cfg) {'maintainer': 'Mert Cobanov', 'pca_params': {'n_components': 224}, 'umap_params': {'metrics': ['euclidean', 'l1', 'manhattan', 'chebyshev'], 'min_dist': [0.01, 0.1, 0.5], 'spread': 1.0}, 'visualize_params': {'median_threshold': 6}} Individual print(config['pca_params']) #'n_components': 224 Updating YAML with Python Updating a YAML configuration file with Python involves loading the existing configuration data, modifying it in memory, and then writing the updated data back to the file. This can be accomplished using the pyyaml library, which provides functions for reading and writing YAML data. Here's an example of how to update a YAML file using Python: cfg[\"n_components\"] = 0.2 cfg[\"umap_params\"][\"metrics\"].append(\"chebyshev\") with open(\"config.yaml\", \"w\") as f: cfg = yaml.dump( cfg, stream=f, default_flow_style=False, sort_keys=False ) It's important to note that the yaml.dump() function will overwrite the entire contents of the file, so you should be careful to include all of the data that you want to keep in the updated file. If you only want to update a specific section of the YAML data, you can modify just that section in memory and then write the entire data structure back to the file.","title":"Python YAML Configs"},{"location":"pyyaml/#python-yaml-configuration-guide","text":"YAML (YAML Ain't Markup Language) is a human-readable data serialization format that is commonly used for configuration files and data exchange between programming languages. Python has built-in support for parsing YAML files using the yaml module, which provides functions for reading and writing YAML data.","title":"Python YAML Configuration Guide"},{"location":"pyyaml/#install-and-import-yaml","text":"pip install PyYAML After the package installation, you can proceed to import the PyYAML package in Python. import yaml","title":"Install and Import YAML"},{"location":"pyyaml/#loading-yaml-with-python","text":"In this example, we open the file config.yaml in read mode and pass it to the yaml.load() function. We also specify the Loader parameter as yaml.FullLoader, which is a safe loader that can load any YAML document. with open(\"config.yaml\") as f: cfg = yaml.load(f, Loader=yaml.FullLoader)","title":"Loading YAML with Python"},{"location":"pyyaml/#printing-config","text":"print(cfg) {'maintainer': 'Mert Cobanov', 'pca_params': {'n_components': 224}, 'umap_params': {'metrics': ['euclidean', 'l1', 'manhattan', 'chebyshev'], 'min_dist': [0.01, 0.1, 0.5], 'spread': 1.0}, 'visualize_params': {'median_threshold': 6}}","title":"Printing Config"},{"location":"pyyaml/#individual","text":"print(config['pca_params']) #'n_components': 224","title":"Individual"},{"location":"pyyaml/#updating-yaml-with-python","text":"Updating a YAML configuration file with Python involves loading the existing configuration data, modifying it in memory, and then writing the updated data back to the file. This can be accomplished using the pyyaml library, which provides functions for reading and writing YAML data. Here's an example of how to update a YAML file using Python: cfg[\"n_components\"] = 0.2 cfg[\"umap_params\"][\"metrics\"].append(\"chebyshev\") with open(\"config.yaml\", \"w\") as f: cfg = yaml.dump( cfg, stream=f, default_flow_style=False, sort_keys=False ) It's important to note that the yaml.dump() function will overwrite the entire contents of the file, so you should be careful to include all of the data that you want to keep in the updated file. If you only want to update a specific section of the YAML data, you can modify just that section in memory and then write the entire data structure back to the file.","title":"Updating YAML with Python"},{"location":"tuning-terms/","text":"Training and Tuning Terms Pre-trained vs Fine-tuned Pretrained models are pre-trained on large amounts of data to perform a particular task, such as image classification, natural language processing, or speech recognition. These models have learned to identify patterns and features in the data, and can be used as a starting point for a new task with similar characteristics. Fine-tuning, on the other hand, is the process of taking a pretrained model and adapting it to a specific task or domain by further training it on a smaller dataset. The goal of fine-tuning is to leverage the knowledge that the pretrained model has already acquired and to adjust it to the new task's requirements. In summary, pretrained models are models that are trained on large datasets to perform a particular task, and fine-tuning is the process of taking a pretrained model and further adapting it to a specific task or domain. Fine-tuning, transfer learning, and learning from scratch are all approaches to developing machine learning models, but they differ in how they use existing knowledge to build new models: Fine-tuning vs Transfer Learning vs Learning From Scratch Fine-tuning: Fine-tuning is the process of taking a pre-trained model and adapting it to a new task or domain by continuing training it on a smaller dataset. Fine-tuning is typically used when the new task is similar to the original task that the model was trained on. The idea is to leverage the knowledge that the model has already acquired from the original training to improve its performance on the new task. Transfer learning: Transfer learning is similar to fine-tuning in that it involves using a pre-trained model for a new task. However, instead of fine-tuning the model, transfer learning involves using the pre-trained model as a feature extractor, and then training a new model on top of those features. Transfer learning is typically used when the new task is different from the original task that the model was trained on, but there are still some useful features that can be extracted. Learning from scratch: Learning from scratch involves training a new model from scratch on a dataset without using any pre-existing knowledge. This approach requires a large amount of data and computational resources, and can take a long time to achieve good performance. Learning from scratch is typically used when there is no existing pre-trained model that can be adapted to the new task, or when the task is so different that transfer learning or fine-tuning is not effective. In summary, fine-tuning involves adapting a pre-trained model to a new task, transfer learning involves using a pre-trained model as a feature extractor, and learning from scratch involves training a new model from scratch without any pre-existing knowledge.","title":"Training and Tuning Terms"},{"location":"tuning-terms/#training-and-tuning-terms","text":"","title":"Training and Tuning Terms"},{"location":"tuning-terms/#pre-trained-vs-fine-tuned","text":"Pretrained models are pre-trained on large amounts of data to perform a particular task, such as image classification, natural language processing, or speech recognition. These models have learned to identify patterns and features in the data, and can be used as a starting point for a new task with similar characteristics. Fine-tuning, on the other hand, is the process of taking a pretrained model and adapting it to a specific task or domain by further training it on a smaller dataset. The goal of fine-tuning is to leverage the knowledge that the pretrained model has already acquired and to adjust it to the new task's requirements. In summary, pretrained models are models that are trained on large datasets to perform a particular task, and fine-tuning is the process of taking a pretrained model and further adapting it to a specific task or domain. Fine-tuning, transfer learning, and learning from scratch are all approaches to developing machine learning models, but they differ in how they use existing knowledge to build new models:","title":"Pre-trained vs Fine-tuned"},{"location":"tuning-terms/#fine-tuning-vs-transfer-learning-vs-learning-from-scratch","text":"Fine-tuning: Fine-tuning is the process of taking a pre-trained model and adapting it to a new task or domain by continuing training it on a smaller dataset. Fine-tuning is typically used when the new task is similar to the original task that the model was trained on. The idea is to leverage the knowledge that the model has already acquired from the original training to improve its performance on the new task. Transfer learning: Transfer learning is similar to fine-tuning in that it involves using a pre-trained model for a new task. However, instead of fine-tuning the model, transfer learning involves using the pre-trained model as a feature extractor, and then training a new model on top of those features. Transfer learning is typically used when the new task is different from the original task that the model was trained on, but there are still some useful features that can be extracted. Learning from scratch: Learning from scratch involves training a new model from scratch on a dataset without using any pre-existing knowledge. This approach requires a large amount of data and computational resources, and can take a long time to achieve good performance. Learning from scratch is typically used when there is no existing pre-trained model that can be adapted to the new task, or when the task is so different that transfer learning or fine-tuning is not effective. In summary, fine-tuning involves adapting a pre-trained model to a new task, transfer learning involves using a pre-trained model as a feature extractor, and learning from scratch involves training a new model from scratch without any pre-existing knowledge.","title":"Fine-tuning vs Transfer Learning vs Learning From Scratch"},{"location":"windows-wsl-ssh/","text":"Step 1. Enable WSL on Windows Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux Step 2. Install Linux Distro curl.exe -L -o ubuntu-2004.appx https://aka.ms/wslubuntu2004 Rename-Item ubuntu-2004.appx ubuntu-2004.zip Expand-Archive ubuntu-2004.zip ubuntu2004 Step 3. Set the Path Environment Variable $userenv = [System.Environment]::GetEnvironmentVariable(\"Path\", \"User\")[System.Environment]::SetEnvironmentVariable(\"PATH\", $userenv + \"C:\\Users\\Administrator\\ubuntu2004\", \"User\") Step 4. Enable SSH on Windows Add-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0 Set-Service -Name ssh-agent -StartupType 'Automatic' Set-Service -Name sshd -StartupType 'Automatic' Get-Service ssh* | Start-Service Step 5. Set default SSH Shell New-ItemProperty -Path \"HKLM:\\SOFTWARE\\OpenSSH\" -Name DefaultShell -Value \"C:\\WINDOWS\\System32\\bash.exe\" -PropertyType String -Force","title":"Windows WSL Activate SSH"},{"location":"windows-wsl-ssh/#step-1-enable-wsl-on-windows","text":"Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux","title":"Step 1. Enable WSL on Windows"},{"location":"windows-wsl-ssh/#step-2-install-linux-distro","text":"curl.exe -L -o ubuntu-2004.appx https://aka.ms/wslubuntu2004 Rename-Item ubuntu-2004.appx ubuntu-2004.zip Expand-Archive ubuntu-2004.zip ubuntu2004","title":"Step 2. Install Linux Distro"},{"location":"windows-wsl-ssh/#step-3-set-the-path-environment-variable","text":"$userenv = [System.Environment]::GetEnvironmentVariable(\"Path\", \"User\")[System.Environment]::SetEnvironmentVariable(\"PATH\", $userenv + \"C:\\Users\\Administrator\\ubuntu2004\", \"User\")","title":"Step 3. Set the Path Environment Variable"},{"location":"windows-wsl-ssh/#step-4-enable-ssh-on-windows","text":"Add-WindowsCapability -Online -Name OpenSSH.Server~~~~0.0.1.0 Set-Service -Name ssh-agent -StartupType 'Automatic' Set-Service -Name sshd -StartupType 'Automatic' Get-Service ssh* | Start-Service","title":"Step 4. Enable SSH on Windows"},{"location":"windows-wsl-ssh/#step-5-set-default-ssh-shell","text":"New-ItemProperty -Path \"HKLM:\\SOFTWARE\\OpenSSH\" -Name DefaultShell -Value \"C:\\WINDOWS\\System32\\bash.exe\" -PropertyType String -Force","title":"Step 5. Set default SSH Shell"},{"location":"archive/dreambooth-rehberi/","text":"Dreambooth Extension for Stable-Diffusion-WebUI This is a WIP port of Shivam Shriao's Diffusers Repo , which is a modified version of the default Huggingface Diffusers Repo optimized for better performance on lower-VRAM GPUs. It also adds several other features, including training multiple concepts simultaneously, and (Coming soon) Inpainting training. Installation To install, simply go to the \"Extensions\" tab in the SD Web UI, select the \"Available\" sub-tab, pick \"Load from:\" to load the list of extensions, and finally, click \"install\" next to the Dreambooth entry. For 8bit adam to run properly, it may be necessary to install the CU116 version of torch and torchvision, which can be accomplished below: Refer to the appropriate script below for extra flags to install requirements: https://github.com/d8ahazard/sd_dreambooth_extension/blob/main/webui-user-dreambooth.bat https://github.com/d8ahazard/sd_dreambooth_extension/blob/main/webui-user-dreambooth.sh Setting the torch command to: TORCH_COMMAND=pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116 will ensure that the proper torch version is installed when webui-user is executed, and then left alone after that, versus trying to install conflicting versions. We also need a newer version of diffusers, as SD-WebUI uses version 0.3.0, while DB training requires > 0.6.0, so we use 0.7.2. Not having the right diffusers version is the cause of the 'UNet2DConditionModel' object has no attribute 'enable_gradient_checkpointing' error message, as well as safety checker warnings. To force sd-web-ui to only install one set of requirements, we can specify the command line argument: set/export REQS_FILE=.\\extensions\\sd_dreambooth_extension\\requirements.txt And last, if you wish to completely skip the \"native\" install routine of Dreambooth, you can set the following environment flag: DREAMBOOTH_SKIP_INSTALL=True This is ideal for \"offline mode\", where you don't want the script to constantly check things from pypi. After installing via the WebUI, it is recommended to set the above flags and re-launch the entire Stable-diffusion-webui, not just reload it. Usage Create a Model Go to the Dreambooth tab. Under the \"Create Model\" sub-tab, enter a new model name and select the source checkpoint to train from. The source checkpoint will be extracted to models\\dreambooth\\MODELNAME\\working - the original will not be touched. 2b. Optionally, you can also specify a huggingface model directory and token to create the Dreambooth dataset from huggingface.co. Model path format should be like so: 'runwayml/stable-diffusion-v1-5' Click \"Create\". This will take a minute or two, but when done, the UI should indicate that a new model directory has been set up. Training (Basic Settings) After creating a new model, select the new model name from the \"Model\" dropdown at the very top. Select the \"Train Model\" sub-tab. Fill in the paramters as described below: Concepts List - The path to a JSON file or a JSON string containing multiple concepts. See here for an example. If a concepts list is specified, then the instance prompt, class prompt, instance data dir, and class data dir fields will be ignored. Instance Prompt - A short descriptor of your subject using a UNIQUE keyword and a classifier word. If training a dog, your instance prompt could be \"photo of zkz dog\". The key here is that \"zkz\" is not a word that might overlap with something in the real world \"fluff\", and \"dog\" is a generic word to describe your subject. This is only necessary if using prior preservation. You can use [filewords] as placeholder for reading caption from the image filename or a seprarte .txt file containing caption, for example, [filewords], in the style of zymkyr . This syntax is the same as textual inversion templates. Class Prompt - A keyword indicating what type of \"thing\" your subject is. If your instance prompt is \"photo of zkz dog\", your class prompt would be \"photo of a dog\". Leave this blank to disable prior preservation training. Dataset Directory - The path to the directory where the images described in Instance Prompt are kept. REQUIRED Classification dataset directory - The path to the directory where the images described in Class Prompt are kept. If a class prompt is specified and this is left blank, images will be generated to /models/dreambooth/MODELNAME/classifiers/ Total number of classification images to use - Leave at 0 to disable prior preservation. For best results you want ~n*10 classification images - so if you have 40 training photos, then set this to 400. This is just a guess. Training steps - How many total training steps to complete. According to this guide , you should train for appx 100 steps per sample image. So, if you have 40 instance/sample images, you would train for 4k steps. This is, of course, a rough approximation, and other values will have an effect on final output fidelity. Batch size - How many training steps to process simultaneously. You probably want to leave this at 1. Class batch size - How many classification images to generate simultaneously. Set this to whatever you can safely process at once using Txt2Image, or just leave it alone. Learning rate - You probably don't want to touch this. Resolution - The resolution to train images at. You probably want to keep this number at 512 or lower unless your GPU is insane. Lowering this (and the resolution of training images) may help with lower-VRAM GPUs. Save a checkpoint every N steps - How frequently to save a checkpoint from the trained data. I should probably change the default of this to 1000. Generate a preview image every N steps - How frequently will an image be generated as an example of training progress. Preview image prompt - The prompt to use to generate preview image. Leave blank to use the instance prompt. Preview image negative prompt - Like above, but negative. Leave blank to do nothing. :P Number of samples to generate - Self explainatory? Sample guidance scale - Like CFG Scale in Txt2Image/Img2Img, used for generating preview. Sample steps - Same as sample guidance scale, but the number of steps to run to generate preview. Advanced Settings Use CPU Only - As indicated, this is more of a last resort if you can't get it to train with any other settings. Also, as indicated, it will be abysmally slow. Also, you cannot use 8Bit-Adam with CPU Training, or you'll have a bad time. Don't Cache Latents - Why is this not just called \"cache\" latents? Because that's what the original script uses, and I'm trying to maintain the ability to update this as easily as possible. Anyway...when this box is checked latents will not be cached. When latents are not cached, you will save a bit of VRAM, but train slightly slower. Train Text Encoder - Not required, but recommended. Enabling this will probably cost a bit more VRAM, but also purportedly increase output image fidelity. Use 8Bit Adam - Enable this to save VRAM. Should now work on both windows and Linux without needing WSL. Center Crop - Crop images if they aren't the right dimensions? I don't use this, and I recommend you just crop your images \"right\". Gradient Checkpointing - Enable this to save VRAM at the cost of a bit of speed. Scale Learning Rate - I don't use this, not sure what impact it has on performance or output quality. Mixed Precision - Set to 'fp16' to save VRAM at the cost of speed. Everything after 'Mixed Precision' - Adjust at your own risk. Performance/quality benefits from changing these remain to be tested. The next two were added after I wrote the above bit, so just ignore me being a big liar. Pad Tokens - Pads the text tokens to a longer length for some reason. Max Token Length - raise the tokenizer's default limit above 75. Requires Pad Tokens for > 75. Apply Horizontal Flip - \"Apply horizontal flip augmentation\". Flips images horizontally at random, which can potentially offer better editability? Use EMA for finetuning - Use exponential moving average weight to reduce overfitting during the last iterations. Continuing Training Once a model has been trained for any number of steps, a config file is saved which contains all of the parameters from the UI. If you wish to continue training a model, you can simply select the model name from the dropdown and then click the blue button next to the model name dropdown to load previous parameters. Use DreamBooth to Fine-Tune Stable Diffusion in Google Colab Prepare Images Choosing Images When choosing images, it\u2019s recommended to keep the following in mind to get the best results: Upload a variety of images of your subject. If you\u2019re uploading images of a person, try something like 70% close-ups, 20% from the chest up, 10% full body, so Stable Diffusion also gets some idea of the rest of the subject and not only the face. Try to change things up as much as possible in each picture. This means: Varying the body pose Taking pictures on different days, in different lighting conditions, and with different backgrounds Showing a variety of expressions and emotions When generating new images, whatever you capture will be over-represented. For example, if you take multiple pictures with the same green field behind you, it\u2019s likely that the generated images of you will also contain the green field, even if you want a dystopic background. This can apply to anything, like jewelry, clothes, or even people in the background. If you want to avoid seeing that element in your generated image, make sure not to repeat it in every shot. On the other hand, if you want it in the generated images, make sure it\u2019s in your pictures more often. It\u2019s recommended that you provide ~50 images of what you\u2019d like to train Stable Diffusion on to get great results. However, I\u2019ve only used 20-30 so far, and the results are pretty good. If you\u2019re just starting out and want to test it out, I think 20-30 images should be good enough for now, and you can get 50 images after you\u2019ve seen it work. Resize & Crop to 512 x 512px Once you\u2019ve chosen your images, you should prepare them. First, we need to resize and crop our images to be 512 x 512px. We can easily do this using the website https://birme.net . To do this, just: Visit the website Upload your images Set your dimensions to 512 x 512px Adjust the cropping area to center your subject Click on Save as Zip to download the archive. You can then unzip it on your computer, and we\u2019ll use them a bit later. Birme.net - Resize Images Resizing Images using Birme.net Renaming Your Images We\u2019ll also want to rename our images to contain the subject\u2019s name: Firstly, the subject name should be one unique/random/unknown keyword. This is because Stable Diffusion also has some knowledge of The Sandman from other sources other than the one played by Tom Sturridge and we don\u2019t want it to get confused and make a combination of interpretations of The Sandman. As such, I\u2019ll call it Sandman2022 to make sure it\u2019s unique. Renaming images to subject (1), subject (2) .. subject (30). This is because, using this method, you can train multiple subjects at once. If you want to fine-tune Stable Diffusion with Sandman, your friend Kevin, and your cat, you can give it prepare images for each of them. For the Sandman you\u2019d have Sandman2022 (1), Sandman2022 (2) \u2026 Sandman (30), for Kevin you\u2019d have KevinKevinson2022 (1), KevinKevinson2022 (2) \u2026 KevinKevinson (30), and for your cat you\u2019d have DexterTheCat (1), DexterTheCat (2) \u2026 DexterTheCat(30). Here\u2019s me renaming my images for Sandman2022 in bulk on Windows. Just select them all, right click one of them and click Rename and give it what name you want and click anywhere to finish the renaming. Everything else will be renamed as well.","title":"Dreambooth Extension for Stable-Diffusion-WebUI"},{"location":"archive/dreambooth-rehberi/#dreambooth-extension-for-stable-diffusion-webui","text":"This is a WIP port of Shivam Shriao's Diffusers Repo , which is a modified version of the default Huggingface Diffusers Repo optimized for better performance on lower-VRAM GPUs. It also adds several other features, including training multiple concepts simultaneously, and (Coming soon) Inpainting training.","title":"Dreambooth Extension for Stable-Diffusion-WebUI"},{"location":"archive/dreambooth-rehberi/#installation","text":"To install, simply go to the \"Extensions\" tab in the SD Web UI, select the \"Available\" sub-tab, pick \"Load from:\" to load the list of extensions, and finally, click \"install\" next to the Dreambooth entry. For 8bit adam to run properly, it may be necessary to install the CU116 version of torch and torchvision, which can be accomplished below: Refer to the appropriate script below for extra flags to install requirements: https://github.com/d8ahazard/sd_dreambooth_extension/blob/main/webui-user-dreambooth.bat https://github.com/d8ahazard/sd_dreambooth_extension/blob/main/webui-user-dreambooth.sh Setting the torch command to: TORCH_COMMAND=pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116 will ensure that the proper torch version is installed when webui-user is executed, and then left alone after that, versus trying to install conflicting versions. We also need a newer version of diffusers, as SD-WebUI uses version 0.3.0, while DB training requires > 0.6.0, so we use 0.7.2. Not having the right diffusers version is the cause of the 'UNet2DConditionModel' object has no attribute 'enable_gradient_checkpointing' error message, as well as safety checker warnings. To force sd-web-ui to only install one set of requirements, we can specify the command line argument: set/export REQS_FILE=.\\extensions\\sd_dreambooth_extension\\requirements.txt And last, if you wish to completely skip the \"native\" install routine of Dreambooth, you can set the following environment flag: DREAMBOOTH_SKIP_INSTALL=True This is ideal for \"offline mode\", where you don't want the script to constantly check things from pypi. After installing via the WebUI, it is recommended to set the above flags and re-launch the entire Stable-diffusion-webui, not just reload it.","title":"Installation"},{"location":"archive/dreambooth-rehberi/#usage","text":"","title":"Usage"},{"location":"archive/dreambooth-rehberi/#create-a-model","text":"Go to the Dreambooth tab. Under the \"Create Model\" sub-tab, enter a new model name and select the source checkpoint to train from. The source checkpoint will be extracted to models\\dreambooth\\MODELNAME\\working - the original will not be touched. 2b. Optionally, you can also specify a huggingface model directory and token to create the Dreambooth dataset from huggingface.co. Model path format should be like so: 'runwayml/stable-diffusion-v1-5' Click \"Create\". This will take a minute or two, but when done, the UI should indicate that a new model directory has been set up.","title":"Create a Model"},{"location":"archive/dreambooth-rehberi/#training-basic-settings","text":"After creating a new model, select the new model name from the \"Model\" dropdown at the very top. Select the \"Train Model\" sub-tab. Fill in the paramters as described below: Concepts List - The path to a JSON file or a JSON string containing multiple concepts. See here for an example. If a concepts list is specified, then the instance prompt, class prompt, instance data dir, and class data dir fields will be ignored. Instance Prompt - A short descriptor of your subject using a UNIQUE keyword and a classifier word. If training a dog, your instance prompt could be \"photo of zkz dog\". The key here is that \"zkz\" is not a word that might overlap with something in the real world \"fluff\", and \"dog\" is a generic word to describe your subject. This is only necessary if using prior preservation. You can use [filewords] as placeholder for reading caption from the image filename or a seprarte .txt file containing caption, for example, [filewords], in the style of zymkyr . This syntax is the same as textual inversion templates. Class Prompt - A keyword indicating what type of \"thing\" your subject is. If your instance prompt is \"photo of zkz dog\", your class prompt would be \"photo of a dog\". Leave this blank to disable prior preservation training. Dataset Directory - The path to the directory where the images described in Instance Prompt are kept. REQUIRED Classification dataset directory - The path to the directory where the images described in Class Prompt are kept. If a class prompt is specified and this is left blank, images will be generated to /models/dreambooth/MODELNAME/classifiers/ Total number of classification images to use - Leave at 0 to disable prior preservation. For best results you want ~n*10 classification images - so if you have 40 training photos, then set this to 400. This is just a guess. Training steps - How many total training steps to complete. According to this guide , you should train for appx 100 steps per sample image. So, if you have 40 instance/sample images, you would train for 4k steps. This is, of course, a rough approximation, and other values will have an effect on final output fidelity. Batch size - How many training steps to process simultaneously. You probably want to leave this at 1. Class batch size - How many classification images to generate simultaneously. Set this to whatever you can safely process at once using Txt2Image, or just leave it alone. Learning rate - You probably don't want to touch this. Resolution - The resolution to train images at. You probably want to keep this number at 512 or lower unless your GPU is insane. Lowering this (and the resolution of training images) may help with lower-VRAM GPUs. Save a checkpoint every N steps - How frequently to save a checkpoint from the trained data. I should probably change the default of this to 1000. Generate a preview image every N steps - How frequently will an image be generated as an example of training progress. Preview image prompt - The prompt to use to generate preview image. Leave blank to use the instance prompt. Preview image negative prompt - Like above, but negative. Leave blank to do nothing. :P Number of samples to generate - Self explainatory? Sample guidance scale - Like CFG Scale in Txt2Image/Img2Img, used for generating preview. Sample steps - Same as sample guidance scale, but the number of steps to run to generate preview.","title":"Training (Basic Settings)"},{"location":"archive/dreambooth-rehberi/#advanced-settings","text":"Use CPU Only - As indicated, this is more of a last resort if you can't get it to train with any other settings. Also, as indicated, it will be abysmally slow. Also, you cannot use 8Bit-Adam with CPU Training, or you'll have a bad time. Don't Cache Latents - Why is this not just called \"cache\" latents? Because that's what the original script uses, and I'm trying to maintain the ability to update this as easily as possible. Anyway...when this box is checked latents will not be cached. When latents are not cached, you will save a bit of VRAM, but train slightly slower. Train Text Encoder - Not required, but recommended. Enabling this will probably cost a bit more VRAM, but also purportedly increase output image fidelity. Use 8Bit Adam - Enable this to save VRAM. Should now work on both windows and Linux without needing WSL. Center Crop - Crop images if they aren't the right dimensions? I don't use this, and I recommend you just crop your images \"right\". Gradient Checkpointing - Enable this to save VRAM at the cost of a bit of speed. Scale Learning Rate - I don't use this, not sure what impact it has on performance or output quality. Mixed Precision - Set to 'fp16' to save VRAM at the cost of speed. Everything after 'Mixed Precision' - Adjust at your own risk. Performance/quality benefits from changing these remain to be tested. The next two were added after I wrote the above bit, so just ignore me being a big liar. Pad Tokens - Pads the text tokens to a longer length for some reason. Max Token Length - raise the tokenizer's default limit above 75. Requires Pad Tokens for > 75. Apply Horizontal Flip - \"Apply horizontal flip augmentation\". Flips images horizontally at random, which can potentially offer better editability? Use EMA for finetuning - Use exponential moving average weight to reduce overfitting during the last iterations.","title":"Advanced Settings"},{"location":"archive/dreambooth-rehberi/#continuing-training","text":"Once a model has been trained for any number of steps, a config file is saved which contains all of the parameters from the UI. If you wish to continue training a model, you can simply select the model name from the dropdown and then click the blue button next to the model name dropdown to load previous parameters.","title":"Continuing Training"},{"location":"archive/dreambooth-rehberi/#use-dreambooth-to-fine-tune-stable-diffusion-in-google-colab","text":"","title":"Use DreamBooth to Fine-Tune Stable Diffusion in Google Colab"},{"location":"archive/dreambooth-rehberi/#prepare-images","text":"","title":"Prepare Images"},{"location":"archive/dreambooth-rehberi/#choosing-images","text":"When choosing images, it\u2019s recommended to keep the following in mind to get the best results: Upload a variety of images of your subject. If you\u2019re uploading images of a person, try something like 70% close-ups, 20% from the chest up, 10% full body, so Stable Diffusion also gets some idea of the rest of the subject and not only the face. Try to change things up as much as possible in each picture. This means: Varying the body pose Taking pictures on different days, in different lighting conditions, and with different backgrounds Showing a variety of expressions and emotions When generating new images, whatever you capture will be over-represented. For example, if you take multiple pictures with the same green field behind you, it\u2019s likely that the generated images of you will also contain the green field, even if you want a dystopic background. This can apply to anything, like jewelry, clothes, or even people in the background. If you want to avoid seeing that element in your generated image, make sure not to repeat it in every shot. On the other hand, if you want it in the generated images, make sure it\u2019s in your pictures more often. It\u2019s recommended that you provide ~50 images of what you\u2019d like to train Stable Diffusion on to get great results. However, I\u2019ve only used 20-30 so far, and the results are pretty good. If you\u2019re just starting out and want to test it out, I think 20-30 images should be good enough for now, and you can get 50 images after you\u2019ve seen it work.","title":"Choosing Images"},{"location":"archive/dreambooth-rehberi/#resize-crop-to-512-x-512px","text":"Once you\u2019ve chosen your images, you should prepare them. First, we need to resize and crop our images to be 512 x 512px. We can easily do this using the website https://birme.net . To do this, just: Visit the website Upload your images Set your dimensions to 512 x 512px Adjust the cropping area to center your subject Click on Save as Zip to download the archive. You can then unzip it on your computer, and we\u2019ll use them a bit later. Birme.net - Resize Images Resizing Images using Birme.net","title":"Resize &amp; Crop to 512 x 512px"},{"location":"archive/dreambooth-rehberi/#renaming-your-images","text":"We\u2019ll also want to rename our images to contain the subject\u2019s name: Firstly, the subject name should be one unique/random/unknown keyword. This is because Stable Diffusion also has some knowledge of The Sandman from other sources other than the one played by Tom Sturridge and we don\u2019t want it to get confused and make a combination of interpretations of The Sandman. As such, I\u2019ll call it Sandman2022 to make sure it\u2019s unique. Renaming images to subject (1), subject (2) .. subject (30). This is because, using this method, you can train multiple subjects at once. If you want to fine-tune Stable Diffusion with Sandman, your friend Kevin, and your cat, you can give it prepare images for each of them. For the Sandman you\u2019d have Sandman2022 (1), Sandman2022 (2) \u2026 Sandman (30), for Kevin you\u2019d have KevinKevinson2022 (1), KevinKevinson2022 (2) \u2026 KevinKevinson (30), and for your cat you\u2019d have DexterTheCat (1), DexterTheCat (2) \u2026 DexterTheCat(30). Here\u2019s me renaming my images for Sandman2022 in bulk on Windows. Just select them all, right click one of them and click Rename and give it what name you want and click anywhere to finish the renaming. Everything else will be renamed as well.","title":"Renaming Your Images"},{"location":"archive/instant-ngp-windows/","text":"Instant Neural Graphics Primitives ! Requirements An NVIDIA GPU ; tensor cores increase performance when available. All shown results come from an RTX 3090. Python ver: 3.9.* Visual Studio Community 2019 (Latest the best, ~8GB) Below are the install requirements CUDA v11.6 . You can check ur CUDA version via nvcc --version in any prompt and if it's not CUDA11.6, refer to this to swap/install the correct version. On some machines, pyexr refuses to install via pip . This can be resolved by installing OpenEXR from here . See later. This installation tutorial will be using Anaconda. Download anaconda prompt here . OptiX 7.3 or higher for faster mesh SDF training. You need to either login or join to obtain the installer. Set the system environment variables OptiX_INSTALL_DIR to the installation directory if it is not discovered automatically. Should look like this: Compilation copy these files C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\extras\\visual_studio_integration\\MSBuildExtensions to here C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\MSBuild\\Microsoft\\VC\\v160\\BuildCustomizations cd into a directory that you want to download the codes at. Eg. cd F:\\Tutorial\\ngp\\ Begin by cloning this repository and all its submodules using the following command (if you don't have git, download here and add to path): $ git clone --recursive https://github.com/nvlabs/instant-ngp $ cd instant-ngp if your python is not 3.9 (check with command python --version ) then you need to run the following command to get it to ver 3.9.* conda install python=3.9 Then, open Developer Command Prompt , you can find this in your search bar. Then cd to where you cloned your repository so you are in its root folder /instant-ng/ : cmake . -B build cmake --build build --config RelWithDebInfo -j 16 If the any of these build fails, please consult this list of possible fixes before opening an issue. If automatic GPU architecture detection fails, (as can happen if you have multiple GPUs installed), set the TCNN_CUDA_ARCHITECTURES enivonment variable for the GPU you would like to use. The following table lists the values for common GPUs. If your GPU is not listed, consult this exhaustive list . RTX 30X0 A100 RTX 20X0 TITAN V / V100 GTX 10X0 / TITAN Xp GTX 9X0 K80 86 80 75 70 61 52 37 Interactive Training and Rendering on Custom Image Sets Install COLMAP , I used ver 3.7 Add it to your system environment variables at Environment Variables > System Variables Path > Edit environment variable open anaconda prompt, if you don't have you don't have you can get it here cd into isntant-ngp as root conda create -n ngp python=3.9 conda activate ngp pip install -r requirements.txt if pyexr cannot be installed via pip install pyexr , download OpenEXR\u20111.3.2\u2011cp39\u2011cp39\u2011win_amd64.whl and move it to your root folder. Then you can run: pip install OpenEXR-1.3.2-cp39-cp39-win_amd64.whl Place your custom image set under data/<image_set_name> Get transform.json from the following command. Insert your path to your images at <image/path> python scripts/colmap2nerf.py --colmap_matcher exhaustive --run_colmap --aabb_scale 16 --images <image/path> transform.json will be generated at the root folder, drag and drop it into your data/<image_set_name> folder. You have to reorganize the folder structure due to how transforms.json is created... For example: File Structure BEFORE generating transform.json \ud83d\udcc2instant-ngp/ # this is root \u251c\u2500\u2500 \ud83d\udcc2data/ \u2502 \u251c\u2500\u2500 \ud83d\udcc2toy_truck/ \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcdctoy_truck_001.jpg \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcdctoy_truck_002.jpg \u2502 \u2502 \u2502... \u2502 \u2502... \u2502... File Structure AFTER generating transform.json \ud83d\udcc2instant-ngp/ # this is root \u251c\u2500\u2500 \ud83d\udcc2data/ \u2502 \u251c\u2500\u2500 \ud83d\udcc2toy_truck/ \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcdctransforms.json/ \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc2data/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc2toy_truck/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcdctoy_truck_001.jpg \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcdctoy_truck_002.jpg \u2502 \u2502 \u2502 \u2502 \u2502... \u2502 \u2502 \u2502 \u2502... \u2502 \u2502 \u2502... \u2502 \u2502... \u2502... Note: adjusting the \"aabb_scale\" inside transform.json can reduce load on GPU VRAM. The lower the value the less intensive it'll be. Finally, to run instant-ngp: <path_to_your_ngp>\\instant-ngp\\build\\testbed.exe --scene data/<image_set_name> eg. C:\\user\\user\\download\\instant-ngp\\build\\testbed.exe --scene data/toy_truck And it should launch the GUI and everything amazing with it Rendering custom camera path May need to install more dependencies. Install pip install tqdm scipy pillow opencv-python , conda install -c conda-forge ffmpeg , might be needed in the conda virtual environment. Refer to installation of pyexr above in the installation section if you didn't install that too. Train any image set like above. After you have reached a point that you are satisfied with your training, save a Snapshot on the GUI. (one of the tabs & no need to edit the path & the name) Find another GUI called camera path, it'll play hide and seek with you but it is there so find that window. The GUI is so well made, if you know how to use any 3D engine, it's really similar. Add camera path will give you a new angle of the camera. After you have finished adding your camera points, save the camera path. (no need to edit the path & the name) Render the path with the following command: python scripts/render.py --scene <scene_path> --n_seconds <seconds> --fps <fps> --render_name <name> --width <resolution_width> --height <resolution_height> eg. python scripts/render.py --scene data/toy --n_seconds 5 --fps 60 --render_name test --width 1920 --height 1080 Your video will be saved at root. You might have to play around with the fps and n_seconds to speed up or slow down. I couldn't get it accurately because of the lack of information and this is the best I could come up with. To be honest, this is only a short-term solution too, since the author has promised to publish an official one. So stay tuned! And my fork edits end here. Interactive training and rendering This codebase comes with an interactive testbed that includes many features beyond our academic publication: Additional training features, such as extrinsics and intrinsics optimization. Marching cubes for NeRF->Mesh and SDF->Mesh conversion. A spline-based camera path editor to create videos. Debug visualizations of the activations of every neuron input and output. And many more task-specific settings. See also our one minute demonstration video of the tool . NeRF fox One test scene is provided in this repository, using a small number of frames from a casually captured phone video: instant-ngp$ ./build/testbed --scene data/nerf/fox Alternatively, download any NeRF-compatible scene (e.g. from the NeRF authors' drive ). Now you can run: instant-ngp$ ./build/testbed --scene data/nerf_synthetic/lego/transforms_train.json For more information about preparing datasets for use with our NeRF implementation, please see this document . SDF armadillo instant-ngp$ ./build/testbed --scene data/sdf/armadillo.obj Image of Einstein instant-ngp$ ./build/testbed --scene data/image/albert.exr To reproduce the gigapixel results, download, for example, the Tokyo image and convert it to .bin using the scripts/image2bin.py script. This custom format improves compatibility and loading speed when resolution is high. Now you can run: instant-ngp$ ./build/testbed --scene data/image/tokyo.bin Volume Renderer Download the nanovdb volume for the Disney cloud , which is derived from here ( CC BY-SA 3.0 ). instant-ngp$ ./build/testbed --mode volume --scene data/volume/wdas_cloud_quarter.nvdb Python bindings To conduct controlled experiments in an automated fashion, all features from the interactive testbed (and more!) have Python bindings that can be easily instrumented. For an example of how the ./build/testbed application can be implemented and extended from within Python, see ./scripts/run.py , which supports a superset of the command line arguments that ./build/testbed does. Happy hacking! Troubleshooting compile errors Before investigating further, make sure all submodules are up-to-date and try compiling again. instant-ngp$ git submodule sync --recursive instant-ngp$ git submodule update --init --recursive If instant-ngp still fails to compile, update CUDA as well as your compiler to the latest versions you can install on your system. It is crucial that you update both , as newer CUDA versions are not always compatible with earlier compilers and vice versa. If your problem persists, consult the following table of known issues. Problem Resolution CMake error: No CUDA toolset found / CUDA_ARCHITECTURES is empty for target \"cmTC_0c70f\" Windows: the Visual Studio CUDA integration was not installed correctly. Follow these instructions to fix the problem without re-installing CUDA. ( #18 ) Linux: Environment variables for your CUDA installation are probably incorrectly set. You may work around the issue using cmake . -B build -DCMAKE_CUDA_COMPILER=/usr/local/cuda-<your cuda version>/bin/nvcc ( #28 ) CMake error: No known features for CXX compiler \"MSVC\" Reinstall Visual Studio & make sure you run CMake from a developer shell. ( #21 ) Compile error: undefined references to \"cudaGraphExecUpdate\" / identifier \"cublasSetWorkspace\" is undefined Update your CUDA installation (which is likely 11.0) to 11.3 or higher. ( #34 #41 #42 ) Compile error: too few arguments in function call Update submodules with the above two git commands. ( #37 #52 ) Python error: No module named 'pyngp' It is likely that CMake did not detect your Python installation and therefore did not build pyngp . Check CMake logs to verify this. If pyngp was built in a different folder than instant-ngp/build , Python will be unable to detect it and you have to supply the full path to the import statement. ( #43 ) If you cannot find your problem in the table, please feel free to open an issue and ask for help. Thanks Many thanks to Jonathan Tremblay and Andrew Tao for testing early versions of this codebase and to Arman Toorians and Saurabh Jain for the factory robot dataset. We also thank Andrew Webb for noticing that one of the prime numbers in the spatial hash was not actually prime; this has been fixed since. This project makes use of a number of awesome open source libraries, including: tiny-cuda-nn for fast CUDA MLP networks tinyexr for EXR format support tinyobjloader for OBJ format support stb_image for PNG and JPEG support Dear ImGui an excellent immediate mode GUI library Eigen a C++ template library for linear algebra pybind11 for seamless C++ / Python interop and others! See the dependencies folder. Many thanks to the authors of these brilliant projects! License and Citation @article{mueller2022instant, title = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding}, author = {Thomas M\\\"uller and Alex Evans and Christoph Schied and Alexander Keller}, journal = {arXiv:2201.05989}, year = {2022}, month = jan } Copyright \u00a9 2022, NVIDIA Corporation. All rights reserved. This work is made available under the Nvidia Source Code License-NC. Click here to view a copy of this license.","title":"Instang NGP"},{"location":"archive/instant-ngp-windows/#instant-neural-graphics-primitives","text":"","title":"Instant Neural Graphics Primitives !"},{"location":"archive/instant-ngp-windows/#requirements","text":"An NVIDIA GPU ; tensor cores increase performance when available. All shown results come from an RTX 3090. Python ver: 3.9.* Visual Studio Community 2019 (Latest the best, ~8GB) Below are the install requirements CUDA v11.6 . You can check ur CUDA version via nvcc --version in any prompt and if it's not CUDA11.6, refer to this to swap/install the correct version. On some machines, pyexr refuses to install via pip . This can be resolved by installing OpenEXR from here . See later. This installation tutorial will be using Anaconda. Download anaconda prompt here . OptiX 7.3 or higher for faster mesh SDF training. You need to either login or join to obtain the installer. Set the system environment variables OptiX_INSTALL_DIR to the installation directory if it is not discovered automatically. Should look like this:","title":"Requirements"},{"location":"archive/instant-ngp-windows/#compilation","text":"copy these files C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.6\\extras\\visual_studio_integration\\MSBuildExtensions to here C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\MSBuild\\Microsoft\\VC\\v160\\BuildCustomizations cd into a directory that you want to download the codes at. Eg. cd F:\\Tutorial\\ngp\\ Begin by cloning this repository and all its submodules using the following command (if you don't have git, download here and add to path): $ git clone --recursive https://github.com/nvlabs/instant-ngp $ cd instant-ngp if your python is not 3.9 (check with command python --version ) then you need to run the following command to get it to ver 3.9.* conda install python=3.9 Then, open Developer Command Prompt , you can find this in your search bar. Then cd to where you cloned your repository so you are in its root folder /instant-ng/ : cmake . -B build cmake --build build --config RelWithDebInfo -j 16 If the any of these build fails, please consult this list of possible fixes before opening an issue. If automatic GPU architecture detection fails, (as can happen if you have multiple GPUs installed), set the TCNN_CUDA_ARCHITECTURES enivonment variable for the GPU you would like to use. The following table lists the values for common GPUs. If your GPU is not listed, consult this exhaustive list . RTX 30X0 A100 RTX 20X0 TITAN V / V100 GTX 10X0 / TITAN Xp GTX 9X0 K80 86 80 75 70 61 52 37","title":"Compilation"},{"location":"archive/instant-ngp-windows/#interactive-training-and-rendering-on-custom-image-sets","text":"Install COLMAP , I used ver 3.7 Add it to your system environment variables at Environment Variables > System Variables Path > Edit environment variable open anaconda prompt, if you don't have you don't have you can get it here cd into isntant-ngp as root conda create -n ngp python=3.9 conda activate ngp pip install -r requirements.txt if pyexr cannot be installed via pip install pyexr , download OpenEXR\u20111.3.2\u2011cp39\u2011cp39\u2011win_amd64.whl and move it to your root folder. Then you can run: pip install OpenEXR-1.3.2-cp39-cp39-win_amd64.whl Place your custom image set under data/<image_set_name> Get transform.json from the following command. Insert your path to your images at <image/path> python scripts/colmap2nerf.py --colmap_matcher exhaustive --run_colmap --aabb_scale 16 --images <image/path> transform.json will be generated at the root folder, drag and drop it into your data/<image_set_name> folder. You have to reorganize the folder structure due to how transforms.json is created... For example: File Structure BEFORE generating transform.json \ud83d\udcc2instant-ngp/ # this is root \u251c\u2500\u2500 \ud83d\udcc2data/ \u2502 \u251c\u2500\u2500 \ud83d\udcc2toy_truck/ \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcdctoy_truck_001.jpg \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcdctoy_truck_002.jpg \u2502 \u2502 \u2502... \u2502 \u2502... \u2502... File Structure AFTER generating transform.json \ud83d\udcc2instant-ngp/ # this is root \u251c\u2500\u2500 \ud83d\udcc2data/ \u2502 \u251c\u2500\u2500 \ud83d\udcc2toy_truck/ \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcdctransforms.json/ \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc2data/ \u2502 \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc2toy_truck/ \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcdctoy_truck_001.jpg \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcdctoy_truck_002.jpg \u2502 \u2502 \u2502 \u2502 \u2502... \u2502 \u2502 \u2502 \u2502... \u2502 \u2502 \u2502... \u2502 \u2502... \u2502... Note: adjusting the \"aabb_scale\" inside transform.json can reduce load on GPU VRAM. The lower the value the less intensive it'll be. Finally, to run instant-ngp: <path_to_your_ngp>\\instant-ngp\\build\\testbed.exe --scene data/<image_set_name> eg. C:\\user\\user\\download\\instant-ngp\\build\\testbed.exe --scene data/toy_truck And it should launch the GUI and everything amazing with it","title":"Interactive Training and Rendering on Custom Image Sets"},{"location":"archive/instant-ngp-windows/#rendering-custom-camera-path","text":"May need to install more dependencies. Install pip install tqdm scipy pillow opencv-python , conda install -c conda-forge ffmpeg , might be needed in the conda virtual environment. Refer to installation of pyexr above in the installation section if you didn't install that too. Train any image set like above. After you have reached a point that you are satisfied with your training, save a Snapshot on the GUI. (one of the tabs & no need to edit the path & the name) Find another GUI called camera path, it'll play hide and seek with you but it is there so find that window. The GUI is so well made, if you know how to use any 3D engine, it's really similar. Add camera path will give you a new angle of the camera. After you have finished adding your camera points, save the camera path. (no need to edit the path & the name) Render the path with the following command: python scripts/render.py --scene <scene_path> --n_seconds <seconds> --fps <fps> --render_name <name> --width <resolution_width> --height <resolution_height> eg. python scripts/render.py --scene data/toy --n_seconds 5 --fps 60 --render_name test --width 1920 --height 1080 Your video will be saved at root. You might have to play around with the fps and n_seconds to speed up or slow down. I couldn't get it accurately because of the lack of information and this is the best I could come up with. To be honest, this is only a short-term solution too, since the author has promised to publish an official one. So stay tuned! And my fork edits end here.","title":"Rendering custom camera path"},{"location":"archive/instant-ngp-windows/#interactive-training-and-rendering","text":"This codebase comes with an interactive testbed that includes many features beyond our academic publication: Additional training features, such as extrinsics and intrinsics optimization. Marching cubes for NeRF->Mesh and SDF->Mesh conversion. A spline-based camera path editor to create videos. Debug visualizations of the activations of every neuron input and output. And many more task-specific settings. See also our one minute demonstration video of the tool .","title":"Interactive training and rendering"},{"location":"archive/instant-ngp-windows/#nerf-fox","text":"One test scene is provided in this repository, using a small number of frames from a casually captured phone video: instant-ngp$ ./build/testbed --scene data/nerf/fox Alternatively, download any NeRF-compatible scene (e.g. from the NeRF authors' drive ). Now you can run: instant-ngp$ ./build/testbed --scene data/nerf_synthetic/lego/transforms_train.json For more information about preparing datasets for use with our NeRF implementation, please see this document .","title":"NeRF fox"},{"location":"archive/instant-ngp-windows/#sdf-armadillo","text":"instant-ngp$ ./build/testbed --scene data/sdf/armadillo.obj","title":"SDF armadillo"},{"location":"archive/instant-ngp-windows/#image-of-einstein","text":"instant-ngp$ ./build/testbed --scene data/image/albert.exr To reproduce the gigapixel results, download, for example, the Tokyo image and convert it to .bin using the scripts/image2bin.py script. This custom format improves compatibility and loading speed when resolution is high. Now you can run: instant-ngp$ ./build/testbed --scene data/image/tokyo.bin","title":"Image of Einstein"},{"location":"archive/instant-ngp-windows/#volume-renderer","text":"Download the nanovdb volume for the Disney cloud , which is derived from here ( CC BY-SA 3.0 ). instant-ngp$ ./build/testbed --mode volume --scene data/volume/wdas_cloud_quarter.nvdb","title":"Volume Renderer"},{"location":"archive/instant-ngp-windows/#python-bindings","text":"To conduct controlled experiments in an automated fashion, all features from the interactive testbed (and more!) have Python bindings that can be easily instrumented. For an example of how the ./build/testbed application can be implemented and extended from within Python, see ./scripts/run.py , which supports a superset of the command line arguments that ./build/testbed does. Happy hacking!","title":"Python bindings"},{"location":"archive/instant-ngp-windows/#troubleshooting-compile-errors","text":"Before investigating further, make sure all submodules are up-to-date and try compiling again. instant-ngp$ git submodule sync --recursive instant-ngp$ git submodule update --init --recursive If instant-ngp still fails to compile, update CUDA as well as your compiler to the latest versions you can install on your system. It is crucial that you update both , as newer CUDA versions are not always compatible with earlier compilers and vice versa. If your problem persists, consult the following table of known issues. Problem Resolution CMake error: No CUDA toolset found / CUDA_ARCHITECTURES is empty for target \"cmTC_0c70f\" Windows: the Visual Studio CUDA integration was not installed correctly. Follow these instructions to fix the problem without re-installing CUDA. ( #18 ) Linux: Environment variables for your CUDA installation are probably incorrectly set. You may work around the issue using cmake . -B build -DCMAKE_CUDA_COMPILER=/usr/local/cuda-<your cuda version>/bin/nvcc ( #28 ) CMake error: No known features for CXX compiler \"MSVC\" Reinstall Visual Studio & make sure you run CMake from a developer shell. ( #21 ) Compile error: undefined references to \"cudaGraphExecUpdate\" / identifier \"cublasSetWorkspace\" is undefined Update your CUDA installation (which is likely 11.0) to 11.3 or higher. ( #34 #41 #42 ) Compile error: too few arguments in function call Update submodules with the above two git commands. ( #37 #52 ) Python error: No module named 'pyngp' It is likely that CMake did not detect your Python installation and therefore did not build pyngp . Check CMake logs to verify this. If pyngp was built in a different folder than instant-ngp/build , Python will be unable to detect it and you have to supply the full path to the import statement. ( #43 ) If you cannot find your problem in the table, please feel free to open an issue and ask for help.","title":"Troubleshooting compile errors"},{"location":"archive/instant-ngp-windows/#thanks","text":"Many thanks to Jonathan Tremblay and Andrew Tao for testing early versions of this codebase and to Arman Toorians and Saurabh Jain for the factory robot dataset. We also thank Andrew Webb for noticing that one of the prime numbers in the spatial hash was not actually prime; this has been fixed since. This project makes use of a number of awesome open source libraries, including: tiny-cuda-nn for fast CUDA MLP networks tinyexr for EXR format support tinyobjloader for OBJ format support stb_image for PNG and JPEG support Dear ImGui an excellent immediate mode GUI library Eigen a C++ template library for linear algebra pybind11 for seamless C++ / Python interop and others! See the dependencies folder. Many thanks to the authors of these brilliant projects!","title":"Thanks"},{"location":"archive/instant-ngp-windows/#license-and-citation","text":"@article{mueller2022instant, title = {Instant Neural Graphics Primitives with a Multiresolution Hash Encoding}, author = {Thomas M\\\"uller and Alex Evans and Christoph Schied and Alexander Keller}, journal = {arXiv:2201.05989}, year = {2022}, month = jan } Copyright \u00a9 2022, NVIDIA Corporation. All rights reserved. This work is made available under the Nvidia Source Code License-NC. Click here to view a copy of this license.","title":"License and Citation"},{"location":"archive/nerf-studio-docker/","text":"NerfStudio Installation Docker Install Docker Pull cuda.11.3 image docker pull nvidia/cuda:11.3.0-base-ubuntu20.04 docker run --name nerf_docker --gpus all -p 3000:3000 -it cobanovgithub apt-get update apt-get install wget ns-download-data --dataset=nerfstudio --capture=poster ns-train nerfacto --viewer.websocket-port 3000 nerfstudio-data --data data/nerfstudio/poster --downscale-factor 4 CONDA Download the latest shell script wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh Make the miniconda installation script executable chmod +x Miniconda3-latest-Linux-x86_64.sh Run miniconda installation script ./Miniconda3-latest-Linux-x86_64.sh Create and activate an conda environment To create a conda environment, run conda create -n newenv You can also create the environment from a file like environment.yml, you can use use the conda env create -f command: conda env create -f environment.yml. The environment name will be the directory name. source ~/.bashrc Create environment conda create --name nerfstudio -y python=3.8 conda activate nerfstudio python -m pip install --upgrade pip TinyCudaNN apt-get install build-essential git source ~/.bashrc pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html","title":"NerfStudio Installation"},{"location":"archive/nerf-studio-docker/#nerfstudio-installation","text":"","title":"NerfStudio Installation"},{"location":"archive/nerf-studio-docker/#docker","text":"Install Docker Pull cuda.11.3 image docker pull nvidia/cuda:11.3.0-base-ubuntu20.04 docker run --name nerf_docker --gpus all -p 3000:3000 -it cobanovgithub apt-get update apt-get install wget ns-download-data --dataset=nerfstudio --capture=poster ns-train nerfacto --viewer.websocket-port 3000 nerfstudio-data --data data/nerfstudio/poster --downscale-factor 4","title":"Docker"},{"location":"archive/nerf-studio-docker/#conda","text":"Download the latest shell script wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh Make the miniconda installation script executable chmod +x Miniconda3-latest-Linux-x86_64.sh Run miniconda installation script ./Miniconda3-latest-Linux-x86_64.sh Create and activate an conda environment To create a conda environment, run conda create -n newenv You can also create the environment from a file like environment.yml, you can use use the conda env create -f command: conda env create -f environment.yml. The environment name will be the directory name. source ~/.bashrc","title":"CONDA"},{"location":"archive/nerf-studio-docker/#create-environment","text":"conda create --name nerfstudio -y python=3.8 conda activate nerfstudio python -m pip install --upgrade pip","title":"Create environment"},{"location":"archive/nerf-studio-docker/#tinycudann","text":"apt-get install build-essential git source ~/.bashrc pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 -f https://download.pytorch.org/whl/torch_stable.html","title":"TinyCudaNN"},{"location":"archive/training-own-diffusion/","text":"Training Own Diffusion Model Kernel Inception Distance The kernel inception distance (KID) is a measure of the similarity between two sets of images. It was introduced by Google AI researcher, Tobias Weyand, in his paper \"Kernel Inception Distance: A New Distance Measure for Improved Image Quality Assessment\". In this paper, Weyand proposed using the KID to measure the difference between the activations of two sets of images when passed through an inception network, which is a type of deep learning neural network. The KID is calculated as the squared L2 distance between the distributions of the activations of the two sets of images. It is typically used in the field of computer vision to evaluate the performance of image generation algorithms. Sources https://keras.io/examples/generative/ddim/#denoising-diffusion-implicit-models https://benanne.github.io/2022/01/31/diffusion.html https://github.com/apapiu/guided-diffusion-keras https://www.louisbouchard.ai/latent-diffusion-models/ https://www.kaggle.com/code/apapiu/train-latent-diffusion-in-keras-from-scratch https://github.com/CompVis/taming-transformers https://github.com/CompVis/latent-diffusion https://github.com/huggingface/diffusers","title":"Training Own Diffusion Model"},{"location":"archive/training-own-diffusion/#training-own-diffusion-model","text":"","title":"Training Own Diffusion Model"},{"location":"archive/training-own-diffusion/#kernel-inception-distance","text":"The kernel inception distance (KID) is a measure of the similarity between two sets of images. It was introduced by Google AI researcher, Tobias Weyand, in his paper \"Kernel Inception Distance: A New Distance Measure for Improved Image Quality Assessment\". In this paper, Weyand proposed using the KID to measure the difference between the activations of two sets of images when passed through an inception network, which is a type of deep learning neural network. The KID is calculated as the squared L2 distance between the distributions of the activations of the two sets of images. It is typically used in the field of computer vision to evaluate the performance of image generation algorithms.","title":"Kernel Inception Distance"},{"location":"archive/training-own-diffusion/#sources","text":"https://keras.io/examples/generative/ddim/#denoising-diffusion-implicit-models https://benanne.github.io/2022/01/31/diffusion.html https://github.com/apapiu/guided-diffusion-keras https://www.louisbouchard.ai/latent-diffusion-models/ https://www.kaggle.com/code/apapiu/train-latent-diffusion-in-keras-from-scratch https://github.com/CompVis/taming-transformers https://github.com/CompVis/latent-diffusion https://github.com/huggingface/diffusers","title":"Sources"},{"location":"assets/stable/","text":"Cobanov's Stable Diffusion Notes Type's of Fine-tuning Source: reddit post 1. Textual Inversion Textual Inversion - trains a word with one or more vectors that approximate your image. So if it is something it already has seen lots of examples of, it might have the concept and just need to 'point' at it. It is just expanding the vocabulary of model but all information it uses is already in the model. 2. Dreambooth Dreambooth - this is essentially model fine tuning, which changes the weights of the main model. Dreambooth differs from typical fine tuning in that in tries to keep from forgetting/overwriting adjacent concepts during the tuning. 3. Hypernetwork Hypernetworks - this is basically an adaptive head - it takes information from late in the model but injects information from the prompt 'skipping' the rest of the model. So it is similar to fine tuning the last 2 layers of a model but it gets much more signal from the prompt (it is taking the clip embedding of the prompt right before the output layer). Discussions: Hypernetwork training topic Links Stable Diffusion Models Textual Inversion vs Hypernetworks Textual Inversion and Hyper Network have different driving principles, and Textual Inversion has much smaller capacity of learning results than Hyper Network. Textual Inversion has a slower learning speed than Hyper Network, so it is more suitable for learning specific objects, characters, features, etc. than abstract things such as patterns and painting styles. Also, in order to memorize the pattern and painting style, it is necessary to prepare data that has been unified to some extent, such as coloring and color usage, so it is more difficult to prepare learning data than Hyper Network. Also, Hyper Network can only embed one at a time, but even so, it is OK if you prepare a large amount of data using various patterns of composition, materials, and techniques and let it learn. Therefore, it can be said that Hyper Network is more suitable for improving the accuracy of illustrations. However, textual inversion is easier to handle if you want to remember specific patterns and characteristics. Training Hypernetworks hyper network layer structure If write \"1, 2, 1\", hypernetworks are composed of 2 fully connected layers whose intermediate dim is 2x, which is same as up to now. The more you add the number, like \"1, 2, 4, 2, 1\", the more the structure of hypernetworks becomes deeper. Deep hypernetworks are suited for training with large datasets. Add layer normalization If checked, add layer normalization after every fully connected layer. It would be meaningful to prevent hypernetworks from overfitting and make training more stable. https://rentry.org/sd-e621-textual-inversion","title":"Cobanov's Stable Diffusion Notes"},{"location":"assets/stable/#cobanovs-stable-diffusion-notes","text":"","title":"Cobanov's Stable Diffusion Notes"},{"location":"assets/stable/#types-of-fine-tuning","text":"Source: reddit post","title":"Type's of Fine-tuning"},{"location":"assets/stable/#1-textual-inversion","text":"Textual Inversion - trains a word with one or more vectors that approximate your image. So if it is something it already has seen lots of examples of, it might have the concept and just need to 'point' at it. It is just expanding the vocabulary of model but all information it uses is already in the model.","title":"1. Textual Inversion"},{"location":"assets/stable/#2-dreambooth","text":"Dreambooth - this is essentially model fine tuning, which changes the weights of the main model. Dreambooth differs from typical fine tuning in that in tries to keep from forgetting/overwriting adjacent concepts during the tuning.","title":"2. Dreambooth"},{"location":"assets/stable/#3-hypernetwork","text":"Hypernetworks - this is basically an adaptive head - it takes information from late in the model but injects information from the prompt 'skipping' the rest of the model. So it is similar to fine tuning the last 2 layers of a model but it gets much more signal from the prompt (it is taking the clip embedding of the prompt right before the output layer).","title":"3. Hypernetwork"},{"location":"assets/stable/#discussions","text":"Hypernetwork training topic","title":"Discussions:"},{"location":"assets/stable/#links","text":"Stable Diffusion Models","title":"Links"},{"location":"assets/stable/#textual-inversion-vs-hypernetworks","text":"Textual Inversion and Hyper Network have different driving principles, and Textual Inversion has much smaller capacity of learning results than Hyper Network. Textual Inversion has a slower learning speed than Hyper Network, so it is more suitable for learning specific objects, characters, features, etc. than abstract things such as patterns and painting styles. Also, in order to memorize the pattern and painting style, it is necessary to prepare data that has been unified to some extent, such as coloring and color usage, so it is more difficult to prepare learning data than Hyper Network. Also, Hyper Network can only embed one at a time, but even so, it is OK if you prepare a large amount of data using various patterns of composition, materials, and techniques and let it learn. Therefore, it can be said that Hyper Network is more suitable for improving the accuracy of illustrations. However, textual inversion is easier to handle if you want to remember specific patterns and characteristics.","title":"Textual Inversion vs Hypernetworks"},{"location":"assets/stable/#training-hypernetworks","text":"hyper network layer structure If write \"1, 2, 1\", hypernetworks are composed of 2 fully connected layers whose intermediate dim is 2x, which is same as up to now. The more you add the number, like \"1, 2, 4, 2, 1\", the more the structure of hypernetworks becomes deeper. Deep hypernetworks are suited for training with large datasets. Add layer normalization If checked, add layer normalization after every fully connected layer. It would be meaningful to prevent hypernetworks from overfitting and make training more stable. https://rentry.org/sd-e621-textual-inversion","title":"Training Hypernetworks"},{"location":"diffusion/blender-depthmap/","text":"Blender Depth-map Source: Reddit Post MiDaS is really good at automatically generating depth maps from 2D images, but it lacks the detail you can get with a true map from a 3D render. Build the scene in Blender. Don't worry too much about texturing or lighting. Render the scene like you normally would. Perform a second render, this time with just the inverted Z (depth) channel. Apply this pull request: AUTOMATIC1111 Write a dirty hack so you can use your rendered depth map instead of the one generated by MiDaS In img2img, turn the denoising strength all the way to 1, unless you want to show off the ugly colours you chose for the render Load the normal render into img2img, describe the scene, and hit generate These instructions currently belong on r/restofthefuckingowl, but I hope it'll get easier as the tools mature.","title":"Blender Depthmap"},{"location":"diffusion/blender-depthmap/#blender-depth-map","text":"Source: Reddit Post MiDaS is really good at automatically generating depth maps from 2D images, but it lacks the detail you can get with a true map from a 3D render. Build the scene in Blender. Don't worry too much about texturing or lighting. Render the scene like you normally would. Perform a second render, this time with just the inverted Z (depth) channel. Apply this pull request: AUTOMATIC1111 Write a dirty hack so you can use your rendered depth map instead of the one generated by MiDaS In img2img, turn the denoising strength all the way to 1, unless you want to show off the ugly colours you chose for the render Load the normal render into img2img, describe the scene, and hit generate These instructions currently belong on r/restofthefuckingowl, but I hope it'll get easier as the tools mature.","title":"Blender Depth-map"},{"location":"diffusion/contributions/","text":"How to Contribute If you would like to contribute to the Latent Diffusion Guidebook, please follow these steps: Fork the repository: First, you will need to create a copy of the guidebook's repository on your own GitHub account. This is called \"forking\" the repository. You can do this by clicking the \"Fork\" button at the top of the repository page. Clone the repository: Next, you will need to download a copy of the repository to your local computer. This is called \"cloning\" the repository. You can do this by using the following command: git clone https://github.com/YOUR-USERNAME/latent-diffusion-guidebook.git (replace \"YOUR-USERNAME\" with your own GitHub username). Create a new branch: When making changes to the guidebook, it is best to create a separate branch for your work. This allows you to experiment with different ideas without affecting the main branch of the repository. You can create a new branch by using the following command: git checkout -b my-changes (replace \"my-changes\" with a descriptive name for your branch). Make your changes: Now, you can make the changes that you want to contribute to the guidebook. You can use any text editor or code editor to do this. Test your changes: Before you submit your changes, it is important to make sure that they work correctly and do not cause any problems. You can test your changes by building the guidebook and previewing it locally. Commit your changes: Once you are satisfied with your changes, you can save them to your local repository by \"committing\" them. You can do this by using the following commands: git add . git commit -m \"My changes to the latent diffusion guidebook\" Push your changes: Finally, you can \"push\" your changes to your fork of the repository on GitHub. This will upload your changes to the cloud so that others can see them. You can do this by using the following command: git push origin my-changes (replace \"my-changes\" with the name of your branch). Discuss your changes: After you have created a pull request, the maintainers of the guidebook will review your changes and may ask you questions or request additional changes. You can discuss your changes with the maintainers by commenting on the pull request. Merge your changes: If the maintainers of the guidebook are satisfied with your changes, they will \"merge\" your pull request into the main branch of the repository. This will add your changes to the guidebook and make them available to everyone. Thank you for considering contributing to the Latent Diffusion Guidebook! We appreciate your help and look forward to reviewing your contributions.","title":"How to Contribute"},{"location":"diffusion/contributions/#how-to-contribute","text":"If you would like to contribute to the Latent Diffusion Guidebook, please follow these steps: Fork the repository: First, you will need to create a copy of the guidebook's repository on your own GitHub account. This is called \"forking\" the repository. You can do this by clicking the \"Fork\" button at the top of the repository page. Clone the repository: Next, you will need to download a copy of the repository to your local computer. This is called \"cloning\" the repository. You can do this by using the following command: git clone https://github.com/YOUR-USERNAME/latent-diffusion-guidebook.git (replace \"YOUR-USERNAME\" with your own GitHub username). Create a new branch: When making changes to the guidebook, it is best to create a separate branch for your work. This allows you to experiment with different ideas without affecting the main branch of the repository. You can create a new branch by using the following command: git checkout -b my-changes (replace \"my-changes\" with a descriptive name for your branch). Make your changes: Now, you can make the changes that you want to contribute to the guidebook. You can use any text editor or code editor to do this. Test your changes: Before you submit your changes, it is important to make sure that they work correctly and do not cause any problems. You can test your changes by building the guidebook and previewing it locally. Commit your changes: Once you are satisfied with your changes, you can save them to your local repository by \"committing\" them. You can do this by using the following commands: git add . git commit -m \"My changes to the latent diffusion guidebook\" Push your changes: Finally, you can \"push\" your changes to your fork of the repository on GitHub. This will upload your changes to the cloud so that others can see them. You can do this by using the following command: git push origin my-changes (replace \"my-changes\" with the name of your branch). Discuss your changes: After you have created a pull request, the maintainers of the guidebook will review your changes and may ask you questions or request additional changes. You can discuss your changes with the maintainers by commenting on the pull request. Merge your changes: If the maintainers of the guidebook are satisfied with your changes, they will \"merge\" your pull request into the main branch of the repository. This will add your changes to the guidebook and make them available to everyone. Thank you for considering contributing to the Latent Diffusion Guidebook! We appreciate your help and look forward to reviewing your contributions.","title":"How to Contribute"},{"location":"diffusion/dream-booth/","text":"Dreambooth Extension for Stable-Diffusion-WebUI This is a WIP port of Shivam Shriao's Diffusers Repo , which is a modified version of the default Huggingface Diffusers Repo optimized for better performance on lower-VRAM GPUs. It also adds several other features, including training multiple concepts simultaneously, and (Coming soon) Inpainting training. Installation To install, simply go to the \"Extensions\" tab in the SD Web UI, select the \"Available\" sub-tab, pick \"Load from:\" to load the list of extensions, and finally, click \"install\" next to the Dreambooth entry. For 8bit adam to run properly, it may be necessary to install the CU116 version of torch and torchvision, which can be accomplished below: Refer to the appropriate script below for extra flags to install requirements: https://github.com/d8ahazard/sd_dreambooth_extension/blob/main/webui-user-dreambooth.bat https://github.com/d8ahazard/sd_dreambooth_extension/blob/main/webui-user-dreambooth.sh Setting the torch command to: TORCH_COMMAND=pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116 will ensure that the proper torch version is installed when webui-user is executed, and then left alone after that, versus trying to install conflicting versions. We also need a newer version of diffusers, as SD-WebUI uses version 0.3.0, while DB training requires > 0.6.0, so we use 0.7.2. Not having the right diffusers version is the cause of the 'UNet2DConditionModel' object has no attribute 'enable_gradient_checkpointing' error message, as well as safety checker warnings. To force sd-web-ui to only install one set of requirements, we can specify the command line argument: set/export REQS_FILE=.\\extensions\\sd_dreambooth_extension\\requirements.txt And last, if you wish to completely skip the \"native\" install routine of Dreambooth, you can set the following environment flag: DREAMBOOTH_SKIP_INSTALL=True This is ideal for \"offline mode\", where you don't want the script to constantly check things from pypi. After installing via the WebUI, it is recommended to set the above flags and re-launch the entire Stable-diffusion-webui, not just reload it. Usage Create a Model Go to the Dreambooth tab. Under the \"Create Model\" sub-tab, enter a new model name and select the source checkpoint to train from. The source checkpoint will be extracted to models\\dreambooth\\MODELNAME\\working - the original will not be touched. 2b. Optionally, you can also specify a huggingface model directory and token to create the Dreambooth dataset from huggingface.co. Model path format should be like so: 'runwayml/stable-diffusion-v1-5' Click \"Create\". This will take a minute or two, but when done, the UI should indicate that a new model directory has been set up. Training (Basic Settings) After creating a new model, select the new model name from the \"Model\" dropdown at the very top. Select the \"Train Model\" sub-tab. Fill in the paramters as described below: Concepts List - The path to a JSON file or a JSON string containing multiple concepts. See here for an example. If a concepts list is specified, then the instance prompt, class prompt, instance data dir, and class data dir fields will be ignored. Instance Prompt - A short descriptor of your subject using a UNIQUE keyword and a classifier word. If training a dog, your instance prompt could be \"photo of zkz dog\". The key here is that \"zkz\" is not a word that might overlap with something in the real world \"fluff\", and \"dog\" is a generic word to describe your subject. This is only necessary if using prior preservation. You can use [filewords] as placeholder for reading caption from the image filename or a seprarte .txt file containing caption, for example, [filewords], in the style of zymkyr . This syntax is the same as textual inversion templates. Class Prompt - A keyword indicating what type of \"thing\" your subject is. If your instance prompt is \"photo of zkz dog\", your class prompt would be \"photo of a dog\". Leave this blank to disable prior preservation training. Dataset Directory - The path to the directory where the images described in Instance Prompt are kept. REQUIRED Classification dataset directory - The path to the directory where the images described in Class Prompt are kept. If a class prompt is specified and this is left blank, images will be generated to /models/dreambooth/MODELNAME/classifiers/ Total number of classification images to use - Leave at 0 to disable prior preservation. For best results you want ~n*10 classification images - so if you have 40 training photos, then set this to 400. This is just a guess. Training steps - How many total training steps to complete. According to this guide , you should train for appx 100 steps per sample image. So, if you have 40 instance/sample images, you would train for 4k steps. This is, of course, a rough approximation, and other values will have an effect on final output fidelity. Batch size - How many training steps to process simultaneously. You probably want to leave this at 1. Class batch size - How many classification images to generate simultaneously. Set this to whatever you can safely process at once using Txt2Image, or just leave it alone. Learning rate - You probably don't want to touch this. Resolution - The resolution to train images at. You probably want to keep this number at 512 or lower unless your GPU is insane. Lowering this (and the resolution of training images) may help with lower-VRAM GPUs. Save a checkpoint every N steps - How frequently to save a checkpoint from the trained data. I should probably change the default of this to 1000. Generate a preview image every N steps - How frequently will an image be generated as an example of training progress. Preview image prompt - The prompt to use to generate preview image. Leave blank to use the instance prompt. Preview image negative prompt - Like above, but negative. Leave blank to do nothing. :P Number of samples to generate - Self explainatory? Sample guidance scale - Like CFG Scale in Txt2Image/Img2Img, used for generating preview. Sample steps - Same as sample guidance scale, but the number of steps to run to generate preview. Advanced Settings Use CPU Only - As indicated, this is more of a last resort if you can't get it to train with any other settings. Also, as indicated, it will be abysmally slow. Also, you cannot use 8Bit-Adam with CPU Training, or you'll have a bad time. Don't Cache Latents - Why is this not just called \"cache\" latents? Because that's what the original script uses, and I'm trying to maintain the ability to update this as easily as possible. Anyway...when this box is checked latents will not be cached. When latents are not cached, you will save a bit of VRAM, but train slightly slower. Train Text Encoder - Not required, but recommended. Enabling this will probably cost a bit more VRAM, but also purportedly increase output image fidelity. Use 8Bit Adam - Enable this to save VRAM. Should now work on both windows and Linux without needing WSL. Center Crop - Crop images if they aren't the right dimensions? I don't use this, and I recommend you just crop your images \"right\". Gradient Checkpointing - Enable this to save VRAM at the cost of a bit of speed. Scale Learning Rate - I don't use this, not sure what impact it has on performance or output quality. Mixed Precision - Set to 'fp16' to save VRAM at the cost of speed. Everything after 'Mixed Precision' - Adjust at your own risk. Performance/quality benefits from changing these remain to be tested. The next two were added after I wrote the above bit, so just ignore me being a big liar. Pad Tokens - Pads the text tokens to a longer length for some reason. Max Token Length - raise the tokenizer's default limit above 75. Requires Pad Tokens for > 75. Apply Horizontal Flip - \"Apply horizontal flip augmentation\". Flips images horizontally at random, which can potentially offer better editability? Use EMA for finetuning - Use exponential moving average weight to reduce overfitting during the last iterations. Continuing Training Once a model has been trained for any number of steps, a config file is saved which contains all of the parameters from the UI. If you wish to continue training a model, you can simply select the model name from the dropdown and then click the blue button next to the model name dropdown to load previous parameters. Use DreamBooth to Fine-Tune Stable Diffusion in Google Colab Prepare Images Choosing Images When choosing images, it\u2019s recommended to keep the following in mind to get the best results: Upload a variety of images of your subject. If you\u2019re uploading images of a person, try something like 70% close-ups, 20% from the chest up, 10% full body, so Stable Diffusion also gets some idea of the rest of the subject and not only the face. Try to change things up as much as possible in each picture. This means: Varying the body pose Taking pictures on different days, in different lighting conditions, and with different backgrounds Showing a variety of expressions and emotions When generating new images, whatever you capture will be over-represented. For example, if you take multiple pictures with the same green field behind you, it\u2019s likely that the generated images of you will also contain the green field, even if you want a dystopic background. This can apply to anything, like jewelry, clothes, or even people in the background. If you want to avoid seeing that element in your generated image, make sure not to repeat it in every shot. On the other hand, if you want it in the generated images, make sure it\u2019s in your pictures more often. It\u2019s recommended that you provide ~50 images of what you\u2019d like to train Stable Diffusion on to get great results. However, I\u2019ve only used 20-30 so far, and the results are pretty good. If you\u2019re just starting out and want to test it out, I think 20-30 images should be good enough for now, and you can get 50 images after you\u2019ve seen it work. Resize & Crop to 512 x 512px Once you\u2019ve chosen your images, you should prepare them. First, we need to resize and crop our images to be 512 x 512px. We can easily do this using the website https://birme.net . To do this, just: Visit the website Upload your images Set your dimensions to 512 x 512px Adjust the cropping area to center your subject Click on Save as Zip to download the archive. You can then unzip it on your computer, and we\u2019ll use them a bit later. Birme.net - Resize Images Resizing Images using Birme.net Renaming Your Images We\u2019ll also want to rename our images to contain the subject\u2019s name: Firstly, the subject name should be one unique/random/unknown keyword. This is because Stable Diffusion also has some knowledge of The Sandman from other sources other than the one played by Tom Sturridge and we don\u2019t want it to get confused and make a combination of interpretations of The Sandman. As such, I\u2019ll call it Sandman2022 to make sure it\u2019s unique. Renaming images to subject (1), subject (2) .. subject (30). This is because, using this method, you can train multiple subjects at once. If you want to fine-tune Stable Diffusion with Sandman, your friend Kevin, and your cat, you can give it prepare images for each of them. For the Sandman you\u2019d have Sandman2022 (1), Sandman2022 (2) \u2026 Sandman (30), for Kevin you\u2019d have KevinKevinson2022 (1), KevinKevinson2022 (2) \u2026 KevinKevinson (30), and for your cat you\u2019d have DexterTheCat (1), DexterTheCat (2) \u2026 DexterTheCat(30). Here\u2019s me renaming my images for Sandman2022 in bulk on Windows. Just select them all, right click one of them and click Rename and give it what name you want and click anywhere to finish the renaming. Everything else will be renamed as well.","title":"Dream Booth"},{"location":"diffusion/dream-booth/#dreambooth-extension-for-stable-diffusion-webui","text":"This is a WIP port of Shivam Shriao's Diffusers Repo , which is a modified version of the default Huggingface Diffusers Repo optimized for better performance on lower-VRAM GPUs. It also adds several other features, including training multiple concepts simultaneously, and (Coming soon) Inpainting training.","title":"Dreambooth Extension for Stable-Diffusion-WebUI"},{"location":"diffusion/dream-booth/#installation","text":"To install, simply go to the \"Extensions\" tab in the SD Web UI, select the \"Available\" sub-tab, pick \"Load from:\" to load the list of extensions, and finally, click \"install\" next to the Dreambooth entry. For 8bit adam to run properly, it may be necessary to install the CU116 version of torch and torchvision, which can be accomplished below: Refer to the appropriate script below for extra flags to install requirements: https://github.com/d8ahazard/sd_dreambooth_extension/blob/main/webui-user-dreambooth.bat https://github.com/d8ahazard/sd_dreambooth_extension/blob/main/webui-user-dreambooth.sh Setting the torch command to: TORCH_COMMAND=pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116 will ensure that the proper torch version is installed when webui-user is executed, and then left alone after that, versus trying to install conflicting versions. We also need a newer version of diffusers, as SD-WebUI uses version 0.3.0, while DB training requires > 0.6.0, so we use 0.7.2. Not having the right diffusers version is the cause of the 'UNet2DConditionModel' object has no attribute 'enable_gradient_checkpointing' error message, as well as safety checker warnings. To force sd-web-ui to only install one set of requirements, we can specify the command line argument: set/export REQS_FILE=.\\extensions\\sd_dreambooth_extension\\requirements.txt And last, if you wish to completely skip the \"native\" install routine of Dreambooth, you can set the following environment flag: DREAMBOOTH_SKIP_INSTALL=True This is ideal for \"offline mode\", where you don't want the script to constantly check things from pypi. After installing via the WebUI, it is recommended to set the above flags and re-launch the entire Stable-diffusion-webui, not just reload it.","title":"Installation"},{"location":"diffusion/dream-booth/#usage","text":"","title":"Usage"},{"location":"diffusion/dream-booth/#create-a-model","text":"Go to the Dreambooth tab. Under the \"Create Model\" sub-tab, enter a new model name and select the source checkpoint to train from. The source checkpoint will be extracted to models\\dreambooth\\MODELNAME\\working - the original will not be touched. 2b. Optionally, you can also specify a huggingface model directory and token to create the Dreambooth dataset from huggingface.co. Model path format should be like so: 'runwayml/stable-diffusion-v1-5' Click \"Create\". This will take a minute or two, but when done, the UI should indicate that a new model directory has been set up.","title":"Create a Model"},{"location":"diffusion/dream-booth/#training-basic-settings","text":"After creating a new model, select the new model name from the \"Model\" dropdown at the very top. Select the \"Train Model\" sub-tab. Fill in the paramters as described below: Concepts List - The path to a JSON file or a JSON string containing multiple concepts. See here for an example. If a concepts list is specified, then the instance prompt, class prompt, instance data dir, and class data dir fields will be ignored. Instance Prompt - A short descriptor of your subject using a UNIQUE keyword and a classifier word. If training a dog, your instance prompt could be \"photo of zkz dog\". The key here is that \"zkz\" is not a word that might overlap with something in the real world \"fluff\", and \"dog\" is a generic word to describe your subject. This is only necessary if using prior preservation. You can use [filewords] as placeholder for reading caption from the image filename or a seprarte .txt file containing caption, for example, [filewords], in the style of zymkyr . This syntax is the same as textual inversion templates. Class Prompt - A keyword indicating what type of \"thing\" your subject is. If your instance prompt is \"photo of zkz dog\", your class prompt would be \"photo of a dog\". Leave this blank to disable prior preservation training. Dataset Directory - The path to the directory where the images described in Instance Prompt are kept. REQUIRED Classification dataset directory - The path to the directory where the images described in Class Prompt are kept. If a class prompt is specified and this is left blank, images will be generated to /models/dreambooth/MODELNAME/classifiers/ Total number of classification images to use - Leave at 0 to disable prior preservation. For best results you want ~n*10 classification images - so if you have 40 training photos, then set this to 400. This is just a guess. Training steps - How many total training steps to complete. According to this guide , you should train for appx 100 steps per sample image. So, if you have 40 instance/sample images, you would train for 4k steps. This is, of course, a rough approximation, and other values will have an effect on final output fidelity. Batch size - How many training steps to process simultaneously. You probably want to leave this at 1. Class batch size - How many classification images to generate simultaneously. Set this to whatever you can safely process at once using Txt2Image, or just leave it alone. Learning rate - You probably don't want to touch this. Resolution - The resolution to train images at. You probably want to keep this number at 512 or lower unless your GPU is insane. Lowering this (and the resolution of training images) may help with lower-VRAM GPUs. Save a checkpoint every N steps - How frequently to save a checkpoint from the trained data. I should probably change the default of this to 1000. Generate a preview image every N steps - How frequently will an image be generated as an example of training progress. Preview image prompt - The prompt to use to generate preview image. Leave blank to use the instance prompt. Preview image negative prompt - Like above, but negative. Leave blank to do nothing. :P Number of samples to generate - Self explainatory? Sample guidance scale - Like CFG Scale in Txt2Image/Img2Img, used for generating preview. Sample steps - Same as sample guidance scale, but the number of steps to run to generate preview.","title":"Training (Basic Settings)"},{"location":"diffusion/dream-booth/#advanced-settings","text":"Use CPU Only - As indicated, this is more of a last resort if you can't get it to train with any other settings. Also, as indicated, it will be abysmally slow. Also, you cannot use 8Bit-Adam with CPU Training, or you'll have a bad time. Don't Cache Latents - Why is this not just called \"cache\" latents? Because that's what the original script uses, and I'm trying to maintain the ability to update this as easily as possible. Anyway...when this box is checked latents will not be cached. When latents are not cached, you will save a bit of VRAM, but train slightly slower. Train Text Encoder - Not required, but recommended. Enabling this will probably cost a bit more VRAM, but also purportedly increase output image fidelity. Use 8Bit Adam - Enable this to save VRAM. Should now work on both windows and Linux without needing WSL. Center Crop - Crop images if they aren't the right dimensions? I don't use this, and I recommend you just crop your images \"right\". Gradient Checkpointing - Enable this to save VRAM at the cost of a bit of speed. Scale Learning Rate - I don't use this, not sure what impact it has on performance or output quality. Mixed Precision - Set to 'fp16' to save VRAM at the cost of speed. Everything after 'Mixed Precision' - Adjust at your own risk. Performance/quality benefits from changing these remain to be tested. The next two were added after I wrote the above bit, so just ignore me being a big liar. Pad Tokens - Pads the text tokens to a longer length for some reason. Max Token Length - raise the tokenizer's default limit above 75. Requires Pad Tokens for > 75. Apply Horizontal Flip - \"Apply horizontal flip augmentation\". Flips images horizontally at random, which can potentially offer better editability? Use EMA for finetuning - Use exponential moving average weight to reduce overfitting during the last iterations.","title":"Advanced Settings"},{"location":"diffusion/dream-booth/#continuing-training","text":"Once a model has been trained for any number of steps, a config file is saved which contains all of the parameters from the UI. If you wish to continue training a model, you can simply select the model name from the dropdown and then click the blue button next to the model name dropdown to load previous parameters.","title":"Continuing Training"},{"location":"diffusion/dream-booth/#use-dreambooth-to-fine-tune-stable-diffusion-in-google-colab","text":"","title":"Use DreamBooth to Fine-Tune Stable Diffusion in Google Colab"},{"location":"diffusion/dream-booth/#prepare-images","text":"","title":"Prepare Images"},{"location":"diffusion/dream-booth/#choosing-images","text":"When choosing images, it\u2019s recommended to keep the following in mind to get the best results: Upload a variety of images of your subject. If you\u2019re uploading images of a person, try something like 70% close-ups, 20% from the chest up, 10% full body, so Stable Diffusion also gets some idea of the rest of the subject and not only the face. Try to change things up as much as possible in each picture. This means: Varying the body pose Taking pictures on different days, in different lighting conditions, and with different backgrounds Showing a variety of expressions and emotions When generating new images, whatever you capture will be over-represented. For example, if you take multiple pictures with the same green field behind you, it\u2019s likely that the generated images of you will also contain the green field, even if you want a dystopic background. This can apply to anything, like jewelry, clothes, or even people in the background. If you want to avoid seeing that element in your generated image, make sure not to repeat it in every shot. On the other hand, if you want it in the generated images, make sure it\u2019s in your pictures more often. It\u2019s recommended that you provide ~50 images of what you\u2019d like to train Stable Diffusion on to get great results. However, I\u2019ve only used 20-30 so far, and the results are pretty good. If you\u2019re just starting out and want to test it out, I think 20-30 images should be good enough for now, and you can get 50 images after you\u2019ve seen it work.","title":"Choosing Images"},{"location":"diffusion/dream-booth/#resize-crop-to-512-x-512px","text":"Once you\u2019ve chosen your images, you should prepare them. First, we need to resize and crop our images to be 512 x 512px. We can easily do this using the website https://birme.net . To do this, just: Visit the website Upload your images Set your dimensions to 512 x 512px Adjust the cropping area to center your subject Click on Save as Zip to download the archive. You can then unzip it on your computer, and we\u2019ll use them a bit later. Birme.net - Resize Images Resizing Images using Birme.net","title":"Resize &amp; Crop to 512 x 512px"},{"location":"diffusion/dream-booth/#renaming-your-images","text":"We\u2019ll also want to rename our images to contain the subject\u2019s name: Firstly, the subject name should be one unique/random/unknown keyword. This is because Stable Diffusion also has some knowledge of The Sandman from other sources other than the one played by Tom Sturridge and we don\u2019t want it to get confused and make a combination of interpretations of The Sandman. As such, I\u2019ll call it Sandman2022 to make sure it\u2019s unique. Renaming images to subject (1), subject (2) .. subject (30). This is because, using this method, you can train multiple subjects at once. If you want to fine-tune Stable Diffusion with Sandman, your friend Kevin, and your cat, you can give it prepare images for each of them. For the Sandman you\u2019d have Sandman2022 (1), Sandman2022 (2) \u2026 Sandman (30), for Kevin you\u2019d have KevinKevinson2022 (1), KevinKevinson2022 (2) \u2026 KevinKevinson (30), and for your cat you\u2019d have DexterTheCat (1), DexterTheCat (2) \u2026 DexterTheCat(30). Here\u2019s me renaming my images for Sandman2022 in bulk on Windows. Just select them all, right click one of them and click Rename and give it what name you want and click anywhere to finish the renaming. Everything else will be renamed as well.","title":"Renaming Your Images"},{"location":"diffusion/fine-tuning/","text":"Fine-tuning Latent Diffusion Model Source: Reddit Textual Inversion Textual Inversion - trains a word with one or more vectors that approximate your image. So if it is something it already has seen lots of examples of, it might have the concept and just need to 'point' at it. It is just expanding the vocabulary of model but all information it uses is already in the model. Dreambooth Dreambooth - this is essentially model fine tuning, which changes the weights of the main model. Dreambooth differs from typical fine tuning in that in tries to keep from forgetting/overwriting adjacent concepts during the tuning. Hypernetwork Hypernetworks - this is basically an adaptive head - it takes information from late in the model but injects information from the prompt 'skipping' the rest of the model. So it is similar to fine tuning the last 2 layers of a model but it gets much more signal from the prompt (it is taking the clip embedding of the prompt right before the output layer). Textual Inversion vs Hypernetworks Textual Inversion and Hyper Network have different driving principles, and Textual Inversion has much smaller capacity of learning results than Hyper Network. Textual Inversion has a slower learning speed than Hyper Network, so it is more suitable for learning specific objects, characters, features, etc. than abstract things such as patterns and painting styles. Also, in order to memorize the pattern and painting style, it is necessary to prepare data that has been unified to some extent, such as coloring and color usage, so it is more difficult to prepare learning data than Hyper Network. Also, Hyper Network can only embed one at a time, but even so, it is OK if you prepare a large amount of data using various patterns of composition, materials, and techniques and let it learn. Therefore, it can be said that Hyper Network is more suitable for improving the accuracy of illustrations. However, textual inversion is easier to handle if you want to remember specific patterns and characteristics. Training Hypernetworks hyper network layer structure If write \"1, 2, 1\", hypernetworks are composed of 2 fully connected layers whose intermediate dim is 2x, which is same as up to now. The more you add the number, like \"1, 2, 4, 2, 1\", the more the structure of hypernetworks becomes deeper. Deep hypernetworks are suited for training with large datasets. Add layer normalization If checked, add layer normalization after every fully connected layer. It would be meaningful to prevent hypernetworks from overfitting and make training more stable. https://rentry.org/sd-e621-textual-inversion Discussions Hypernetwork training topic Links Stable Diffusion Models","title":"Fine Tuning"},{"location":"diffusion/fine-tuning/#fine-tuning-latent-diffusion-model","text":"Source: Reddit","title":"Fine-tuning Latent Diffusion Model"},{"location":"diffusion/fine-tuning/#textual-inversion","text":"Textual Inversion - trains a word with one or more vectors that approximate your image. So if it is something it already has seen lots of examples of, it might have the concept and just need to 'point' at it. It is just expanding the vocabulary of model but all information it uses is already in the model.","title":"Textual Inversion"},{"location":"diffusion/fine-tuning/#dreambooth","text":"Dreambooth - this is essentially model fine tuning, which changes the weights of the main model. Dreambooth differs from typical fine tuning in that in tries to keep from forgetting/overwriting adjacent concepts during the tuning.","title":"Dreambooth"},{"location":"diffusion/fine-tuning/#hypernetwork","text":"Hypernetworks - this is basically an adaptive head - it takes information from late in the model but injects information from the prompt 'skipping' the rest of the model. So it is similar to fine tuning the last 2 layers of a model but it gets much more signal from the prompt (it is taking the clip embedding of the prompt right before the output layer).","title":"Hypernetwork"},{"location":"diffusion/fine-tuning/#textual-inversion-vs-hypernetworks","text":"Textual Inversion and Hyper Network have different driving principles, and Textual Inversion has much smaller capacity of learning results than Hyper Network. Textual Inversion has a slower learning speed than Hyper Network, so it is more suitable for learning specific objects, characters, features, etc. than abstract things such as patterns and painting styles. Also, in order to memorize the pattern and painting style, it is necessary to prepare data that has been unified to some extent, such as coloring and color usage, so it is more difficult to prepare learning data than Hyper Network. Also, Hyper Network can only embed one at a time, but even so, it is OK if you prepare a large amount of data using various patterns of composition, materials, and techniques and let it learn. Therefore, it can be said that Hyper Network is more suitable for improving the accuracy of illustrations. However, textual inversion is easier to handle if you want to remember specific patterns and characteristics.","title":"Textual Inversion vs Hypernetworks"},{"location":"diffusion/fine-tuning/#training-hypernetworks","text":"hyper network layer structure If write \"1, 2, 1\", hypernetworks are composed of 2 fully connected layers whose intermediate dim is 2x, which is same as up to now. The more you add the number, like \"1, 2, 4, 2, 1\", the more the structure of hypernetworks becomes deeper. Deep hypernetworks are suited for training with large datasets. Add layer normalization If checked, add layer normalization after every fully connected layer. It would be meaningful to prevent hypernetworks from overfitting and make training more stable. https://rentry.org/sd-e621-textual-inversion","title":"Training Hypernetworks"},{"location":"diffusion/fine-tuning/#discussions","text":"Hypernetwork training topic","title":"Discussions"},{"location":"diffusion/fine-tuning/#links","text":"Stable Diffusion Models","title":"Links"},{"location":"diffusion/glossary/","text":"Glossary Noise UNet GroupNorm applies group normalization to the inputs of each block Dropout layers for smoother training Multiple resnet layers per block (if layers_per_block isn't set to 1) Attention (usually used only at lower resolution blocks) Conditioning on the timestep. Downsampling and upsampling blocks with learnable parameters DDPM Schedulers or Samplers Sources https://github.com/huggingface/diffusion-models-class/tree/main/unit1","title":"Glossary"},{"location":"diffusion/glossary/#glossary","text":"","title":"Glossary"},{"location":"diffusion/glossary/#noise","text":"","title":"Noise"},{"location":"diffusion/glossary/#unet","text":"GroupNorm applies group normalization to the inputs of each block Dropout layers for smoother training Multiple resnet layers per block (if layers_per_block isn't set to 1) Attention (usually used only at lower resolution blocks) Conditioning on the timestep. Downsampling and upsampling blocks with learnable parameters","title":"UNet"},{"location":"diffusion/glossary/#ddpm","text":"","title":"DDPM"},{"location":"diffusion/glossary/#schedulers-or-samplers","text":"","title":"Schedulers or Samplers"},{"location":"diffusion/glossary/#sources","text":"https://github.com/huggingface/diffusion-models-class/tree/main/unit1","title":"Sources"},{"location":"diffusion/ldm/","text":"Latent Diffusion Models Recent advances in machine learning and artificial intelligence have made it possible to generate high-resolution images using latent diffusion models. These models are a type of mathematical model that is used to study the spread of knowledge, ideas, or innovations through a population or social network. In the context of image synthesis, these models can be used to generate photorealistic images from low-resolution inputs. One of the key challenges in generating high-resolution images with latent diffusion models is the so-called \"resolution gap.\" This refers to the difference between the resolution of the input image and the desired output image. In order to generate a high-resolution output image from a low-resolution input image, the latent diffusion model must be able to fill in the missing details and add new features to the image. Recent research has demonstrated that latent diffusion models can be trained to perform high-resolution image synthesis using a process called \"progressive growing.\" In this process, the latent diffusion model is trained on a set of low-resolution images and then gradually increased in resolution as it generates high-resolution outputs. This allows the model to learn the finer details of the images and generate more realistic outputs. One of the key advantages of using latent diffusion models for image synthesis is that they can generate high-resolution images that are consistent with the input image. This means that the generated images will have the same style and composition as the input image, but with added details and features. This can be useful for applications such as image super-resolution, inpainting, and stylization. Overall, the use of latent diffusion models for high-resolution image synthesis is a promising area of research that has the potential to revolutionize the field of computer graphics and image processing. By leveraging the power of machine learning and AI, these models can generate photorealistic images from low-resolution inputs, and open up new possibilities for image manipulation and generation.","title":"What is Latent Diffusion"},{"location":"diffusion/ldm/#latent-diffusion-models","text":"Recent advances in machine learning and artificial intelligence have made it possible to generate high-resolution images using latent diffusion models. These models are a type of mathematical model that is used to study the spread of knowledge, ideas, or innovations through a population or social network. In the context of image synthesis, these models can be used to generate photorealistic images from low-resolution inputs. One of the key challenges in generating high-resolution images with latent diffusion models is the so-called \"resolution gap.\" This refers to the difference between the resolution of the input image and the desired output image. In order to generate a high-resolution output image from a low-resolution input image, the latent diffusion model must be able to fill in the missing details and add new features to the image. Recent research has demonstrated that latent diffusion models can be trained to perform high-resolution image synthesis using a process called \"progressive growing.\" In this process, the latent diffusion model is trained on a set of low-resolution images and then gradually increased in resolution as it generates high-resolution outputs. This allows the model to learn the finer details of the images and generate more realistic outputs. One of the key advantages of using latent diffusion models for image synthesis is that they can generate high-resolution images that are consistent with the input image. This means that the generated images will have the same style and composition as the input image, but with added details and features. This can be useful for applications such as image super-resolution, inpainting, and stylization. Overall, the use of latent diffusion models for high-resolution image synthesis is a promising area of research that has the potential to revolutionize the field of computer graphics and image processing. By leveraging the power of machine learning and AI, these models can generate photorealistic images from low-resolution inputs, and open up new possibilities for image manipulation and generation.","title":"Latent Diffusion Models"},{"location":"diffusion/prompting/","text":"Prompting Prompt Weighting Source: AUTOMATIC1111 Using () in the prompt increases the model's attention to enclosed words, and [] decreases it. You can combine multiple modifiers: Cheat Sheet a (word) - increase attention to word by a factor of 1.1 a ((word)) - increase attention to word by a factor of 1.21 (= 1.1 * 1.1) a [word] - decrease attention to word by a factor of 1.1 a (word:1.5) - increase attention to word by a factor of 1.5 a (word:0.25) - decrease attention to word by a factor of 4 (= 1 / 0.25) a (word) - use literal () characters in prompt With (), a weight can be specified like this: (text:1.4). If the weight is not specified, it is assumed to be 1.1. Specifying weight only works with () not with []. If you want to use any of the literal ()[] characters in the prompt, use the backslash to escape them: anime_(character). On 2022-09-29, a new implementation was added that supports escape characters and numerical weights. A downside of the new implementation is that the old one was not perfect and sometimes ate characters: \"a (((farm))), daytime\", for example, would become \"a farm daytime\" without the comma. This behavior is not shared by the new implementation which preserves all text correctly, and this means that your saved seeds may produce different pictures. For now, there is an option in settings to use the old implementation. NAI uses my implementation from before 2022-09-29, except they have 1.05 as the multiplier and use {} instead of (). So the conversion applies: their {word} = our (word:1.05) their {{word}} = our (word:1.1025) their [word] = our (word:0.952) (0.952 = 1/1.05) their [[word]] = our (word:0.907) (0.907 = 1/1.05/1.05) Negative Prompts Negative prompts are essential for 2.0 or greater models. Negative Prompt Example disfigured, kitsch, ugly, oversaturated, grain, low-res, Deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, ugly, disgusting, poorly drawn, childish, mutilated, mangled, old, surreal, watermark","title":"Prompting"},{"location":"diffusion/prompting/#prompting","text":"","title":"Prompting"},{"location":"diffusion/prompting/#prompt-weighting","text":"Source: AUTOMATIC1111 Using () in the prompt increases the model's attention to enclosed words, and [] decreases it. You can combine multiple modifiers: Cheat Sheet a (word) - increase attention to word by a factor of 1.1 a ((word)) - increase attention to word by a factor of 1.21 (= 1.1 * 1.1) a [word] - decrease attention to word by a factor of 1.1 a (word:1.5) - increase attention to word by a factor of 1.5 a (word:0.25) - decrease attention to word by a factor of 4 (= 1 / 0.25) a (word) - use literal () characters in prompt With (), a weight can be specified like this: (text:1.4). If the weight is not specified, it is assumed to be 1.1. Specifying weight only works with () not with []. If you want to use any of the literal ()[] characters in the prompt, use the backslash to escape them: anime_(character). On 2022-09-29, a new implementation was added that supports escape characters and numerical weights. A downside of the new implementation is that the old one was not perfect and sometimes ate characters: \"a (((farm))), daytime\", for example, would become \"a farm daytime\" without the comma. This behavior is not shared by the new implementation which preserves all text correctly, and this means that your saved seeds may produce different pictures. For now, there is an option in settings to use the old implementation. NAI uses my implementation from before 2022-09-29, except they have 1.05 as the multiplier and use {} instead of (). So the conversion applies: their {word} = our (word:1.05) their {{word}} = our (word:1.1025) their [word] = our (word:0.952) (0.952 = 1/1.05) their [[word]] = our (word:0.907) (0.907 = 1/1.05/1.05)","title":"Prompt Weighting"},{"location":"diffusion/prompting/#negative-prompts","text":"Negative prompts are essential for 2.0 or greater models. Negative Prompt Example disfigured, kitsch, ugly, oversaturated, grain, low-res, Deformed, blurry, bad anatomy, disfigured, poorly drawn face, mutation, mutated, extra limb, ugly, poorly drawn hands, missing limb, blurry, floating limbs, disconnected limbs, malformed hands, blur, out of focus, long neck, long body, ugly, disgusting, poorly drawn, childish, mutilated, mangled, old, surreal, watermark","title":"Negative Prompts"},{"location":"diffusion/samplers/","text":"Samplers Source: Reddit There are ancestral samplers (marked by the letter \"a\") whose output will keep changing as the number of steps increases, and the others, which will eventually settle on a final image. This final image is different between Karras and non-Karras samplers, but very similar within those groups. Then there's DPM fast, which doesn't feel particularly fast, and which always seems to produce inferior images for me. DPM adaptive is also its own thing, as it ignores step count and works with cfg scale instead. More cfg = more steps. I kind of like it when I'm not sure how many steps I should use, but the final step count is generally high. It can also take a long, long time if you use the \"AND\" prompt syntax - I have interrupted it after waiting for over 2000 steps. Most differences between the different samplers appear at low step counts < 20. Some produce distinguishable images faster and some slower, and may look very different in the early stages. That's random though, there's no good way to predict what those early images will turn into with more steps. In practice, the choice of samplers is just preference, there's actually very little difference in the long run. First, you have to understand what samplers are. These are discretized differential equations. I'm not going to go into these at all in this post, but I've covered them before. DDIM and PLMS were the original samplers. They were part of Latent Diffusion's repository. They stand for the papers that introduced them, Denoising Diffusion Implicit Models and Pseudo Numerical Methods for Diffusion Models on Manifolds. Almost all other samplers come from work done by @RiversHaveWings or Katherine Crowson, which is mostly contained in her work at this repository. She is listed as the principal researcher at Stability AI. Her notes for those samplers are as follows: \u2060Euler - Implements Algorithm 2 (Euler steps) from Karras et al. (2022) \u2060Euler_a - Ancestral sampling with Euler method steps. \u2060LMS - No information, but can be inferred that the name comes from linear multistep coefficients \u2060Heun - Implements Algorithm 2 (Heun steps) from Karras et al. (2022). \u2060DPM2 - A sampler inspired by DPM-Solver-2 and Algorithm 2 from Karras et al. (2022). \u2060DPM2 a - Ancestral sampling with DPM-Solver second-order steps \u2060DPM++ 2s a - Ancestral sampling with DPM-Solver++(2S) second-order steps \u2060DPM++ 2M - DPM-Solver++(2M) \u2060DPM++ SDE - DPM-Solver++ (stochastic) \u2060DPM fast - DPM-Solver-Fast (fixed step size). See https://arxiv.org/abs/2206.00927 \u2060DPM adaptive - DPM-Solver-12 and 23 (adaptive step size). See https://arxiv.org/abs/2206.00927 The 'Karras' versions of these weren't made by Karras as far as I can tell, but instead are using a variance-exploding scheduler from the Karras paper, which of course is extra confusing given that most of the other samplers were inspired by that paper in the first place. In terms of \"what will I get at high step counts\", most of the time you will get similar pictures from: \u2060Group A: Euler_a, DPM2 a, DPM++ 2S a, DPM fast (after many steps), DPM adaptive, DPM2 a Karras \u2060Group B: Euler, LMS, Heun, DPM2, DPM++ 2M, DDIM, PLMS \u2060Group C: LMS Karras, DPM2 Karras, DPM++ 2M Karras As far as convergence behavior: \u2060Does not converge: Euler_a, DPM2 a, DPM Fast, DDIM, PLMS, DPM adaptive, DPM2 a Karras \u2060Converges: Euler, LMS, Heun, DPM2, DPM++ 2M, LMS Karras, DPM2 Karras, DPM++ 2M Karras By required steps: \u2060Euler_a = Euler = DPM++2M = LMS Karras (image degraded at high steps) > \u2060LMS = DPM++ 2M Karras = Heun (slower) = DPM++ 2S a (slower) = DPM++ 2S a Karras > \u2060DDIM = PLMS = DPM2 (slower) = DPM 2 Karras> \u2060DPM Fast = DPM2 a (slower) These all give somewhat different results so a person could prefer the output of any of the models at a given CFG or step range. I do think that there is an argument to be made that DPM++ 2M and Euler_a are good generic samplers for most people, however, as they both resolve to a good picture at low seeds (sub-20) without a hit to iteration speed. DPM++ 2M has the advantage of converging to a single image more often (if you choose to run the same image at higher seed), but is slightly more prone to deformations at high CFG. To combine all the above: \u2060Fast, new, converges: DPM++ 2M, DPM++ 2M Karras \u2060Fast, doesn't converge: Euler_a, DPM2 a Karras \u2060Others worth considering: DPM2 a, LMS, DPM++ 2S a Karras \u2060Bugged: LMS Karras (at high steps \u2060Older, fast but maybe lower quality final result: Euler, LMS, Heun \u2060Slow: DDIM, PLMS, DPM2, DPM 2 Karras, DPM Fast, DPM2 a TL;DR These are confusingly named and mostly come from academic papers. The actual mechanisms of each sampler aren't really relevant to their outputs. In general PLMS, DDIM, or DPM fast are slower and give worse results. Instead, try out DPM++ 2M and Euler_a, along with DPM++ 2M Karras. These should all give good results at a low seed value.","title":"Samplers"},{"location":"diffusion/samplers/#samplers","text":"Source: Reddit There are ancestral samplers (marked by the letter \"a\") whose output will keep changing as the number of steps increases, and the others, which will eventually settle on a final image. This final image is different between Karras and non-Karras samplers, but very similar within those groups. Then there's DPM fast, which doesn't feel particularly fast, and which always seems to produce inferior images for me. DPM adaptive is also its own thing, as it ignores step count and works with cfg scale instead. More cfg = more steps. I kind of like it when I'm not sure how many steps I should use, but the final step count is generally high. It can also take a long, long time if you use the \"AND\" prompt syntax - I have interrupted it after waiting for over 2000 steps. Most differences between the different samplers appear at low step counts < 20. Some produce distinguishable images faster and some slower, and may look very different in the early stages. That's random though, there's no good way to predict what those early images will turn into with more steps. In practice, the choice of samplers is just preference, there's actually very little difference in the long run. First, you have to understand what samplers are. These are discretized differential equations. I'm not going to go into these at all in this post, but I've covered them before. DDIM and PLMS were the original samplers. They were part of Latent Diffusion's repository. They stand for the papers that introduced them, Denoising Diffusion Implicit Models and Pseudo Numerical Methods for Diffusion Models on Manifolds. Almost all other samplers come from work done by @RiversHaveWings or Katherine Crowson, which is mostly contained in her work at this repository. She is listed as the principal researcher at Stability AI. Her notes for those samplers are as follows: \u2060Euler - Implements Algorithm 2 (Euler steps) from Karras et al. (2022) \u2060Euler_a - Ancestral sampling with Euler method steps. \u2060LMS - No information, but can be inferred that the name comes from linear multistep coefficients \u2060Heun - Implements Algorithm 2 (Heun steps) from Karras et al. (2022). \u2060DPM2 - A sampler inspired by DPM-Solver-2 and Algorithm 2 from Karras et al. (2022). \u2060DPM2 a - Ancestral sampling with DPM-Solver second-order steps \u2060DPM++ 2s a - Ancestral sampling with DPM-Solver++(2S) second-order steps \u2060DPM++ 2M - DPM-Solver++(2M) \u2060DPM++ SDE - DPM-Solver++ (stochastic) \u2060DPM fast - DPM-Solver-Fast (fixed step size). See https://arxiv.org/abs/2206.00927 \u2060DPM adaptive - DPM-Solver-12 and 23 (adaptive step size). See https://arxiv.org/abs/2206.00927 The 'Karras' versions of these weren't made by Karras as far as I can tell, but instead are using a variance-exploding scheduler from the Karras paper, which of course is extra confusing given that most of the other samplers were inspired by that paper in the first place. In terms of \"what will I get at high step counts\", most of the time you will get similar pictures from: \u2060Group A: Euler_a, DPM2 a, DPM++ 2S a, DPM fast (after many steps), DPM adaptive, DPM2 a Karras \u2060Group B: Euler, LMS, Heun, DPM2, DPM++ 2M, DDIM, PLMS \u2060Group C: LMS Karras, DPM2 Karras, DPM++ 2M Karras As far as convergence behavior: \u2060Does not converge: Euler_a, DPM2 a, DPM Fast, DDIM, PLMS, DPM adaptive, DPM2 a Karras \u2060Converges: Euler, LMS, Heun, DPM2, DPM++ 2M, LMS Karras, DPM2 Karras, DPM++ 2M Karras By required steps: \u2060Euler_a = Euler = DPM++2M = LMS Karras (image degraded at high steps) > \u2060LMS = DPM++ 2M Karras = Heun (slower) = DPM++ 2S a (slower) = DPM++ 2S a Karras > \u2060DDIM = PLMS = DPM2 (slower) = DPM 2 Karras> \u2060DPM Fast = DPM2 a (slower) These all give somewhat different results so a person could prefer the output of any of the models at a given CFG or step range. I do think that there is an argument to be made that DPM++ 2M and Euler_a are good generic samplers for most people, however, as they both resolve to a good picture at low seeds (sub-20) without a hit to iteration speed. DPM++ 2M has the advantage of converging to a single image more often (if you choose to run the same image at higher seed), but is slightly more prone to deformations at high CFG. To combine all the above: \u2060Fast, new, converges: DPM++ 2M, DPM++ 2M Karras \u2060Fast, doesn't converge: Euler_a, DPM2 a Karras \u2060Others worth considering: DPM2 a, LMS, DPM++ 2S a Karras \u2060Bugged: LMS Karras (at high steps \u2060Older, fast but maybe lower quality final result: Euler, LMS, Heun \u2060Slow: DDIM, PLMS, DPM2, DPM 2 Karras, DPM Fast, DPM2 a TL;DR These are confusingly named and mostly come from academic papers. The actual mechanisms of each sampler aren't really relevant to their outputs. In general PLMS, DDIM, or DPM fast are slower and give worse results. Instead, try out DPM++ 2M and Euler_a, along with DPM++ 2M Karras. These should all give good results at a low seed value.","title":"Samplers"},{"location":"diffusion/tools/","text":"Tools & Blogs and Useful Links Tools Diffusion Image Search Engines Lexica Openart.ai AI Art Apps Prompt Engineering https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion img2prompt https://replicate.com/methexis-inc/img2prompt https://colab.research.google.com/github/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator.ipynb#scrollTo=xpPKQR40qvz2 Upscaling https://github.com/xinntao/Real-ESRGAN Depth-map https://huggingface.co/spaces/pytorch/MiDaS Image Prep https://www.birme.net/?target_width=512&target_height=512 Blogs Stable Diffusion Training for Personal Embedding The Illustrated Stable Diffusion A Traveler\u2019s Guide to the Latent Space Useful Links Stable Diffusion Update https://rentry.org/sdupdates3 https://rentry.org/sdgoldmine","title":"Tools & Blogs and Useful Links"},{"location":"diffusion/tools/#tools-blogs-and-useful-links","text":"","title":"Tools &amp; Blogs and Useful Links"},{"location":"diffusion/tools/#tools","text":"","title":"Tools"},{"location":"diffusion/tools/#diffusion-image-search-engines","text":"Lexica Openart.ai AI Art Apps","title":"Diffusion Image Search Engines"},{"location":"diffusion/tools/#prompt-engineering","text":"https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion","title":"Prompt Engineering"},{"location":"diffusion/tools/#img2prompt","text":"https://replicate.com/methexis-inc/img2prompt https://colab.research.google.com/github/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator.ipynb#scrollTo=xpPKQR40qvz2","title":"img2prompt"},{"location":"diffusion/tools/#upscaling","text":"https://github.com/xinntao/Real-ESRGAN","title":"Upscaling"},{"location":"diffusion/tools/#depth-map","text":"https://huggingface.co/spaces/pytorch/MiDaS","title":"Depth-map"},{"location":"diffusion/tools/#image-prep","text":"https://www.birme.net/?target_width=512&target_height=512","title":"Image Prep"},{"location":"diffusion/tools/#blogs","text":"Stable Diffusion Training for Personal Embedding The Illustrated Stable Diffusion A Traveler\u2019s Guide to the Latent Space","title":"Blogs"},{"location":"diffusion/tools/#useful-links","text":"","title":"Useful Links"},{"location":"diffusion/tools/#stable-diffusion-update","text":"https://rentry.org/sdupdates3 https://rentry.org/sdgoldmine","title":"Stable Diffusion Update"},{"location":"diffusion/training-ldm/","text":"Training Latent Diffusion Model Kernel Inception Distance The kernel inception distance (KID) is a measure of the similarity between two sets of images. It was introduced by Google AI researcher, Tobias Weyand, in his paper \"Kernel Inception Distance: A New Distance Measure for Improved Image Quality Assessment\". In this paper, Weyand proposed using the KID to measure the difference between the activations of two sets of images when passed through an inception network, which is a type of deep learning neural network. The KID is calculated as the squared L2 distance between the distributions of the activations of the two sets of images. It is typically used in the field of computer vision to evaluate the performance of image generation algorithms. Sources https://keras.io/examples/generative/ddim #denoising-diffusion-implicit-models https://benanne.github.io/2022/01/31/diffusion.html https://github.com/apapiu/guided-diffusion-keras https://www.louisbouchard.ai/latent-diffusion-models/ https://www.kaggle.com/code/apapiu/train-latent-diffusion-in-keras-from-scratch https://github.com/CompVis/taming-transformers https://github.com/CompVis/latent-diffusion https://github.com/huggingface/diffusers","title":"Training LDM"},{"location":"diffusion/training-ldm/#training-latent-diffusion-model","text":"","title":"Training Latent Diffusion Model"},{"location":"diffusion/training-ldm/#kernel-inception-distance","text":"The kernel inception distance (KID) is a measure of the similarity between two sets of images. It was introduced by Google AI researcher, Tobias Weyand, in his paper \"Kernel Inception Distance: A New Distance Measure for Improved Image Quality Assessment\". In this paper, Weyand proposed using the KID to measure the difference between the activations of two sets of images when passed through an inception network, which is a type of deep learning neural network. The KID is calculated as the squared L2 distance between the distributions of the activations of the two sets of images. It is typically used in the field of computer vision to evaluate the performance of image generation algorithms.","title":"Kernel Inception Distance"},{"location":"diffusion/training-ldm/#sources","text":"https://keras.io/examples/generative/ddim #denoising-diffusion-implicit-models https://benanne.github.io/2022/01/31/diffusion.html https://github.com/apapiu/guided-diffusion-keras https://www.louisbouchard.ai/latent-diffusion-models/ https://www.kaggle.com/code/apapiu/train-latent-diffusion-in-keras-from-scratch https://github.com/CompVis/taming-transformers https://github.com/CompVis/latent-diffusion https://github.com/huggingface/diffusers","title":"Sources"},{"location":"diffusion/vit/","text":"Vision Transformers ViTs Uzun yillardir CNN algoritmalari goruntu isleme konularinda neredeyse tek cozumumuzdu. ResNet, EfficientNetm Inception vb. gibi tum mimariler temelde CNN mimarilerini kullanarak goruntu isleme problemlerimizi cozmede bize yardimci oluyor. Bugun sizinle goruntu isleme konusunda farkli bir yaklasim olan ViT'ler yani Vision Transformerlari inceleyecegiz. Aslinda Transformer kavrami NLP alaninda yurutulen teknolojiler icin ortaya konmustu. Attention is all you need adiyla yayinlanan makale NLP problemlerinin cozumu icin devrimsel cozumler getirdi, artik Transformer-based mimarilar nlp gorevleri icin standart bir hale geldi. Cok da uzun bir sure gecmeden dogal dil alaninda kullanilan bu mimari goruntu alaninda da ufak degisikliklerle uyarlandi. Bu calismayi \"An image is worth 16x16 words\" olarak bu linkteki paperdan okuyabilirsiniz. Asagida daha detayli anlatacagim fakat temel olarak bir goruntuyu 16x16 boyutlu parcalara ayirarak embeddinglerini cikartmak uzere. Temel bazi konulari anlatmadan bu mekanikleri aciklamak cok zor bu yuzden hiz kaybetmeden konuyu daha iyi anlamak icin alt basliklara gecelim. Attention Mekanizmasi Mimari ViT mimarisi birka\u00e7 a\u015famadan olu\u015fur: Patch + Position Embedding (inputs) - Giri\u015f g\u00f6r\u00fcnt\u00fcs\u00fcn\u00fc bir dizi g\u00f6r\u00fcnt\u00fc parcalarina (patches) d\u00f6n\u00fc\u015ft\u00fcr\u00fcr ve parcalarin hangi s\u0131rayla geldi\u011fini bilmek icin bir konum numaras\u0131 ekler. Linear projection of flattened patches (Embedded Patches) - G\u00f6r\u00fcnt\u00fc parcalari embeddinglere d\u00f6n\u00fc\u015f\u00fcr, g\u00f6r\u00fcnt\u00fcleri direkt kullanmak yerine embeddingleri kullanman\u0131n yarar\u0131, embeddingler g\u00f6r\u00fcnt\u00fcn\u00fcn e\u011fitimle \u00f6\u011frenilebilir bir temsili olmas\u0131d\u0131r. Norm - Bir sinir a\u011f\u0131n\u0131 d\u00fczenli hale getirmek (overfitting'i azaltmak) i\u00e7in bir teknik olan \"Layer Normalization\" veya \"LayerNorm\"un k\u0131saltmas\u0131d\u0131r. Multi-Head Attention - Bu, Multi-Headed Self-Attention layer veya k\u0131saca \"MSA\" d\u0131r. MLP (Multilayer perceptron) - Genellikle herhangi bir ileri besleme katman\u0131 koleksiyonunu olarak dusunebilirsiniz. Transformer Encoder - Transformer Encoder, yukar\u0131da listelenen katmanlar\u0131n bir koleksiyonudur. Transformer Encoderin i\u00e7inde iki atlama (skip) ba\u011flant\u0131s\u0131 vard\u0131r (\"+\" sembolleri), katman\u0131n girdilerinin do\u011frudan sonraki katmanlar\u0131n yan\u0131 s\u0131ra hemen sonraki katmanlara beslendi\u011fi anlam\u0131na gelir. Genel ViT mimarisi, birbiri \u00fczerine y\u0131\u011f\u0131lm\u0131\u015f bir dizi Transformer kodlay\u0131c\u0131dan olu\u015fur. MLP Head - Bu, mimarinin \u00e7\u0131kt\u0131 katman\u0131d\u0131r, bir girdinin \u00f6\u011frenilen \u00f6zelliklerini bir s\u0131n\u0131f \u00e7\u0131kt\u0131s\u0131na d\u00f6n\u00fc\u015ft\u00fcr\u00fcr. G\u00f6r\u00fcnt\u00fc s\u0131n\u0131fland\u0131rmas\u0131 \u00fczerinde \u00e7al\u0131\u015ft\u0131\u011f\u0131m\u0131z i\u00e7in buna \"s\u0131n\u0131fland\u0131r\u0131c\u0131 kafa\" da diyebilirsiniz. MLP Kafas\u0131n\u0131n yap\u0131s\u0131 MLP blo\u011funa benzer. Patch Embeddings Standart Transformer, giri\u015fi tek boyutlu token embedding dizisi olarak al\u0131r. 2B g\u00f6r\u00fcnt\u00fcleri i\u015flemek i\u00e7in x\u2208R^{H\u00d7W\u00d7C} g\u00f6r\u00fcnt\u00fcs\u00fcn\u00fc d\u00fczle\u015ftirilmi\u015f 2B patchlere (goruntu parcalarina) yeniden \u015fekillendiriyoruz. Burada, (H, W) orijinal g\u00f6r\u00fcnt\u00fcn\u00fcn \u00e7\u00f6z\u00fcn\u00fcrl\u00fc\u011f\u00fcd\u00fcr ve (P, P) her g\u00f6r\u00fcnt\u00fc par\u00e7as\u0131n\u0131n \u00e7\u00f6z\u00fcn\u00fcrl\u00fc\u011f\u00fcd\u00fcr. Resim sabit boyutlu parcalara b\u00f6l\u00fcnm\u00fc\u015ft\u00fcr, a\u015fa\u011f\u0131daki resimde yama boyutu 16\u00d716 olarak al\u0131nm\u0131\u015ft\u0131r. Yani g\u00f6r\u00fcnt\u00fcn\u00fcn boyutlar\u0131 48\u00d748 olacakt\u0131r. Self-attention maliyeti quadratictir. G\u00f6r\u00fcnt\u00fcn\u00fcn her pikselini girdi olarak iletirsek, Self-attention her pikselin di\u011fer t\u00fcm piksellerle ilgilenmesini gerektirir. Self-attention ikinci dereceden maliyeti \u00e7ok maliyetli olacak ve ger\u00e7ek\u00e7i girdi boyutuna \u00f6l\u00e7eklenmeyecek; bu nedenle, g\u00f6r\u00fcnt\u00fc parcalara b\u00f6l\u00fcn\u00fcr. import matplotlib.pyplot as plt from PIL import Image import numpy as np img = Image.open('cobanov-profile.jpg') img.thumbnail((224, 224)) array_img = np.array(img) array_img.shape # Setup hyperparameters and make sure img_size and patch_size are compatible img_size = 224 patch_size = 16 num_patches = img_size/patch_size assert img_size % patch_size == 0, \"Image size must be divisible by patch size\" print(f\"Number of patches per row: {num_patches}\\ \\nNumber of patches per column: {num_patches}\\ \\nTotal patches: {num_patches*num_patches}\\ \\nPatch size: {patch_size} pixels x {patch_size} pixels\") # Create a series of subplots fig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float ncols=img_size // patch_size, figsize=(num_patches, num_patches), sharex=True, sharey=True) # Loop through height and width of image for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width # Plot the permuted image patch (image_permuted -> (Height, Width, Color Channels)) axs[i, j].imshow(array_img[patch_height:patch_height+patch_size, # iterate through height patch_width:patch_width+patch_size, # iterate through width :]) # get all color channels # Set up label information, remove the ticks for clarity and set labels to outside axs[i, j].set_ylabel(i+1, rotation=\"horizontal\", horizontalalignment=\"right\", verticalalignment=\"center\") axs[i, j].set_xlabel(j+1) axs[i, j].set_xticks([]) axs[i, j].set_yticks([]) axs[i, j].label_outer() # Set a super title plt.show() Linear Projection of Flattened Patches Parcalar Transformer blo\u011funa ge\u00e7irmeden \u00f6nce, makalenin yazarlar\u0131 yamalar\u0131 \u00f6nce do\u011frusal bir projeksiyondan ge\u00e7irmeyi faydal\u0131 buldular. Bir yamay\u0131 al\u0131p b\u00fcy\u00fck bir vekt\u00f6re a\u00e7arlar ve yamal\u0131 g\u00f6mmeler olu\u015fturmak i\u00e7in g\u00f6mme matrisiyle \u00e7arparlar ve bu, konumsal g\u00f6mmeyle birlikte transformat\u00f6re giden \u015feydir. Her yama, t\u00fcm piksel kanallar\u0131n\u0131 bir yamada birle\u015ftirerek ve ard\u0131ndan bunu do\u011frusal olarak istenen giri\u015f boyutuna yans\u0131tarak g\u00f6m\u00fclen bir 1B yamaya d\u00fczle\u015ftirilir. Positional embeddings Nasil konusurken dilde kelimelerin s\u0131ras\u0131 kurdugunuz cumlenin anlamini tamamen degistiriyorsa, goruntuler uzerinde de buna dikkat etmek gerekir. Maalesef transformerlar, patch embeddinglerin \"s\u0131ras\u0131n\u0131\" dikkate alan herhangi bir varsay\u0131lan mekanizmaya sahip de\u011fildir. Bir yapboz yaptiginizi dusunun, elinizdeki parcalar (yani onceki adimlarda yaptigimiz patch embeddingler) karisik bir duzende geldiginde goruntunun tamaminda ne oldugunu anlamak oldukca zordur, bu transformat\u00f6rler i\u00e7in de ge\u00e7erli. Modelin yapboz par\u00e7alar\u0131n\u0131n s\u0131ras\u0131n\u0131 veya konumunu \u00e7\u0131karmas\u0131n\u0131 sa\u011flaman\u0131n bir yoluna ihtiyac\u0131m\u0131z var. Transformat\u00f6rler, giri\u015f elemanlar\u0131n\u0131n yap\u0131s\u0131ndan ba\u011f\u0131ms\u0131zd\u0131r. Her yamaya \u00f6\u011frenilebilir konum yerle\u015ftirmeleri eklemek, modelin g\u00f6r\u00fcnt\u00fcn\u00fcn yap\u0131s\u0131 hakk\u0131nda bilgi edinmesine olanak tan\u0131r. Positional embeddinglerde d\u00fczeni modele aktarmamizi sagliyor. ViT i\u00e7in, bu Positional embeddingler, patch embeddingler ile ayn\u0131 boyutlulu\u011fa sahip \u00f6\u011frenilmi\u015f vekt\u00f6rlerdir. Bu konumsal yerle\u015ftirmeler, \u00f6n e\u011fitim s\u0131ras\u0131nda ve (bazen) ince ayar s\u0131ras\u0131nda \u00f6\u011frenilir. E\u011fitim s\u0131ras\u0131nda, bu yerle\u015ftirmeler, \u00f6zellikle ayn\u0131 s\u00fctunu ve sat\u0131r\u0131 payla\u015fan kom\u015fu konum yerle\u015ftirmelerine y\u00fcksek benzerlik g\u00f6sterdikleri vekt\u00f6r uzaylar\u0131nda birle\u015fir. Transformer Encoding Multi-Head Self Attention Layer(MSP) birden fazla attention ciktisini lineer olarak beklenen boyutlara esitlemek i\u00e7in kullanilir. MSP, g\u00f6r\u00fcnt\u00fcdeki yerel ve global ba\u011f\u0131ml\u0131l\u0131klar\u0131 \u00f6\u011frenmeye yard\u0131mc\u0131 olur. Multi-Layer Perceptrons(MLP) - Klasik sinir agi katmani fakat aktivasyon fonksiyonu olarak GELU kullaniyoruz. Layer Norm(LN) e\u011fitim g\u00f6r\u00fcnt\u00fcleri aras\u0131nda herhangi bir yeni ba\u011f\u0131ml\u0131l\u0131k getirmedi\u011finden her bloktan \u00f6nce uygulan\u0131r. E\u011fitim s\u00fcresini ve genelleme performans\u0131n\u0131 iyile\u015ftirmeye yard\u0131mc\u0131 olun. Residual connections gradyanlar\u0131n do\u011frusal olmayan aktivasyonlardan ge\u00e7meden do\u011frudan a\u011f \u00fczerinden akmas\u0131na izin verdi\u011fi i\u00e7in her bloktan sonra uygulan\u0131r. G\u00f6r\u00fcnt\u00fc s\u0131n\u0131fland\u0131rmas\u0131 i\u00e7in, \u00f6n e\u011fitim zaman\u0131nda bir gizli katman ve ince ayar i\u00e7in tek bir do\u011frusal katman ile MLP kullan\u0131larak bir s\u0131n\u0131fland\u0131rma kafas\u0131 uygulan\u0131r. ViT'nin \u00fcst katmanlar\u0131 global \u00f6zellikleri \u00f6\u011frenirken, alt katmanlar hem global hem de yerel \u00f6zellikleri \u00f6\u011frenir. Bu, ViT'nin daha genel kal\u0131plar\u0131 \u00f6\u011frenmesini sa\u011flar. Egitim ViT, b\u00fcy\u00fck veri k\u00fcmelerinde \u00f6nceden e\u011fitilmi\u015ftir ve daha k\u00fc\u00e7\u00fck bir veri k\u00fcmesine ince ayar yap\u0131lm\u0131\u015ft\u0131r. \u0130nce ayar yap\u0131l\u0131rken, \u00f6nceden e\u011fitilmi\u015f son tahmin kafas\u0131 kald\u0131r\u0131l\u0131r ve daha k\u00fc\u00e7\u00fck veri k\u00fcmesine dayal\u0131 olarak s\u0131n\u0131flar\u0131 tahmin etmek i\u00e7in s\u0131f\u0131r ba\u015flat\u0131lm\u0131\u015f bir ileri besleme katman\u0131 ekleriz. \u0130nce ayar, modelin \u00f6nceden e\u011fitildi\u011finden daha y\u00fcksek \u00e7\u00f6z\u00fcn\u00fcrl\u00fckl\u00fc bir g\u00f6r\u00fcnt\u00fcye uygulanabilir, ancak yama boyutu ayn\u0131 kalmal\u0131d\u0131r. Transformers, g\u00f6r\u00fcnt\u00fc yap\u0131s\u0131 hakk\u0131nda \u00f6nceden bilgiye sahip de\u011fildir ve bu nedenle daha uzun e\u011fitim s\u00fcrelerine sahiptir ve modeli e\u011fitmek i\u00e7in b\u00fcy\u00fck veri k\u00fcmeleri gerektirir. Sources https://www.learnpytorch.io/08_pytorch_paper_replicating/#3-replicating-the-vit-paper-an-overview https://theaisummer.com/vision-transformer/ https://medium.com/swlh/visual-transformers-a-new-computer-vision-paradigm-aa78c2a2ccf2","title":"ViT Visual Transformers"},{"location":"diffusion/vit/#vision-transformers-vits","text":"Uzun yillardir CNN algoritmalari goruntu isleme konularinda neredeyse tek cozumumuzdu. ResNet, EfficientNetm Inception vb. gibi tum mimariler temelde CNN mimarilerini kullanarak goruntu isleme problemlerimizi cozmede bize yardimci oluyor. Bugun sizinle goruntu isleme konusunda farkli bir yaklasim olan ViT'ler yani Vision Transformerlari inceleyecegiz. Aslinda Transformer kavrami NLP alaninda yurutulen teknolojiler icin ortaya konmustu. Attention is all you need adiyla yayinlanan makale NLP problemlerinin cozumu icin devrimsel cozumler getirdi, artik Transformer-based mimarilar nlp gorevleri icin standart bir hale geldi. Cok da uzun bir sure gecmeden dogal dil alaninda kullanilan bu mimari goruntu alaninda da ufak degisikliklerle uyarlandi. Bu calismayi \"An image is worth 16x16 words\" olarak bu linkteki paperdan okuyabilirsiniz. Asagida daha detayli anlatacagim fakat temel olarak bir goruntuyu 16x16 boyutlu parcalara ayirarak embeddinglerini cikartmak uzere. Temel bazi konulari anlatmadan bu mekanikleri aciklamak cok zor bu yuzden hiz kaybetmeden konuyu daha iyi anlamak icin alt basliklara gecelim.","title":"Vision Transformers ViTs"},{"location":"diffusion/vit/#attention-mekanizmasi","text":"","title":"Attention Mekanizmasi"},{"location":"diffusion/vit/#mimari","text":"ViT mimarisi birka\u00e7 a\u015famadan olu\u015fur: Patch + Position Embedding (inputs) - Giri\u015f g\u00f6r\u00fcnt\u00fcs\u00fcn\u00fc bir dizi g\u00f6r\u00fcnt\u00fc parcalarina (patches) d\u00f6n\u00fc\u015ft\u00fcr\u00fcr ve parcalarin hangi s\u0131rayla geldi\u011fini bilmek icin bir konum numaras\u0131 ekler. Linear projection of flattened patches (Embedded Patches) - G\u00f6r\u00fcnt\u00fc parcalari embeddinglere d\u00f6n\u00fc\u015f\u00fcr, g\u00f6r\u00fcnt\u00fcleri direkt kullanmak yerine embeddingleri kullanman\u0131n yarar\u0131, embeddingler g\u00f6r\u00fcnt\u00fcn\u00fcn e\u011fitimle \u00f6\u011frenilebilir bir temsili olmas\u0131d\u0131r. Norm - Bir sinir a\u011f\u0131n\u0131 d\u00fczenli hale getirmek (overfitting'i azaltmak) i\u00e7in bir teknik olan \"Layer Normalization\" veya \"LayerNorm\"un k\u0131saltmas\u0131d\u0131r. Multi-Head Attention - Bu, Multi-Headed Self-Attention layer veya k\u0131saca \"MSA\" d\u0131r. MLP (Multilayer perceptron) - Genellikle herhangi bir ileri besleme katman\u0131 koleksiyonunu olarak dusunebilirsiniz. Transformer Encoder - Transformer Encoder, yukar\u0131da listelenen katmanlar\u0131n bir koleksiyonudur. Transformer Encoderin i\u00e7inde iki atlama (skip) ba\u011flant\u0131s\u0131 vard\u0131r (\"+\" sembolleri), katman\u0131n girdilerinin do\u011frudan sonraki katmanlar\u0131n yan\u0131 s\u0131ra hemen sonraki katmanlara beslendi\u011fi anlam\u0131na gelir. Genel ViT mimarisi, birbiri \u00fczerine y\u0131\u011f\u0131lm\u0131\u015f bir dizi Transformer kodlay\u0131c\u0131dan olu\u015fur. MLP Head - Bu, mimarinin \u00e7\u0131kt\u0131 katman\u0131d\u0131r, bir girdinin \u00f6\u011frenilen \u00f6zelliklerini bir s\u0131n\u0131f \u00e7\u0131kt\u0131s\u0131na d\u00f6n\u00fc\u015ft\u00fcr\u00fcr. G\u00f6r\u00fcnt\u00fc s\u0131n\u0131fland\u0131rmas\u0131 \u00fczerinde \u00e7al\u0131\u015ft\u0131\u011f\u0131m\u0131z i\u00e7in buna \"s\u0131n\u0131fland\u0131r\u0131c\u0131 kafa\" da diyebilirsiniz. MLP Kafas\u0131n\u0131n yap\u0131s\u0131 MLP blo\u011funa benzer.","title":"Mimari"},{"location":"diffusion/vit/#patch-embeddings","text":"Standart Transformer, giri\u015fi tek boyutlu token embedding dizisi olarak al\u0131r. 2B g\u00f6r\u00fcnt\u00fcleri i\u015flemek i\u00e7in x\u2208R^{H\u00d7W\u00d7C} g\u00f6r\u00fcnt\u00fcs\u00fcn\u00fc d\u00fczle\u015ftirilmi\u015f 2B patchlere (goruntu parcalarina) yeniden \u015fekillendiriyoruz. Burada, (H, W) orijinal g\u00f6r\u00fcnt\u00fcn\u00fcn \u00e7\u00f6z\u00fcn\u00fcrl\u00fc\u011f\u00fcd\u00fcr ve (P, P) her g\u00f6r\u00fcnt\u00fc par\u00e7as\u0131n\u0131n \u00e7\u00f6z\u00fcn\u00fcrl\u00fc\u011f\u00fcd\u00fcr. Resim sabit boyutlu parcalara b\u00f6l\u00fcnm\u00fc\u015ft\u00fcr, a\u015fa\u011f\u0131daki resimde yama boyutu 16\u00d716 olarak al\u0131nm\u0131\u015ft\u0131r. Yani g\u00f6r\u00fcnt\u00fcn\u00fcn boyutlar\u0131 48\u00d748 olacakt\u0131r. Self-attention maliyeti quadratictir. G\u00f6r\u00fcnt\u00fcn\u00fcn her pikselini girdi olarak iletirsek, Self-attention her pikselin di\u011fer t\u00fcm piksellerle ilgilenmesini gerektirir. Self-attention ikinci dereceden maliyeti \u00e7ok maliyetli olacak ve ger\u00e7ek\u00e7i girdi boyutuna \u00f6l\u00e7eklenmeyecek; bu nedenle, g\u00f6r\u00fcnt\u00fc parcalara b\u00f6l\u00fcn\u00fcr. import matplotlib.pyplot as plt from PIL import Image import numpy as np img = Image.open('cobanov-profile.jpg') img.thumbnail((224, 224)) array_img = np.array(img) array_img.shape # Setup hyperparameters and make sure img_size and patch_size are compatible img_size = 224 patch_size = 16 num_patches = img_size/patch_size assert img_size % patch_size == 0, \"Image size must be divisible by patch size\" print(f\"Number of patches per row: {num_patches}\\ \\nNumber of patches per column: {num_patches}\\ \\nTotal patches: {num_patches*num_patches}\\ \\nPatch size: {patch_size} pixels x {patch_size} pixels\") # Create a series of subplots fig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float ncols=img_size // patch_size, figsize=(num_patches, num_patches), sharex=True, sharey=True) # Loop through height and width of image for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width # Plot the permuted image patch (image_permuted -> (Height, Width, Color Channels)) axs[i, j].imshow(array_img[patch_height:patch_height+patch_size, # iterate through height patch_width:patch_width+patch_size, # iterate through width :]) # get all color channels # Set up label information, remove the ticks for clarity and set labels to outside axs[i, j].set_ylabel(i+1, rotation=\"horizontal\", horizontalalignment=\"right\", verticalalignment=\"center\") axs[i, j].set_xlabel(j+1) axs[i, j].set_xticks([]) axs[i, j].set_yticks([]) axs[i, j].label_outer() # Set a super title plt.show()","title":"Patch Embeddings"},{"location":"diffusion/vit/#linear-projection-of-flattened-patches","text":"Parcalar Transformer blo\u011funa ge\u00e7irmeden \u00f6nce, makalenin yazarlar\u0131 yamalar\u0131 \u00f6nce do\u011frusal bir projeksiyondan ge\u00e7irmeyi faydal\u0131 buldular. Bir yamay\u0131 al\u0131p b\u00fcy\u00fck bir vekt\u00f6re a\u00e7arlar ve yamal\u0131 g\u00f6mmeler olu\u015fturmak i\u00e7in g\u00f6mme matrisiyle \u00e7arparlar ve bu, konumsal g\u00f6mmeyle birlikte transformat\u00f6re giden \u015feydir. Her yama, t\u00fcm piksel kanallar\u0131n\u0131 bir yamada birle\u015ftirerek ve ard\u0131ndan bunu do\u011frusal olarak istenen giri\u015f boyutuna yans\u0131tarak g\u00f6m\u00fclen bir 1B yamaya d\u00fczle\u015ftirilir.","title":"Linear Projection of Flattened Patches"},{"location":"diffusion/vit/#positional-embeddings","text":"Nasil konusurken dilde kelimelerin s\u0131ras\u0131 kurdugunuz cumlenin anlamini tamamen degistiriyorsa, goruntuler uzerinde de buna dikkat etmek gerekir. Maalesef transformerlar, patch embeddinglerin \"s\u0131ras\u0131n\u0131\" dikkate alan herhangi bir varsay\u0131lan mekanizmaya sahip de\u011fildir. Bir yapboz yaptiginizi dusunun, elinizdeki parcalar (yani onceki adimlarda yaptigimiz patch embeddingler) karisik bir duzende geldiginde goruntunun tamaminda ne oldugunu anlamak oldukca zordur, bu transformat\u00f6rler i\u00e7in de ge\u00e7erli. Modelin yapboz par\u00e7alar\u0131n\u0131n s\u0131ras\u0131n\u0131 veya konumunu \u00e7\u0131karmas\u0131n\u0131 sa\u011flaman\u0131n bir yoluna ihtiyac\u0131m\u0131z var. Transformat\u00f6rler, giri\u015f elemanlar\u0131n\u0131n yap\u0131s\u0131ndan ba\u011f\u0131ms\u0131zd\u0131r. Her yamaya \u00f6\u011frenilebilir konum yerle\u015ftirmeleri eklemek, modelin g\u00f6r\u00fcnt\u00fcn\u00fcn yap\u0131s\u0131 hakk\u0131nda bilgi edinmesine olanak tan\u0131r. Positional embeddinglerde d\u00fczeni modele aktarmamizi sagliyor. ViT i\u00e7in, bu Positional embeddingler, patch embeddingler ile ayn\u0131 boyutlulu\u011fa sahip \u00f6\u011frenilmi\u015f vekt\u00f6rlerdir. Bu konumsal yerle\u015ftirmeler, \u00f6n e\u011fitim s\u0131ras\u0131nda ve (bazen) ince ayar s\u0131ras\u0131nda \u00f6\u011frenilir. E\u011fitim s\u0131ras\u0131nda, bu yerle\u015ftirmeler, \u00f6zellikle ayn\u0131 s\u00fctunu ve sat\u0131r\u0131 payla\u015fan kom\u015fu konum yerle\u015ftirmelerine y\u00fcksek benzerlik g\u00f6sterdikleri vekt\u00f6r uzaylar\u0131nda birle\u015fir.","title":"Positional embeddings"},{"location":"diffusion/vit/#transformer-encoding","text":"Multi-Head Self Attention Layer(MSP) birden fazla attention ciktisini lineer olarak beklenen boyutlara esitlemek i\u00e7in kullanilir. MSP, g\u00f6r\u00fcnt\u00fcdeki yerel ve global ba\u011f\u0131ml\u0131l\u0131klar\u0131 \u00f6\u011frenmeye yard\u0131mc\u0131 olur. Multi-Layer Perceptrons(MLP) - Klasik sinir agi katmani fakat aktivasyon fonksiyonu olarak GELU kullaniyoruz. Layer Norm(LN) e\u011fitim g\u00f6r\u00fcnt\u00fcleri aras\u0131nda herhangi bir yeni ba\u011f\u0131ml\u0131l\u0131k getirmedi\u011finden her bloktan \u00f6nce uygulan\u0131r. E\u011fitim s\u00fcresini ve genelleme performans\u0131n\u0131 iyile\u015ftirmeye yard\u0131mc\u0131 olun. Residual connections gradyanlar\u0131n do\u011frusal olmayan aktivasyonlardan ge\u00e7meden do\u011frudan a\u011f \u00fczerinden akmas\u0131na izin verdi\u011fi i\u00e7in her bloktan sonra uygulan\u0131r. G\u00f6r\u00fcnt\u00fc s\u0131n\u0131fland\u0131rmas\u0131 i\u00e7in, \u00f6n e\u011fitim zaman\u0131nda bir gizli katman ve ince ayar i\u00e7in tek bir do\u011frusal katman ile MLP kullan\u0131larak bir s\u0131n\u0131fland\u0131rma kafas\u0131 uygulan\u0131r. ViT'nin \u00fcst katmanlar\u0131 global \u00f6zellikleri \u00f6\u011frenirken, alt katmanlar hem global hem de yerel \u00f6zellikleri \u00f6\u011frenir. Bu, ViT'nin daha genel kal\u0131plar\u0131 \u00f6\u011frenmesini sa\u011flar.","title":"Transformer Encoding"},{"location":"diffusion/vit/#egitim","text":"ViT, b\u00fcy\u00fck veri k\u00fcmelerinde \u00f6nceden e\u011fitilmi\u015ftir ve daha k\u00fc\u00e7\u00fck bir veri k\u00fcmesine ince ayar yap\u0131lm\u0131\u015ft\u0131r. \u0130nce ayar yap\u0131l\u0131rken, \u00f6nceden e\u011fitilmi\u015f son tahmin kafas\u0131 kald\u0131r\u0131l\u0131r ve daha k\u00fc\u00e7\u00fck veri k\u00fcmesine dayal\u0131 olarak s\u0131n\u0131flar\u0131 tahmin etmek i\u00e7in s\u0131f\u0131r ba\u015flat\u0131lm\u0131\u015f bir ileri besleme katman\u0131 ekleriz. \u0130nce ayar, modelin \u00f6nceden e\u011fitildi\u011finden daha y\u00fcksek \u00e7\u00f6z\u00fcn\u00fcrl\u00fckl\u00fc bir g\u00f6r\u00fcnt\u00fcye uygulanabilir, ancak yama boyutu ayn\u0131 kalmal\u0131d\u0131r. Transformers, g\u00f6r\u00fcnt\u00fc yap\u0131s\u0131 hakk\u0131nda \u00f6nceden bilgiye sahip de\u011fildir ve bu nedenle daha uzun e\u011fitim s\u00fcrelerine sahiptir ve modeli e\u011fitmek i\u00e7in b\u00fcy\u00fck veri k\u00fcmeleri gerektirir.","title":"Egitim"},{"location":"diffusion/vit/#sources","text":"https://www.learnpytorch.io/08_pytorch_paper_replicating/#3-replicating-the-vit-paper-an-overview https://theaisummer.com/vision-transformer/ https://medium.com/swlh/visual-transformers-a-new-computer-vision-paradigm-aa78c2a2ccf2","title":"Sources"}]}