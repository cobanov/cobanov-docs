<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Mert Cobanov" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>ViT Visual Transformers - Cobanov</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "ViT Visual Transformers";
        var mkdocs_page_input_path = "diffusion\\vit.md";
        var mkdocs_page_url = null;
      </script>
    
    <script src="../../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
      <script>hljs.initHighlightingOnLoad();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Cobanov
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Blog</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../..">Hello ğŸ‘‹</a>
                  </li>
                  <li class="toctree-l1"><a class="" href="../../helpersv2.md">Helper one-liners</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../new-macbook/">New Macbook Installation</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cyberpunk/">Cypherpunk Manifesto</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../python-conf/">Doing Python Configuration Right</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../windows-wsl-ssh/">Windows WSL Activate SSH</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../nerf/">NeRF</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../instagram/">Instagram</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../tuning-terms/">Training and Tuning Terms</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../pyyaml/">Python YAML Configs</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">About</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../about/">Main Page</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">diffusion-book</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../ldm/">What is Latent Diffusion</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">ViT Visual Transformers</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#attention-mekanizmasi">Attention Mekanizmasi</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#mimari">Mimari</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#patch-embeddings">Patch Embeddings</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#linear-projection-of-flattened-patches">Linear Projection of Flattened Patches</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#positional-embeddings">Positional embeddings</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#transformer-encoding">Transformer Encoding</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#egitim">Egitim</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#sources">Sources</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../glossary/">Glossary</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../samplers/">Samplers</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../prompting/">Prompting</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../blender-depthmap/">Blender Depthmap</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../training-ldm/">Training LDM</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../fine-tuning/">Fine Tuning</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../dream-booth/">Dream Booth</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../tools/">Tools & Blogs and Useful Links</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Cobanov</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a> &raquo;</li>
          <li>diffusion-book &raquo;</li>
      <li>ViT Visual Transformers</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="vision-transformers-vits">Vision Transformers ViTs</h1>
<p><img alt="vit" src="../../assets/vit.png" /></p>
<p>Uzun yillardir CNN algoritmalari goruntu isleme konularinda neredeyse tek cozumumuzdu. ResNet, EfficientNetm Inception vb. gibi tum mimariler temelde CNN mimarilerini kullanarak goruntu isleme problemlerimizi cozmede bize yardimci oluyor. Bugun sizinle goruntu isleme konusunda farkli bir yaklasim olan ViT'ler yani Vision Transformerlari inceleyecegiz.</p>
<p>Aslinda Transformer kavrami NLP alaninda yurutulen teknolojiler icin ortaya konmustu. Attention is all you need adiyla yayinlanan makale NLP problemlerinin cozumu icin devrimsel cozumler getirdi, artik Transformer-based mimarilar nlp gorevleri icin standart bir hale geldi.</p>
<p>Cok da uzun bir sure gecmeden dogal dil alaninda kullanilan bu mimari goruntu alaninda da ufak degisikliklerle uyarlandi. Bu calismayi <a href="">"An image is worth 16x16 words"</a> olarak bu linkteki paperdan okuyabilirsiniz. Asagida daha detayli anlatacagim fakat temel olarak bir goruntuyu 16x16 boyutlu parcalara ayirarak embeddinglerini cikartmak uzere. Temel bazi konulari anlatmadan bu mekanikleri aciklamak cok zor bu yuzden hiz kaybetmeden konuyu daha iyi anlamak icin alt basliklara gecelim.</p>
<h2 id="attention-mekanizmasi">Attention Mekanizmasi</h2>
<h2 id="mimari">Mimari</h2>
<p>ViT mimarisi birkaÃ§ aÅŸamadan oluÅŸur:</p>
<ol>
<li>
<p><strong>Patch + Position Embedding (inputs)</strong> - GiriÅŸ gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ bir dizi gÃ¶rÃ¼ntÃ¼ parcalarina (patches) dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r ve parcalarin hangi sÄ±rayla geldiÄŸini bilmek icin bir konum numarasÄ± ekler.</p>
</li>
<li>
<p><strong>Linear projection of flattened patches (Embedded Patches)</strong> - GÃ¶rÃ¼ntÃ¼ parcalari embeddinglere dÃ¶nÃ¼ÅŸÃ¼r, gÃ¶rÃ¼ntÃ¼leri direkt kullanmak yerine embeddingleri kullanmanÄ±n yararÄ±, embeddingler gÃ¶rÃ¼ntÃ¼nÃ¼n eÄŸitimle Ã¶ÄŸrenilebilir bir temsili olmasÄ±dÄ±r.</p>
</li>
<li>
<p><strong>Norm</strong> - Bir sinir aÄŸÄ±nÄ± dÃ¼zenli hale getirmek (overfitting'i azaltmak) iÃ§in bir teknik olan "Layer Normalization" veya "LayerNorm"un kÄ±saltmasÄ±dÄ±r.</p>
</li>
<li>
<p><strong>Multi-Head Attention</strong> - Bu, Multi-Headed Self-Attention layer veya kÄ±saca "MSA" dÄ±r.</p>
</li>
<li>
<p><strong>MLP (Multilayer perceptron)</strong> - Genellikle herhangi bir ileri besleme katmanÄ± koleksiyonunu olarak dusunebilirsiniz.</p>
</li>
<li>
<p><strong>Transformer Encoder</strong> - Transformer Encoder, yukarÄ±da listelenen katmanlarÄ±n bir koleksiyonudur. Transformer Encoderin iÃ§inde iki atlama (skip) baÄŸlantÄ±sÄ± vardÄ±r ("+" sembolleri), katmanÄ±n girdilerinin doÄŸrudan sonraki katmanlarÄ±n yanÄ± sÄ±ra hemen sonraki katmanlara beslendiÄŸi anlamÄ±na gelir. Genel ViT mimarisi, birbiri Ã¼zerine yÄ±ÄŸÄ±lmÄ±ÅŸ bir dizi Transformer kodlayÄ±cÄ±dan oluÅŸur.</p>
</li>
<li>
<p><strong>MLP Head</strong> - Bu, mimarinin Ã§Ä±ktÄ± katmanÄ±dÄ±r, bir girdinin Ã¶ÄŸrenilen Ã¶zelliklerini bir sÄ±nÄ±f Ã§Ä±ktÄ±sÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r. GÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rmasÄ± Ã¼zerinde Ã§alÄ±ÅŸtÄ±ÄŸÄ±mÄ±z iÃ§in buna "sÄ±nÄ±flandÄ±rÄ±cÄ± kafa" da diyebilirsiniz. MLP KafasÄ±nÄ±n yapÄ±sÄ± MLP bloÄŸuna benzer.</p>
</li>
</ol>
<p><img alt="vit architecture" src="../../assets/vit-arch.png" /></p>
<h3 id="patch-embeddings">Patch Embeddings</h3>
<p>Standart Transformer, giriÅŸi tek boyutlu token embedding dizisi olarak alÄ±r. 2B gÃ¶rÃ¼ntÃ¼leri iÅŸlemek iÃ§in <strong>xâˆˆR^{HÃ—WÃ—C}</strong> gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ dÃ¼zleÅŸtirilmiÅŸ 2B patchlere (goruntu parcalarina) yeniden ÅŸekillendiriyoruz.</p>
<p>Burada, (H, W) orijinal gÃ¶rÃ¼ntÃ¼nÃ¼n Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼dÃ¼r ve (P, P) her gÃ¶rÃ¼ntÃ¼ parÃ§asÄ±nÄ±n Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸÃ¼dÃ¼r. Resim sabit boyutlu parcalara bÃ¶lÃ¼nmÃ¼ÅŸtÃ¼r, aÅŸaÄŸÄ±daki resimde yama boyutu 16Ã—16 olarak alÄ±nmÄ±ÅŸtÄ±r. Yani gÃ¶rÃ¼ntÃ¼nÃ¼n boyutlarÄ± 48Ã—48 olacaktÄ±r.</p>
<p>Self-attention maliyeti quadratictir. GÃ¶rÃ¼ntÃ¼nÃ¼n her pikselini girdi olarak iletirsek, Self-attention her pikselin diÄŸer tÃ¼m piksellerle ilgilenmesini gerektirir. Self-attention ikinci dereceden maliyeti Ã§ok maliyetli olacak ve gerÃ§ekÃ§i girdi boyutuna Ã¶lÃ§eklenmeyecek; bu nedenle, gÃ¶rÃ¼ntÃ¼ parcalara bÃ¶lÃ¼nÃ¼r.</p>
<pre><code class="language-python">import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
</code></pre>
<pre><code class="language-python">img = Image.open('cobanov-profile.jpg')
img.thumbnail((224, 224))
array_img = np.array(img)
array_img.shape
</code></pre>
<pre><code class="language-python"># Setup hyperparameters and make sure img_size and patch_size are compatible
img_size = 224
patch_size = 16
num_patches = img_size/patch_size 
assert img_size % patch_size == 0, &quot;Image size must be divisible by patch size&quot; 
print(f&quot;Number of patches per row: {num_patches}\
        \nNumber of patches per column: {num_patches}\
        \nTotal patches: {num_patches*num_patches}\
        \nPatch size: {patch_size} pixels x {patch_size} pixels&quot;)

</code></pre>
<pre><code class="language-python"># Create a series of subplots
fig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float
                        ncols=img_size // patch_size, 
                        figsize=(num_patches, num_patches),
                        sharex=True,
                        sharey=True)

# Loop through height and width of image
for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height
    for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width

        # Plot the permuted image patch (image_permuted -&gt; (Height, Width, Color Channels))
        axs[i, j].imshow(array_img[patch_height:patch_height+patch_size, # iterate through height 
                                        patch_width:patch_width+patch_size, # iterate through width
                                        :]) # get all color channels

        # Set up label information, remove the ticks for clarity and set labels to outside
        axs[i, j].set_ylabel(i+1, 
                             rotation=&quot;horizontal&quot;, 
                             horizontalalignment=&quot;right&quot;, 
                             verticalalignment=&quot;center&quot;) 
        axs[i, j].set_xlabel(j+1) 
        axs[i, j].set_xticks([])
        axs[i, j].set_yticks([])
        axs[i, j].label_outer()

# Set a super title
plt.show()
</code></pre>
<p><img alt="patch-embeddings" src="../../assets/patch_emb.png" /></p>
<h2 id="linear-projection-of-flattened-patches">Linear Projection of Flattened Patches</h2>
<p>Parcalar Transformer bloÄŸuna geÃ§irmeden Ã¶nce, makalenin yazarlarÄ± yamalarÄ± Ã¶nce doÄŸrusal bir projeksiyondan geÃ§irmeyi faydalÄ± buldular. Bir yamayÄ± alÄ±p bÃ¼yÃ¼k bir vektÃ¶re aÃ§arlar ve yamalÄ± gÃ¶mmeler oluÅŸturmak iÃ§in gÃ¶mme matrisiyle Ã§arparlar ve bu, konumsal gÃ¶mmeyle birlikte transformatÃ¶re giden ÅŸeydir.</p>
<p>Her yama, tÃ¼m piksel kanallarÄ±nÄ± bir yamada birleÅŸtirerek ve ardÄ±ndan bunu doÄŸrusal olarak istenen giriÅŸ boyutuna yansÄ±tarak gÃ¶mÃ¼len bir 1B yamaya dÃ¼zleÅŸtirilir.</p>
<h2 id="positional-embeddings">Positional embeddings</h2>
<p>Nasil konusurken dilde kelimelerin sÄ±rasÄ± kurdugunuz cumlenin anlamini tamamen degistiriyorsa, goruntuler uzerinde de buna dikkat etmek gerekir. Maalesef transformerlar, patch embeddinglerin "sÄ±rasÄ±nÄ±" dikkate alan herhangi bir varsayÄ±lan mekanizmaya sahip deÄŸildir.</p>
<p>Bir yapboz yaptiginizi dusunun, elinizdeki parcalar (yani onceki adimlarda yaptigimiz patch embeddingler) karisik bir duzende geldiginde goruntunun tamaminda ne oldugunu anlamak oldukca zordur, bu transformatÃ¶rler iÃ§in de geÃ§erli. Modelin yapboz parÃ§alarÄ±nÄ±n sÄ±rasÄ±nÄ± veya konumunu Ã§Ä±karmasÄ±nÄ± saÄŸlamanÄ±n bir yoluna ihtiyacÄ±mÄ±z var.</p>
<p>TransformatÃ¶rler, giriÅŸ elemanlarÄ±nÄ±n yapÄ±sÄ±ndan baÄŸÄ±msÄ±zdÄ±r. Her yamaya Ã¶ÄŸrenilebilir konum yerleÅŸtirmeleri eklemek, modelin gÃ¶rÃ¼ntÃ¼nÃ¼n yapÄ±sÄ± hakkÄ±nda bilgi edinmesine olanak tanÄ±r. Positional embeddinglerde dÃ¼zeni modele aktarmamizi sagliyor. ViT iÃ§in, bu Positional embeddingler, patch embeddingler ile aynÄ± boyutluluÄŸa sahip Ã¶ÄŸrenilmiÅŸ vektÃ¶rlerdir.</p>
<p>Bu konumsal yerleÅŸtirmeler, Ã¶n eÄŸitim sÄ±rasÄ±nda ve (bazen) ince ayar sÄ±rasÄ±nda Ã¶ÄŸrenilir. EÄŸitim sÄ±rasÄ±nda, bu yerleÅŸtirmeler, Ã¶zellikle aynÄ± sÃ¼tunu ve satÄ±rÄ± paylaÅŸan komÅŸu konum yerleÅŸtirmelerine yÃ¼ksek benzerlik gÃ¶sterdikleri vektÃ¶r uzaylarÄ±nda birleÅŸir.</p>
<p><img alt="" src="../../assets/visualizing-positional-encodings-vit.png" /></p>
<h2 id="transformer-encoding">Transformer Encoding</h2>
<ul>
<li>
<p><strong>Multi-Head Self Attention Layer(MSP)</strong> birden fazla attention ciktisini lineer olarak beklenen boyutlara esitlemek iÃ§in kullanilir. MSP, gÃ¶rÃ¼ntÃ¼deki yerel ve global baÄŸÄ±mlÄ±lÄ±klarÄ± Ã¶ÄŸrenmeye yardÄ±mcÄ± olur.</p>
</li>
<li>
<p><strong>Multi-Layer Perceptrons(MLP)</strong> - Klasik sinir agi katmani fakat aktivasyon fonksiyonu olarak GELU kullaniyoruz.</p>
</li>
<li>
<p><strong>Layer Norm(LN)</strong> eÄŸitim gÃ¶rÃ¼ntÃ¼leri arasÄ±nda herhangi bir yeni baÄŸÄ±mlÄ±lÄ±k getirmediÄŸinden her bloktan Ã¶nce uygulanÄ±r. EÄŸitim sÃ¼resini ve genelleme performansÄ±nÄ± iyileÅŸtirmeye yardÄ±mcÄ± olun.</p>
</li>
<li>
<p><strong>Residual connections</strong> gradyanlarÄ±n doÄŸrusal olmayan aktivasyonlardan geÃ§meden doÄŸrudan aÄŸ Ã¼zerinden akmasÄ±na izin verdiÄŸi iÃ§in her bloktan sonra uygulanÄ±r.
GÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rmasÄ± iÃ§in, Ã¶n eÄŸitim zamanÄ±nda bir gizli katman ve ince ayar iÃ§in tek bir doÄŸrusal katman ile MLP kullanÄ±larak bir sÄ±nÄ±flandÄ±rma kafasÄ± uygulanÄ±r. ViT'nin Ã¼st katmanlarÄ± global Ã¶zellikleri Ã¶ÄŸrenirken, alt katmanlar hem global hem de yerel Ã¶zellikleri Ã¶ÄŸrenir. Bu, ViT'nin daha genel kalÄ±plarÄ± Ã¶ÄŸrenmesini saÄŸlar.</p>
</li>
</ul>
<h2 id="egitim">Egitim</h2>
<p>ViT, bÃ¼yÃ¼k veri kÃ¼melerinde Ã¶nceden eÄŸitilmiÅŸtir ve daha kÃ¼Ã§Ã¼k bir veri kÃ¼mesine ince ayar yapÄ±lmÄ±ÅŸtÄ±r.</p>
<p>Ä°nce ayar yapÄ±lÄ±rken, Ã¶nceden eÄŸitilmiÅŸ son tahmin kafasÄ± kaldÄ±rÄ±lÄ±r ve daha kÃ¼Ã§Ã¼k veri kÃ¼mesine dayalÄ± olarak sÄ±nÄ±flarÄ± tahmin etmek iÃ§in sÄ±fÄ±r baÅŸlatÄ±lmÄ±ÅŸ bir ileri besleme katmanÄ± ekleriz.</p>
<p>Ä°nce ayar, modelin Ã¶nceden eÄŸitildiÄŸinden daha yÃ¼ksek Ã§Ã¶zÃ¼nÃ¼rlÃ¼klÃ¼ bir gÃ¶rÃ¼ntÃ¼ye uygulanabilir, ancak yama boyutu aynÄ± kalmalÄ±dÄ±r.</p>
<p>Transformers, gÃ¶rÃ¼ntÃ¼ yapÄ±sÄ± hakkÄ±nda Ã¶nceden bilgiye sahip deÄŸildir ve bu nedenle daha uzun eÄŸitim sÃ¼relerine sahiptir ve modeli eÄŸitmek iÃ§in bÃ¼yÃ¼k veri kÃ¼meleri gerektirir.</p>
<h2 id="sources">Sources</h2>
<ul>
<li><a href="https://www.learnpytorch.io/08_pytorch_paper_replicating/#3-replicating-the-vit-paper-an-overview">https://www.learnpytorch.io/08_pytorch_paper_replicating/#3-replicating-the-vit-paper-an-overview</a></li>
<li><a href="https://theaisummer.com/vision-transformer/">https://theaisummer.com/vision-transformer/</a></li>
<li><a href="https://medium.com/swlh/visual-transformers-a-new-computer-vision-paradigm-aa78c2a2ccf2">https://medium.com/swlh/visual-transformers-a-new-computer-vision-paradigm-aa78c2a2ccf2</a></li>
</ul>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../ldm/" class="btn btn-neutral float-left" title="What is Latent Diffusion"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../glossary/" class="btn btn-neutral float-right" title="Glossary">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../ldm/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../glossary/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme_extra.js" defer></script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
